{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biostat M280 Homework 4\n",
    "Sarah Ji\n",
    "\n",
    "**Due June 12 @ 11:59PM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, we build a classifer for handwritten digit recognition. Following figure shows example bitmaps of handwritten digits from U.S. postal envelopes. \n",
    "\n",
    "Each digit is represented by a $32 \\times 32$ bitmap in which each element indicates one pixel with a value of white or black. Each $32 \\times 32$ bitmap is divided into blocks of $4 \\times 4$, and the number of white pixels are counted in each block. Therefore each handwritten digit is summarized by a vector $\\mathbf{x} = (x_1, \\ldots, x_{64})$ of length 64 where each element is a count between 0 and 16. \n",
    "\n",
    "We will use a model-based method by assuming a distribution on the count vector and carry out classification using probabilities. A common distribution for count vectors is the multinomial distribution. However as you will see in Q10, it is not a good model for handwritten digits. Let's work on a more flexible model for count vectors. In the Dirichlet-multinomial model, we assume the multinomial probabilities $\\mathbf{p} = (p_1,\\ldots, p_d)$ follow a Dirichlet distribution with parameter vector $\\alpha = (\\alpha_1,\\ldots, \\alpha_d)$, $\\alpha_j>0$, and density\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\t\\pi(\\mathbf{p}) =  \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1},\n",
    "\\end{eqnarray*} \n",
    "$$\n",
    "where $|\\alpha|=\\sum_{j=1}^d \\alpha_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1\n",
    "\n",
    "For a multivariate count vector $\\mathbf{x}=(x_1,\\ldots,x_d)$ with batch size $|\\mathbf{x}|=\\sum_{j=1}^d x_j$, show that the probability mass function for Dirichlet-multinomial distribution is\n",
    "$$\n",
    "    f(\\mathbf{x} \\mid \\alpha) \n",
    "\t= \\int_{\\Delta_d} \\binom{|\\mathbf{x}|}{\\mathbf{x}} \\prod_{j=1}^d p_j^{x_j} \\pi(\\mathbf{p}) \\, d \\mathbf{p}  \n",
    "    = \\binom{|\\mathbf{x}|}{\\mathbf{x}} \\frac{\\prod_{j=1}^d \\Gamma(\\alpha_j+x_j)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\frac{\\Gamma(|\\alpha|)}{\\Gamma(|\\alpha|+|\\mathbf{x}|)}\n",
    "$$\n",
    "where $\\Delta_d$ is the unit simplex in $d$ dimensions and $|\\alpha| = \\sum_{j=1}^d \\alpha_j$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 Solution: \n",
    " \n",
    ">First we should plug in the pi(p) formula in the f(x) formula and then combine like terms.\n",
    "\n",
    " $$\n",
    "\\int_{\\Delta_d} \\binom{|\\mathbf{x}|}{\\mathbf{x}} \\prod_{j=1}^d p_j^{x_j} \\pi(\\mathbf{p}) \\, d \\mathbf{p} \n",
    "= \\int_{\\Delta_d}\\binom{|\\mathbf{x}|}{\\mathbf{x}} \\prod_{j=1}^d p_j^{x_j}\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1}d \\mathbf{p}\n",
    "$$\n",
    " \n",
    ">Then we can use the kernel trick to integrate the first pdf to 1 and change the coefficients accordingly.\n",
    " \n",
    "\n",
    "$$\n",
    "=\\binom{|\\mathbf{x}|}{\\mathbf{x}}\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\int_{\\Delta_d} \\prod_{j=1}^d p_j^{x_j + {\\alpha_j-1}} d\\mathbf{p}\n",
    "$$\n",
    "$$\n",
    "=\\binom{|\\mathbf{x}|}{\\mathbf{x}}\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\int_{\\Delta_d} \\prod_{j=1}^d p_j^{x_j + {\\alpha_j-1}} \\frac{\\prod_{j=1}^d \\Gamma(\\alpha_j+x_j)}{\\prod_{j=1}^d \\Gamma(\\alpha_j+x_j)}\\frac{\\Gamma(|\\alpha|+|\\mathbf{x}|)}{\\Gamma(|\\alpha|+|\\mathbf{x}|)}d\\mathbf{p}\n",
    "$$\n",
    "\n",
    "$$\n",
    "=\\binom{|\\mathbf{x}|}{\\mathbf{x}}\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\frac{\\prod_{j=1}^d \\Gamma(\\alpha_j+x_j)}{\\Gamma(|\\alpha|+|\\mathbf{x}|)}\\int_{\\Delta_d} \\prod_{j=1}^d p_j^{x_j + {\\alpha_j-1}} \\frac{\\prod_{j=1}^d \\Gamma(\\alpha_j+x_j)}{\\Gamma(|\\alpha|+|\\mathbf{x}|)}d\\mathbf{p}\n",
    "$$\n",
    "\n",
    "> Note this is a density of a Dirchlet pdf and so the integral evalutes to 1, giving us: \n",
    "\n",
    "$$\n",
    "=\\binom{|\\mathbf{x}|}{\\mathbf{x}}\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\frac{\\prod_{j=1}^d \\Gamma(\\alpha_j+x_j)}{\\Gamma(|\\alpha|+|\\mathbf{x}|)} 1\n",
    "$$\n",
    "$$\n",
    "=\\binom{|\\mathbf{x}|}{\\mathbf{x}} \\frac{\\prod_{j=1}^d \\Gamma(\\alpha_j+x_j)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\frac{\\Gamma(|\\alpha|)}{\\Gamma(|\\alpha|+|\\mathbf{x}|)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= f(\\mathbf{x} \\mid \\alpha)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2\n",
    "\n",
    "Given independent data points $\\mathbf{x}_1, \\ldots, \\mathbf{x}_n$, show that the log-likelihood is\n",
    "$$\n",
    "L(\\alpha) = \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} + \\sum_{i=1}^n \\sum_{j=1}^d [\\ln \\Gamma(\\alpha_j + x_{ij}) - \\ln \\Gamma(\\alpha_j)] - \\sum_{i=1}^n [\\ln \\Gamma(|\\alpha|+|\\mathbf{x}_i|) - \\ln \\Gamma(|\\alpha|)].\n",
    "$$\n",
    "Is the log-likelihood a concave function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2 Solution:\n",
    "\n",
    "The log likelihood is neither a concave function nor a convex function. Tricky, tricky!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3\n",
    "\n",
    "Write Julia function to compute the log-density of the Dirichlet-multinomial distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3 Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_logpdf"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using SpecialFunctions\n",
    "\"\"\"\n",
    "    dirmult_logpdf(x::Vector, α::Vector)\n",
    "    \n",
    "Compute the log-pdf of Dirichlet-multinomial distribution with parameter `α` \n",
    "at data point `x`.\n",
    "\"\"\"\n",
    "function dirmult_logpdf(x::Vector, α::Vector)\n",
    "    term1 = 0.0\n",
    "    Logpdf = 0.0\n",
    "    sumalpha = sum(α)\n",
    "    sumx = sum(x)\n",
    "    for i in 1:length(x)\n",
    "        if α[i] == 0 && x[i] == 0\n",
    "            Logpdf += 0\n",
    "        elseif α[i] == 0 && x[i] > 0\n",
    "            Logpdf += - Inf\n",
    "        else\n",
    "        term1 += lgamma(α[i] + x[i]) - lgamma(α[i]) - lfact(x[i])\n",
    "        Logpdf = term1 + lfact(sumx) + lgamma(sumalpha) - lgamma(sumx + sumalpha)\n",
    "    end\n",
    "    end\n",
    "    return Logpdf\n",
    "end\n",
    "\n",
    "function dirmult_logpdf!(r::Vector, X::Matrix, α::Vector)\n",
    "    for j in 1:size(X, 2)\n",
    "        r[j] = dirmult_logpdf(X[:, j], α)\n",
    "    end\n",
    "    return r\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    dirmult_logpdf(X, α)\n",
    "    \n",
    "Compute the log-pdf of Dirichlet-multinomial distribution with parameter `α` \n",
    "at each data point in `X`. Each column of `X` is one data point.\n",
    "\"\"\"\n",
    "function dirmult_logpdf(X::Matrix, α::Vector)\n",
    "    r = zeros(size(X, 2))\n",
    "    dirmult_logpdf!(r, X, α)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4\n",
    "\n",
    "Read in `optdigits.tra`, the training set of 3823 handwritten digits. Each row contains the 64 counts of a digit and the last element (65th element) indicates what digit it is. For grading purpose, evaluate the total log-likelihood of this data at parameter values $\\alpha=(1,\\ldots,1)$ using your function in Q3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4 Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64×3823 Array{Int64,2}:\n",
       "  0   0   0   0   0   0   0   0   0  …   0   0   0   0   0   0   0   0   0\n",
       "  1   0   0   0   0   0   0   0   0      0   0   1   0   0   0   0   0   0\n",
       "  6  10   8   0   5  11   1   8  15      9   9  10   6   5   0   3   6   2\n",
       " 15  16  15   3  14  16  11  10   2     16  16  16  16  13   1  15  16  15\n",
       " 12   6  16  11   4  10  13   8  14      6  12  16  11  11  12   0   2  16\n",
       "  1   0  13  16   0   1  11   7  13  …   0   1   4   0   2   1   0   0  13\n",
       "  0   0   0   0   0   0   7   2   2      0   0   0   0   0   0   0   0   1\n",
       "  0   0   0   0   0   0   0   0   0      0   0   0   0   0   0   0   0   0\n",
       "  0   0   0   0   0   0   0   0   0      0   0   0   0   0   0   0   0   0\n",
       "  7   7   1   0   0   4   0   1   0      2   3   8   1   2   0   0   0   0\n",
       " 16  16  11   5  13  16   9  15  16  …  14  16  13  16  15   0  11  15   3\n",
       "  6   8   9  16   8  10  14  14  15     16  10  12   2   6  14  14  10   7\n",
       "  6  16  11  11   0  15   6  12  12     16  15  16  12   5  10   0   0  10\n",
       "  ⋮                   ⋮              ⋱       ⋮                   ⋮        \n",
       " 15  10   4  16   4  15  15  14  16      8   4  10   0   1  15   9   2   7\n",
       "  9  15   0   0  11   8   3   5   6      0   9  16   8   8   1  15   8   0\n",
       "  0   3   0   0  12   8   0   0   0      0  16   5   4  10   0   4  15   0\n",
       "  0   0   0   0   0   3   0   0   0  …   0   0   0   0   0   0   0   0   0\n",
       "  0   0   0   0   0   0   0   0   0      0   0   0   0   0   0   0   0   0\n",
       "  0   0   0   0   0   0   0   0   0      0   0   2   1   0   0   0   0   0\n",
       "  6  10   9   0   4  10   1   4  10     10   8  13   7   8   0   4   5   4\n",
       " 14  16  14   1  12  16  13  13  12     13  16  16  14  13   4  14  16  14\n",
       "  7  15   0  15  14  16   5   8   5  …   1  16  12  16  15   9  16  16   1\n",
       "  1   3   0   2   7  16   0   0   0      0  16   5  12  10   0   9  16   0\n",
       "  0   0   0   0   0  16   0   0   0      0   8   0   1   1   0   0   5   0\n",
       "  0   0   0   0   0   6   0   0   0      0   0   0   0   0   0   0   0   0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindata = readcsv(\"optdigits.tra\", Int)\n",
    "digit = traindata[:, end]\n",
    "X = copy(traindata[:, 1:64])\n",
    "counts = traindata[:, 1:64]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3823-element Array{Int64,1}:\n",
       " 0\n",
       " 0\n",
       " 7\n",
       " 4\n",
       " 6\n",
       " 2\n",
       " 5\n",
       " 5\n",
       " 0\n",
       " 8\n",
       " 7\n",
       " 1\n",
       " 9\n",
       " ⋮\n",
       " 1\n",
       " 4\n",
       " 4\n",
       " 8\n",
       " 9\n",
       " 3\n",
       " 9\n",
       " 9\n",
       " 4\n",
       " 6\n",
       " 6\n",
       " 7"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3823-element Array{Float64,1}:\n",
       " -165.188\n",
       " -176.23 \n",
       " -167.774\n",
       " -165.564\n",
       " -157.79 \n",
       " -176.071\n",
       " -158.423\n",
       " -159.258\n",
       " -174.302\n",
       " -178.407\n",
       " -171.294\n",
       " -169.383\n",
       " -175.753\n",
       "    ⋮    \n",
       " -160.49 \n",
       " -158.633\n",
       " -156.935\n",
       " -171.975\n",
       " -177.638\n",
       " -169.735\n",
       " -158.423\n",
       " -159.465\n",
       " -159.877\n",
       " -169.735\n",
       " -173.149\n",
       " -155.411"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eachlogpdf = dirmult_logpdf(counts, ones(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that each observation is independent, so the overall log likelihood is the sum of each of the log pdf's in the `eachlogpdf` vector above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-638817.993292528"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Log_Likelihood = sum(eachlogpdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5\n",
    "\n",
    "Derive the score function $\\nabla L(\\alpha)$, observed information matrix $-d^2L(\\alpha)$, and Fisher information matrix $\\mathbf{E}[-d^2L(\\alpha)]$ for the Dirichlet-multinomial distribution.\n",
    "\n",
    "Comment on why Fisher scoring method is inefficient for computing MLE in this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5 Solution:\n",
    "\n",
    ">Let $\\psi(x) = \\frac{d}{dx}[ \\ln \\Gamma(x)]$ be the digamma function of x.\n",
    "Let $\\psi_1(x) = \\frac{d}{dx}[\\psi(x)]$ be the trigamma function of x.\n",
    "\n",
    ">The Score function, $\\nabla L(\\alpha) = \\frac{d}{d\\alpha_j}L(\\alpha) = 0 + \\sum_{i=1}^n[\\psi(\\alpha_j + x_{ij}) - \\psi(\\alpha_j)] - \\sum_{i=1}^n[\\psi(|\\alpha| + |x_i|) - \\psi(|\\alpha|)]$\n",
    "\n",
    "\n",
    ">The observed information matrix $-d^2L(\\alpha)$ has different forms on the diagonals and on the off diagonals.\n",
    "<br>\n",
    ">On the diagonals (where i = j):\n",
    "$$-\\cfrac{\\partial^2}{\\partial\\alpha_i\\alpha_j}L(\\alpha) = \\sum_{i=1}^n[\\psi_1(\\alpha_j) - \\psi_1(\\alpha_j + x_{ij})] - \\sum_{i=1}^n[\\psi_1(|\\alpha|) - \\psi_1(|\\alpha| + |x_i|)]$$\n",
    "<br>\n",
    "\n",
    "\n",
    ">On the off diagonals (where i $\\ne$ j)\n",
    "$$-\\cfrac{\\partial^2}{\\partial\\alpha_i\\alpha_j}L(\\alpha) = - \\sum_{i=1}^n[\\psi_1(|\\alpha|) - \\psi_1(|\\alpha| + |x_i|)]$$\n",
    "\n",
    ">Now we can see that the observed information matrix has a special structure of a diagonal plus an easy low-rank matrix which is rank 1.\n",
    "$$\n",
    "-\\cfrac{\\partial^2}{\\partial\\alpha_i\\alpha_j}L(\\alpha) = \\mathbf{D} - c\\mathbf{1}\\mathbf{1}^T\n",
    "$$\n",
    "\n",
    "$$\n",
    "d_j = \\sum_{i=1}^n\\Big[\\psi_1(\\alpha_j) - \\psi_1(\\alpha_j + x_{ij})\\Big]\n",
    "$$\n",
    "\n",
    "$$\n",
    "c = \\sum_{i=1}^n\\Big[\\psi_1(|\\alpha|) - \\psi_1(|\\alpha| + |x_i|) \\Big]\n",
    "$$\n",
    "\n",
    "<br>\n",
    ">The Fisher information matrix $\\mathbf{E}[-d^2L(\\alpha)]$ is difficult to compute, which highlights why Fisher scoring method is inefficient for computing MLE in this example. \n",
    "\n",
    "$$\n",
    "-E\\Big[\\cfrac{\\partial^2}{\\partial\\alpha_i\\alpha_j}L(\\alpha) \\Big] = E[\\mathbf{D} - c\\mathbf{1}\\mathbf{1}^T]\n",
    "$$\n",
    "\n",
    "$$\n",
    "E[d_j] = \\sum_{i = 1}^n\\sum_{k=0}^{|x_i|-1}\\frac{P(X_{ij} > k)}{(\\alpha_j + k)^2}\n",
    "$$\n",
    "\n",
    "Where $X_{ij}$ is the $j^{th}$ element of the $i^{th}$ observation of $X$, so it is a beta binomial distribution\n",
    "and you have to evaluate the beta binomial distribution percentile function to get the tail probabilities for each observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6\n",
    "\n",
    "What structure does the observed information matrix possess that can facilitate the evaluation of the Newton direction? Is the observed information matrix always positive definite? What remedy can we take if it fails to be positive definite? (Hint: HW1 Q6.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6 Solution:\n",
    "\n",
    "No, the observed information matrix is not always positive definite. Some remedies we can make to it involve taking the expectation, resulting in swapping out the observed information matrix for the expected information matrix (Fisher Scoring). However, this remedy may not be the best case in this scenario, as taking expectation of the observed information matrix may be difficult. Thus, we use the Sherman Morrison formula from HW 1 to use the special structure of the observed information matrix, a diagonal plus a rank one update. \n",
    "\n",
    ">The observed information matrix has a special structure, a diagonal matrix plus a low rank matrix so we can use the Woodbury formula for the evaluation of the Newton direction. \n",
    "\n",
    ">In the case where the matrix fails to be diagonal we can add a positive matrix $\\eta \\mathbf{I}$, where $\\eta$ > 0 to make it become positive definite.  \n",
    "\n",
    ">From the Sherman-Morrison formula and the determinant formula from HW1:\n",
    "\n",
    "$$\n",
    "det(\\mathbf{D} - c\\mathbf{1}\\mathbf{1}^T) = det(\\mathbf{D})det(\\mathbf{I} + c\\mathbf{1}^T\\mathbf{D}^{-1}\\mathbf{1})\n",
    "$$\n",
    "\n",
    "$$\n",
    "= (\\prod_{i = 1}^d d_j)(1 - c \\sum^d_{j=1}d_j^{-1})\n",
    "$$\n",
    "> In order to have a positive definite matrix, the diagonals and the determinant must be positive! So we check that:\n",
    "$(1 - c \\sum^d_{j=1}d_j^{-1}) > 0$\n",
    "\n",
    "\n",
    "$$\n",
    "1 > c\\sum^d_{j=1}d_j^{-1} \\implies c < \\cfrac{1}{\\sum^d_{j=1}d_j^{-1}}\n",
    "$$\n",
    "\n",
    "\n",
    ">We find that the observed information matrix is positive definite when $c < \\cfrac{1}{\\sum_{i = 1}^d d_j^{-1}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7\n",
    "\n",
    "Discuss how to choose a good starting point. Implement this as the default starting value in your function below. (Hint: Method of moment estimator may furnish a good starting point.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7 Solution:\n",
    "\n",
    "Let $\\mathbf{p} = (p_1,\\ldots, p_d)$ follow a Dirichlet distribution with parameter vector $\\alpha = (\\alpha_1,\\ldots, \\alpha_d)$, $\\alpha_j>0$. Then, we can begin by noting the first and second moments of this distribution. \n",
    "\n",
    ">The first moment of the Dirichlet Distribution is:\n",
    "$$E[P_j] = \\frac{\\alpha_j}{|\\alpha|}, 1 \\le j \\le d$$\n",
    "<br>\n",
    "\n",
    ">The second moment of the Dirichlet Distribution is:\n",
    "$$E[P_j^2] = \\frac{\\alpha_j(\\alpha_j + 1)}{|\\alpha|(|\\alpha| + 1)}, 1 \\le j \\le d$$\n",
    "<br>\n",
    "\n",
    "Since we know $E[P_j] = \\frac{\\alpha_j}{|\\alpha|}, 1 \\le j \\le d,$ we know that $\\alpha_j = E[P_j]*|\\alpha|.$ Now we will use these two moments to first find the value of $\\hat|\\alpha|$ to find a good estimate for the starting value $\\hat\\alpha_j$\n",
    "<br>\n",
    "\n",
    ">Let $Z = \\sum_{j=1}^d \\frac{E[P_j^2]}{E[P_j]},$ then we have:\n",
    "\n",
    "$$Z = \\sum_{j = 1}^d \\cfrac{E(p_j^2)}{E(p_j)} = \\sum_{j = 1}^d \\cfrac{\\frac{\\alpha_j(\\alpha_j + 1)}{|\\alpha|(|\\alpha|+ 1)}}{\\frac{\\alpha_j}{|\\alpha|}} = \\sum_{j=1}^d \\frac{\\alpha_j + 1}{|\\alpha| + 1} = \\frac{|\\alpha| + \\sum_{j=1}^d 1}{|\\alpha| + 1} =  \\cfrac{|\\alpha| + d}{|\\alpha| + 1}$$\n",
    "<br>\n",
    ">Now we want to solve for $|\\alpha|$:\n",
    "<br>\n",
    "$$ Z(|\\alpha| + 1) = |\\alpha| + d$$\n",
    "<br>\n",
    "$$Z|\\alpha| + Z = |\\alpha| + d$$\n",
    "<br>\n",
    "$$Z|\\alpha| - |\\alpha| = d - Z$$\n",
    "<br>\n",
    "$$|\\alpha|(Z - 1) = d - Z$$\n",
    "<br>\n",
    "$$|\\alpha| = \\frac{d - Z}{Z - 1}$$\n",
    "<br>\n",
    "\n",
    "> Recall that we are estimating Z by $\\hat{Z}$:\n",
    "\n",
    "$$\\hat{Z} = \\sum_{j=1}^d\\cfrac{{(\\sum_i^n\\frac{x_{ij}}{|x_i|}})^2}{(\\sum_i^n\\frac{x_{ij}}{|x_i|})}$$\n",
    "<br>\n",
    "\n",
    ">Now we can estimate $\\hat{|\\alpha|}$ using the estimates for $\\hat{Z}$:\n",
    "<br>\n",
    "\n",
    "$$\\hat{|\\alpha|} = \\frac{d - \\hat{Z}}{\\hat{Z} - 1}\n",
    "$$\n",
    "<br>\n",
    "\n",
    "$$ = \\frac{d - \\sum_{j=1}^d\\cfrac{{(\\sum_i^n\\frac{x_{ij}}{|x_i|}})^2}{(\\sum_i^n\\frac{x_{ij}}{|x_i|})}}{\\sum_{j=1}^d\\cfrac{{(\\sum_i^n\\frac{x_{ij}}{|x_i|}})^2}{(\\sum_i^n\\frac{x_{ij}}{|x_i|})} - 1}\n",
    "$$\n",
    "<br>\n",
    "\n",
    ">Now we recall that $E[P_j] = \\frac{\\alpha_j}{|\\alpha|}, 1 \\le j \\le d,$ which means $\\alpha_j = E[P_j]*|\\alpha|.$ Using our method of moments estimate of $\\hat{|\\alpha|}$, we get:\n",
    "\n",
    "$$\\hat{\\alpha_j} = E[P_j]*\\hat{|\\alpha|}\n",
    "$$\n",
    "<br>\n",
    "$$ = \\frac{(\\sum_i^n\\frac{x_{ij}}{|x_i|})}{n}*\\hat{|\\alpha|}\n",
    "$$\n",
    "<br>\n",
    "$$ = \\frac{(\\sum_i^n\\frac{x_{ij}}{|x_i|})}{n}*\\frac{d - \\sum_{j=1}^d\\cfrac{{(\\sum_i^n\\frac{x_{ij}}{|x_i|}})^2}{(\\sum_i^n\\frac{x_{ij}}{|x_i|})}}{\\sum_{j=1}^d\\cfrac{{(\\sum_i^n\\frac{x_{ij}}{|x_i|}})^2}{(\\sum_i^n\\frac{x_{ij}}{|x_i|})} - 1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64-element Array{Float64,1}:\n",
       " 0.0        \n",
       " 0.0714198  \n",
       " 1.30543    \n",
       " 2.83281    \n",
       " 2.75176    \n",
       " 1.32356    \n",
       " 0.336655   \n",
       " 0.0343316  \n",
       " 0.000461199\n",
       " 0.463798   \n",
       " 2.52787    \n",
       " 2.8072     \n",
       " 2.53299    \n",
       " ⋮          \n",
       " 2.33097    \n",
       " 2.20214    \n",
       " 0.894695   \n",
       " 0.0353529  \n",
       " 5.71236e-5 \n",
       " 0.066766   \n",
       " 1.39754    \n",
       " 2.8661     \n",
       " 2.744      \n",
       " 1.59808    \n",
       " 0.506003   \n",
       " 0.0475158  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## code for finding initial alpha\n",
    "function initial_alpha(X::Matrix{Int})\n",
    "    n, d = size(X)    #@show n #64    #@show d #3823\n",
    "    alpha0 = zeros(d) #creates 64x1 matrix\n",
    "    tolerance = 0.0001\n",
    "    \n",
    "    Z_hat = 0\n",
    "    Z_numerator = 0\n",
    "       \n",
    "    sumcol = sum(X,1)\n",
    "    sumrow = sum(X,2)\n",
    "    \n",
    "    for j in 1:d #each row   \n",
    "    Z_numerator = 0 #reset each value\n",
    "        \n",
    "        if sumcol[j] < tolerance #check to see if there is data in the column\n",
    "            continue\n",
    "        end\n",
    "        \n",
    "        for i in 1:n #for each column\n",
    "            Z_numerator += (X[i,j] / sumrow[i])^2 #numerator in the k equation\n",
    "            alpha0[j] += (X[i,j] / sumrow[i]) #denominator in the k equation, and second part of the equation for aj0\n",
    "        end\n",
    "        \n",
    "        Z_hat += (Z_numerator / alpha0[j]) #calculating khat\n",
    "        alpha0[j] /= n #divide by n in front of the sigma\n",
    "\n",
    "    end\n",
    "    alpha0 *= ((d - Z_hat) / (Z_hat - 1))\n",
    "    \n",
    "    return alpha0\n",
    "    \n",
    "end\n",
    "\n",
    "alpha0 = initial_alpha(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_newton (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function dirmult_newton(X) \n",
    "    maxiters = 100\n",
    "    tolfun = 1e-6\n",
    "    orign, origd = size(X)\n",
    "    #first reduce to where the obs are 0\n",
    "    nonzeroobs = find(sum(X, 1) .> 0.01) \n",
    "    #remove those rows in X\n",
    "    X = copy(X[:, nonzeroobs])\n",
    "    Xt = copy(X.')\n",
    "    # set default starting point from Q7\n",
    "    α0 = initial_alpha(X)\n",
    "    rowsumX = sum(X, 2)\n",
    "    n, d = size(X)\n",
    "    dga = zeros(d)\n",
    "    score = zeros(d)\n",
    "    αhat = copy(α0)\n",
    "    ndir = zeros(d)\n",
    "    triarow = 0.0\n",
    "    logl = 0.0\n",
    "    loglold = sum(dirmult_logpdf(Xt, αhat))\n",
    "    # Newton loop\n",
    "    for iter in 1:maxiters\n",
    "        rowsumα = sum(αhat)\n",
    "        score = zeros(d)       \n",
    "        # evaluate gradient (score)\n",
    "        dga = digamma.(αhat)\n",
    "        dgarow = digamma.(rowsumα)\n",
    "        for j in 1:d\n",
    "            for i in 1:n\n",
    "                score[j] += digamma.(αhat[j] + X[i, j]) - dga[j] - digamma.(rowsumα + rowsumX[i]) + dgarow\n",
    "            end\n",
    "        end\n",
    "        # approximated observed information matrix\n",
    "\n",
    "        D = zeros(d)\n",
    "        c = 0.0\n",
    "        triga = trigamma.(αhat)\n",
    "        for j = 1:d\n",
    "            for i = 1:n\n",
    "                D[j] += -trigamma.(αhat[j] + X[i, j]) + triga[j]\n",
    "                c += -trigamma.(rowsumα + rowsumX[i]) + triarow\n",
    "            end\n",
    "        end\n",
    "\n",
    "        #need to make sure that the condition for c holds for it to be p.d.\n",
    "        invD = (1 ./ D)\n",
    "        if !(c < (1 / (sum(invD))))\n",
    "            c = (1 / sum(invD)) * 0.95\n",
    "        end\n",
    "\n",
    "        # compute Newton's direction\n",
    "        #use Sherman-Morrison inverse formula\n",
    "        #first part is invD * score\n",
    "        #second is 1 / (1 - c*sum(Dinv)) * Dinv' * Dinv * c\n",
    "\n",
    "        for j in 1:d\n",
    "            ndir[j] = invD[j] * score[j]\n",
    "        end\n",
    "        ndir += (((1/c - sum(invD)) * (invD * invD')) * score)\n",
    "\n",
    "        # line search loop\n",
    "        step = 1\n",
    "        for i in 1:d\n",
    "            if ndir[i] < 0\n",
    "                step = min(αhat[i] / abs(ndir[i]) * 0.95, step)\n",
    "            end\n",
    "        end        \n",
    "        αupdate = 0.0\n",
    "        logl = 0.0\n",
    "        loglupdate = 0.0\n",
    "        for lsiter in 1:10\n",
    "            αupdate = αhat + step * ndir\n",
    "            loglupdate = sum(dirmult_logpdf(Xt, αupdate))\n",
    "            # step halving\n",
    "            if (loglupdate < loglold)\n",
    "                step = step / 2\n",
    "            end\n",
    "        end\n",
    "        αhat = copy(αupdate)\n",
    "        loglold = copy(logl)\n",
    "        logl = copy(loglupdate)\n",
    "\n",
    "        # check convergence criterion\n",
    "        if abs(logl - loglold) < (tolfun * (abs(loglold) + 1))\n",
    "            break;\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # compute logl, gradient, Hessian from final iterate \n",
    "\n",
    "    #logl already computed. \n",
    "    rowsumα = sum(αhat)\n",
    "    score = zeros(d)       \n",
    "    # evaluate gradient (score)\n",
    "    dga = digamma.(αhat)\n",
    "    dgarow = digamma.(rowsumα)\n",
    "    for j in 1:d\n",
    "        for i in 1:n\n",
    "            score[j] += digamma.(αhat[j] + X[i, j]) - dga[j] - digamma.(rowsumα + rowsumX[i]) + dgarow\n",
    "        end\n",
    "    end\n",
    "\n",
    "    #evaluating the observed info matrix D is the diagonal and c is the off constant. \n",
    "    D = zeros(d)\n",
    "    c = 0.0\n",
    "    triga = trigamma.(αhat)\n",
    "    triarow = trigamma.(rowsumα)\n",
    "    for j = 1:d\n",
    "        for i = 1:n\n",
    "            D[j] += -trigamma.(αhat[j] + X[i, j]) + triga[j]\n",
    "            c += -trigamma.(rowsumα + rowsumX[i]) + triarow\n",
    "        end\n",
    "    end\n",
    "\n",
    "    Hessian = -1 * (Diagonal(D) + (c * ones(d, d)))\n",
    "    \n",
    "    αestimate = zeros(origd)\n",
    "    αestimate[nonzeroobs] = αhat[1:d]\n",
    "    return αestimate, logl, Hessian\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.0, 0.0903296, 1.20097, 2.90755, 2.79236, 0.996218, 0.0886892, 0.00691079, 0.00046394, 0.374372  …  0.539707, 0.0326407, 9.76968e-5, 0.0765621, 1.23843, 2.92678, 2.70497, 1.27362, 0.19604, 0.0296197], -474620.026854726, [-75969.2 -3000.75 … -3000.75 -3000.75; -3000.75 -6217.96 … -3000.75 -3000.75; … ; -3000.75 -3000.75 … -37920.8 -3000.75; -3000.75 -3000.75 … -3000.75 -2.39119e5])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "αestimate_out, logl_out, Hessian_out = dirmult_newton(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_newton_MLEnLogl (generic function with 1 method)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function dirmult_newton_MLEnLogl(X) \n",
    "    maxiters = 100\n",
    "    tolfun = 1e-6\n",
    "    orign, origd = size(X)\n",
    "    #first reduce to where the obs are 0\n",
    "    nonzeroobs = find(sum(X, 1) .> 0.01) \n",
    "    #remove those rows in X\n",
    "    X = copy(X[:, nonzeroobs])\n",
    "    Xt = copy(X.')\n",
    "    # set default starting point from Q7\n",
    "    α0 = initial_alpha(X)\n",
    "    rowsumX = sum(X, 2)\n",
    "    n, d = size(X)\n",
    "    dga = zeros(d)\n",
    "    score = zeros(d)\n",
    "    αhat = copy(α0)\n",
    "    ndir = zeros(d)\n",
    "    triarow = 0.0\n",
    "    logl = 0.0\n",
    "    loglold = sum(dirmult_logpdf(Xt, αhat))\n",
    "    # Newton loop\n",
    "    for iter in 1:maxiters\n",
    "        rowsumα = sum(αhat)\n",
    "        score = zeros(d)       \n",
    "        # evaluate gradient (score)\n",
    "        dga = digamma.(αhat)\n",
    "        dgarow = digamma.(rowsumα)\n",
    "        for j in 1:d\n",
    "            for i in 1:n\n",
    "                score[j] += digamma.(αhat[j] + X[i, j]) - dga[j] - digamma.(rowsumα + rowsumX[i]) + dgarow\n",
    "            end\n",
    "        end\n",
    "        # approximated observed information matrix\n",
    "\n",
    "        D = zeros(d)\n",
    "        c = 0.0\n",
    "        triga = trigamma.(αhat)\n",
    "        for j = 1:d\n",
    "            for i = 1:n\n",
    "                D[j] += -trigamma.(αhat[j] + X[i, j]) + triga[j]\n",
    "                c += -trigamma.(rowsumα + rowsumX[i]) + triarow\n",
    "            end\n",
    "        end\n",
    "\n",
    "        #need to make sure that the condition for c holds for it to be p.d.\n",
    "        invD = (1 ./ D)\n",
    "        if !(c < (1 / (sum(invD))))\n",
    "            c = (1 / sum(invD)) * 0.95\n",
    "        end\n",
    "\n",
    "        # compute Newton's direction\n",
    "        #use Sherman-Morrison inverse formula\n",
    "        #first part is invD * score\n",
    "        #second is 1 / (1 - c*sum(Dinv)) * Dinv' * Dinv * c\n",
    "\n",
    "        for j in 1:d\n",
    "            ndir[j] = invD[j] * score[j]\n",
    "        end\n",
    "        ndir += (((1/c - sum(invD)) * (invD * invD')) * score)\n",
    "\n",
    "        # line search loop\n",
    "        step = 1\n",
    "        for i in 1:d\n",
    "            if ndir[i] < 0\n",
    "                step = min(- αhat[i] / ndir[i] * 0.95, step)\n",
    "            end\n",
    "        end\n",
    "        αupdate = 0.0\n",
    "        logl = 0.0\n",
    "        loglupdate = 0.0\n",
    "        for lsiter in 1:10\n",
    "            αupdate = αhat + step * ndir\n",
    "            loglupdate = sum(dirmult_logpdf(Xt, αupdate))\n",
    "            # step halving\n",
    "            if (loglupdate < loglold)\n",
    "                step = step / 2\n",
    "            end\n",
    "        end\n",
    "        αhat = copy(αupdate)\n",
    "        loglold = copy(logl)\n",
    "        logl = copy(loglupdate)\n",
    "\n",
    "        # check convergence criterion\n",
    "        if abs(logl - loglold) < (tolfun * (abs(loglold) + 1))\n",
    "            break;\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # compute logl, gradient, Hessian from final iterate \n",
    "\n",
    "    #logl already computed. \n",
    "    rowsumα = sum(αhat)\n",
    "    score = zeros(d)       \n",
    "    # evaluate gradient (score)\n",
    "    dga = digamma.(αhat)\n",
    "    dgarow = digamma.(rowsumα)\n",
    "    for j in 1:d\n",
    "        for i in 1:n\n",
    "            score[j] += digamma.(αhat[j] + X[i, j]) - dga[j] - digamma.(rowsumα + rowsumX[i]) + dgarow\n",
    "        end\n",
    "    end\n",
    "\n",
    "    #evaluating the observed info matrix D is the diagonal and c is the off constant. \n",
    "    D = zeros(d)\n",
    "    c = 0.0\n",
    "    triga = trigamma.(αhat)\n",
    "    triarow = trigamma.(rowsumα)\n",
    "    for j = 1:d\n",
    "        for i = 1:n\n",
    "            D[j] += -trigamma.(αhat[j] + X[i, j]) + triga[j]\n",
    "            c += -trigamma.(rowsumα + rowsumX[i]) + triarow\n",
    "        end\n",
    "    end\n",
    "\n",
    "\n",
    "    Hessian = -1 * (Diagonal(D) + (c * ones(d, d)))\n",
    "    \n",
    "    αestimate = zeros(origd)\n",
    "    αestimate[nonzeroobs] = αhat[1:d]\n",
    "    return αestimate, logl, Hessian\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62×62 Array{Float64,2}:\n",
       " -75969.2   -3000.75  -3000.75  …  -3000.75   -3000.75  -3000.75     \n",
       "  -3000.75  -6217.96  -3000.75     -3000.75   -3000.75  -3000.75     \n",
       "  -3000.75  -3000.75  -4230.45     -3000.75   -3000.75  -3000.75     \n",
       "  -3000.75  -3000.75  -3000.75     -3000.75   -3000.75  -3000.75     \n",
       "  -3000.75  -3000.75  -3000.75     -3000.75   -3000.75  -3000.75     \n",
       "  -3000.75  -3000.75  -3000.75  …  -3000.75   -3000.75  -3000.75     \n",
       "  -3000.75  -3000.75  -3000.75     -3000.75   -3000.75  -3000.75     \n",
       "  -3000.75  -3000.75  -3000.75     -3000.75   -3000.75  -3000.75     \n",
       "  -3000.75  -3000.75  -3000.75     -3000.75   -3000.75  -3000.75     \n",
       "  -3000.75  -3000.75  -3000.75     -3000.75   -3000.75  -3000.75     \n",
       "  -3000.75  -3000.75  -3000.75  …  -3000.75   -3000.75  -3000.75     \n",
       "  -3000.75  -3000.75  -3000.75     -3000.75   -3000.75  -3000.75     \n",
       "  -3000.75  -3000.75  -3000.75     -3000.75   -3000.75  -3000.75     \n",
       "      ⋮                         ⋱                 ⋮                  \n",
       "  -3000.75  -3000.75  -3000.75  …  -3000.75   -3000.75  -3000.75     \n",
       "  -3000.75  -3000.75  -3000.75     -3000.75   -3000.75  -3000.75     \n",
       "  -3000.75  -3000.75  -3000.75     -3000.75   -3000.75  -3000.75     \n",
       "  -3000.75  -3000.75  -3000.75     -3000.75   -3000.75  -3000.75     \n",
       "  -3000.75  -3000.75  -3000.75     -3000.75   -3000.75  -3000.75     \n",
       "  -3000.75  -3000.75  -3000.75  …  -3000.75   -3000.75  -3000.75     \n",
       "  -3000.75  -3000.75  -3000.75     -3000.75   -3000.75  -3000.75     \n",
       "  -3000.75  -3000.75  -3000.75     -3000.75   -3000.75  -3000.75     \n",
       "  -3000.75  -3000.75  -3000.75     -3000.75   -3000.75  -3000.75     \n",
       "  -3000.75  -3000.75  -3000.75     -5851.62   -3000.75  -3000.75     \n",
       "  -3000.75  -3000.75  -3000.75  …  -3000.75  -37920.8   -3000.75     \n",
       "  -3000.75  -3000.75  -3000.75     -3000.75   -3000.75     -2.39119e5"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.0, 0.0903296, 1.20097, 2.90755, 2.79236, 0.996218, 0.0886892, 0.00691079, 0.00046394, 0.374372  …  0.539707, 0.0326407, 9.76968e-5, 0.0765621, 1.23843, 2.92678, 2.70497, 1.27362, 0.19604, 0.0296197], -474620.026854726, [-75969.2 -3000.75 … -3000.75 -3000.75; -3000.75 -6217.96 … -3000.75 -3000.75; … ; -3000.75 -3000.75 … -37920.8 -3000.75; -3000.75 -3000.75 … -3000.75 -2.39119e5])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "αestimate_input, logl_input, hessi_input = dirmult_newton_MLEnLogl(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64-element Array{Float64,1}:\n",
       " 0.0       \n",
       " 0.0903296 \n",
       " 1.20097   \n",
       " 2.90755   \n",
       " 2.79236   \n",
       " 0.996218  \n",
       " 0.0886892 \n",
       " 0.00691079\n",
       " 0.00046394\n",
       " 0.374372  \n",
       " 2.4307    \n",
       " 2.94382   \n",
       " 2.57634   \n",
       " ⋮         \n",
       " 2.28646   \n",
       " 1.9535    \n",
       " 0.539707  \n",
       " 0.0326407 \n",
       " 9.76968e-5\n",
       " 0.0765621 \n",
       " 1.23843   \n",
       " 2.92678   \n",
       " 2.70497   \n",
       " 1.27362   \n",
       " 0.19604   \n",
       " 0.0296197 "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "αestimate_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-474620.026854726"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logl_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62×62 Array{Float64,2}:\n",
       " -75969.2   -3000.75  -3000.75  …  -3000.75   -3000.75  -3000.75     \n",
       "  -3000.75  -6217.96  -3000.75     -3000.75   -3000.75  -3000.75     \n",
       "  -3000.75  -3000.75  -4230.45     -3000.75   -3000.75  -3000.75     \n",
       "  -3000.75  -3000.75  -3000.75     -3000.75   -3000.75  -3000.75     \n",
       "  -3000.75  -3000.75  -3000.75     -3000.75   -3000.75  -3000.75     \n",
       "  -3000.75  -3000.75  -3000.75  …  -3000.75   -3000.75  -3000.75     \n",
       "  -3000.75  -3000.75  -3000.75     -3000.75   -3000.75  -3000.75     \n",
       "  -3000.75  -3000.75  -3000.75     -3000.75   -3000.75  -3000.75     \n",
       "  -3000.75  -3000.75  -3000.75     -3000.75   -3000.75  -3000.75     \n",
       "  -3000.75  -3000.75  -3000.75     -3000.75   -3000.75  -3000.75     \n",
       "  -3000.75  -3000.75  -3000.75  …  -3000.75   -3000.75  -3000.75     \n",
       "  -3000.75  -3000.75  -3000.75     -3000.75   -3000.75  -3000.75     \n",
       "  -3000.75  -3000.75  -3000.75     -3000.75   -3000.75  -3000.75     \n",
       "      ⋮                         ⋱                 ⋮                  \n",
       "  -3000.75  -3000.75  -3000.75  …  -3000.75   -3000.75  -3000.75     \n",
       "  -3000.75  -3000.75  -3000.75     -3000.75   -3000.75  -3000.75     \n",
       "  -3000.75  -3000.75  -3000.75     -3000.75   -3000.75  -3000.75     \n",
       "  -3000.75  -3000.75  -3000.75     -3000.75   -3000.75  -3000.75     \n",
       "  -3000.75  -3000.75  -3000.75     -3000.75   -3000.75  -3000.75     \n",
       "  -3000.75  -3000.75  -3000.75  …  -3000.75   -3000.75  -3000.75     \n",
       "  -3000.75  -3000.75  -3000.75     -3000.75   -3000.75  -3000.75     \n",
       "  -3000.75  -3000.75  -3000.75     -3000.75   -3000.75  -3000.75     \n",
       "  -3000.75  -3000.75  -3000.75     -3000.75   -3000.75  -3000.75     \n",
       "  -3000.75  -3000.75  -3000.75     -5851.62   -3000.75  -3000.75     \n",
       "  -3000.75  -3000.75  -3000.75  …  -3000.75  -37920.8   -3000.75     \n",
       "  -3000.75  -3000.75  -3000.75     -3000.75   -3000.75     -2.39119e5"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hessi_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q8\n",
    "\n",
    "Write a function for finding MLE of Dirichlet-multinomial distribution given iid observations $\\mathbf{x}_1,\\ldots,\\mathbf{x}_n$, using the Newton's method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8.Solution:\n",
    "\n",
    "I wrote a function for all 3, since this takes long make it 3 separate functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Distributions\n",
    "α_MLE = zeros(64, 10)\n",
    "logl_dir_mult = zeros(10)\n",
    "logl_mult = zeros(10)\n",
    "LRT_pvalue = zeros(10)\n",
    "\n",
    "for digit in 0:9\n",
    "data = X[find(traindata[:, end] .== digit), :]\n",
    "    p = vec(sum(data, 1))\n",
    "    p = p / sum(p)\n",
    "    posind = p .> 0\n",
    "    α_MLE[:, digit + 1], logl_dir_mult[digit + 1] = dirmult_newton_MLEnLogl(data)\n",
    "    for j in 1:size(data, 1)\n",
    "        logl_mult[digit + 1] += logpdf(Multinomial(Int(sum(data[j, posind])), \n",
    "                p[posind]), data[j, posind])\n",
    "        end\n",
    "    # compute LRT p-value\n",
    "    LRT_pvalue[digit + 1] = ccdf(Chisq(1), logl_dir_mult[digit + 1] - logl_mult[digit + 1])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q9\n",
    "\n",
    "Read in `optdigits.tra`, the training set of 3823 handwritten digits. Find the MLE for the subset of digit 0, digit 1, ..., and digit 9 separately using your function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9.Solution:\n",
    "\n",
    "Each column represents each digit from 0-9. For each digit, there are 64 estimated paramaters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64×10 Array{Float64,2}:\n",
       " 0.0        0.0         0.0         …  0.0          0.0         0.0       \n",
       " 0.018732   0.00462795  0.322827       0.114388     0.0698143   0.0609866 \n",
       " 2.22827    0.548807    3.13647        2.01391      2.10357     1.51798   \n",
       " 6.45112    2.2561      4.29164        3.97315      4.85509     3.66409   \n",
       " 5.2467     3.05446     2.06893        4.28656      4.65403     3.6936    \n",
       " 1.08438    1.43627     0.224179    …  3.57434      2.04556     1.46169   \n",
       " 0.0329209  0.139595    0.00501409     1.54337      0.173982    0.133596  \n",
       " 0.0        0.0         0.0            0.178033     0.0         0.0155583 \n",
       " 0.0        0.0         0.0            0.0          0.00024313  0.0       \n",
       " 0.482175   0.0562337   1.44383        0.234165     0.877006    0.662042  \n",
       " 6.3123     1.1265      4.49919     …  2.55557      5.08191     3.97826   \n",
       " 6.38582    3.33312     3.89515        3.12412      3.80947     3.31473   \n",
       " 5.94014    4.1051      3.64313        3.27458      3.41989     3.25473   \n",
       " ⋮                                  ⋱                                     \n",
       " 5.04933    4.01082     3.60406        1.34781      2.81098     2.20389   \n",
       " 6.60007    2.40554     3.15816        0.0534397    3.36903     2.59681   \n",
       " 1.12812    0.318198    1.96141        0.000979039  0.607385    0.929526  \n",
       " 0.0        0.0350717   0.116394    …  0.0          0.0         0.0154181 \n",
       " 0.0        0.0         0.00113888     0.0          0.0         0.0       \n",
       " 0.0106639  0.0117462   0.303087       0.0946951    0.0589708   0.0460625 \n",
       " 2.24813    0.547242    3.19427        2.32339      2.17067     1.45407   \n",
       " 6.67086    2.16389     4.30827        3.2859       5.13294     3.72952   \n",
       " 6.53627    3.49831     4.04383     …  0.413727     4.75836     3.52857   \n",
       " 2.51182    2.32981     3.70697        0.00193899   2.26226     1.55626   \n",
       " 0.104017   0.477514    2.48339        0.0          0.256143    0.310965  \n",
       " 0.0        0.00785456  0.286976       0.0          0.00592048  0.00698623"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindata = readcsv(\"optdigits.tra\", Int)\n",
    "α_MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q10\n",
    "\n",
    "As $\\alpha / |\\alpha| \\to \\mathbf{p}$, the Dirichlet-multinomial distribution converges to a multinomial with parameter $\\mathbf{p}$. Therefore multinomial can be considered as a special Dirichlet-multinomial with $|\\alpha|=\\infty$. Perform a likelihood ratio test (LRT) whether Dirichlet-multinomial offers a better fit than multinomial for digits 0, 1, ..., 9 respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q10 Solution:\n",
    "\n",
    "Dirichlet Multinomial is better for all digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Array{Float64,1}:\n",
       " -39592.2\n",
       " -54039.2\n",
       " -49111.5\n",
       " -47089.1\n",
       " -57344.1\n",
       " -51713.0\n",
       " -42597.3\n",
       " -49473.0\n",
       " -49695.9\n",
       " -54577.8"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logl_mult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Array{Float64,1}:\n",
       " -38070.0\n",
       " -42232.7\n",
       " -40094.0\n",
       " -40678.8\n",
       " -43505.9\n",
       " -41351.5\n",
       " -37810.7\n",
       " -40406.2\n",
       " -43181.5\n",
       " -43772.5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logl_dir_mult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Array{Float64,1}:\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LRT_pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q11\n",
    "\n",
    "Now we can construct a simple Bayesian rule for handwritten digits recognition:\n",
    "$$\n",
    "\t\\mathbf{x}\t\\mapsto \\arg \\max_k \\widehat \\pi_k f(x|\\widehat \\alpha_k).\n",
    "$$\n",
    "Here we can use the proportion of digit $k$ in the training set as the prior probability $\\widehat \\pi_k$. Report the performance of your classifier on the test set of 1797 digits in `optdigits.tes`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q11.Solution:\n",
    "\n",
    "test set to get the prior probability distribution for p and then multiply each DMloglikelihood to get posterior distribution of each digit, plug in new image to this posterior to do classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11741791875347801"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the training and test data\n",
    "train_data = readcsv(\"optdigits.tra\", Int)\n",
    "test_data = readcsv(\"optdigits.tes\", Int)\n",
    "\n",
    "#first we pre-allocate\n",
    "p_hat = zeros(10)\n",
    "n1 = size(train, 1)\n",
    "n2 = size(test, 1)\n",
    "probs = zeros( 10, size(test_data, 1))\n",
    "\n",
    "# find prior probabilities for each digit in the training set\n",
    "for digit in 0:9\n",
    "    p_hat[digit + 1] += sum(train_data[:, 65] .== digit) / n1\n",
    "end\n",
    "\n",
    "#find the maximum logpdf\n",
    "for digit in 0:9\n",
    "    probs[digit + 1, :] = dirmult_logpdf(test_data[:, 1:64]', α_MLE[:, digit + 1])\n",
    "end\n",
    "\n",
    "# multiply the prior with the logpdf to get the posterior distribution\n",
    "for j in 1:10\n",
    "    probs[j,:] *= p_hat[j]\n",
    "end\n",
    "\n",
    "# predict which digit, based on which has max log likelihood\n",
    "predicted = zeros(n2)\n",
    "for i in 1:n2\n",
    "    predicted[i] = (findmax(probs[:,i],1)[2][1] - 1)\n",
    "end\n",
    "\n",
    "# get the % of error\n",
    "sum(test_data[:, 65] .!= predicted) / length(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Array{Float64,1}:\n",
       " 0.0983521\n",
       " 0.101753 \n",
       " 0.0993984\n",
       " 0.101753 \n",
       " 0.101229 \n",
       " 0.0983521\n",
       " 0.0986137\n",
       " 0.101229 \n",
       " 0.0993984\n",
       " 0.0999215"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×1797 Array{Float64,2}:\n",
       "  -9.36015  -17.7985   -17.659   -15.5212  …  -15.8654  -13.5023   -16.5712\n",
       " -17.0878    -9.2009   -12.1829  -12.612      -11.1107  -13.7413   -14.3545\n",
       " -18.2424   -13.0749   -13.5765  -12.0016     -13.8073  -16.0182   -13.8368\n",
       " -16.6902   -13.6322   -16.0671   -9.466      -14.1068  -13.1395   -13.9056\n",
       " -14.2769   -11.6118   -14.2116  -15.2448     -12.5603  -14.1944   -15.7532\n",
       " -14.0002   -12.414    -15.8191  -11.0459  …  -13.4411  -12.771    -14.9051\n",
       " -16.6013   -14.6962   -15.0941  -13.9899     -14.2648  -16.2053   -14.3884\n",
       " -16.3802   -14.1013  -Inf       -16.2947     -14.5148  -16.4374  -Inf     \n",
       " -14.717    -11.4841   -12.6017  -12.4372     -10.5821  -12.8851   -12.4407\n",
       " -13.4498   -13.0982   -15.7582  -11.2021     -13.4321  -10.7974   -14.7992"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "65px",
    "width": "252px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
