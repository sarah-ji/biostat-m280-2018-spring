{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biostat M280 Homework 3\n",
    "\n",
    "Sarah Ji\n",
    "\n",
    "**Due Friday, May 25 @ 11:59PM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1 - Big $n$ regression\n",
    "\n",
    "Those who took my _203B: Introduction to Data Science_ last quarter had a (painful) experience of wrangling an Apache Spark cluster to do linear regression on a dataset with more than 100 million observations. Now we learnt various methods for solving linear regression and should realize that, with right choice of algorithm, it is a problem that can be handled by any moderate computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1(1)\n",
    "\n",
    "Download the flight data from <http://stat-computing.org/dataexpo/2009/the-data.html>. For this exercise, we only need data from years 2003-2008. If you are using Mac or Linux, you can run the following Bash script, which downloads and unzips files for all years.\n",
    "```bash\n",
    "# Download flight data by year\n",
    "for i in {1987..2008}\n",
    "  do\n",
    "    echo \"$(date) $i Download\"\n",
    "    fnam=$i.csv.bz2\n",
    "    wget -O ./$fnam http://stat-computing.org/dataexpo/2009/$fnam\n",
    "    echo \"$(date) $i unzip\"\n",
    "    bzip2 -d ./$fnam\n",
    "  done\n",
    "\n",
    "# Download airline carrier data\n",
    "wget -O ./airlines.csv http://www.transtats.bts.gov/Download_Lookup.asp?Lookup=L_UNIQUE_CARRIERS\n",
    "\n",
    "# Download airports data\n",
    "wget -O ./airports.csv https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat\n",
    "```\n",
    "Find out how many data points in each year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1(1) Solution:\n",
    "\n",
    "Below, we use the `countlines()` function in Julia to see how many data points there are in each year. We find that there is a steady increase in the number of data points from years 2003-2007, but a sharp decrease in data points from 2007 to 2008. We suspect this drop in 2008 may be related to the national airport shutdowns following the 9/11 attacks this year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "countlines(\"allyears/2003.csv\") = 6488541\n",
      "countlines(\"allyears/2004.csv\") = 7129271\n",
      "countlines(\"allyears/2005.csv\") = 7140597\n",
      "countlines(\"allyears/2006.csv\") = 7141923\n",
      "countlines(\"allyears/2007.csv\") = 7453216\n",
      "countlines(\"allyears/2008.csv\") = 7009729\n"
     ]
    }
   ],
   "source": [
    "# how many data points in 2003\n",
    "@show countlines(\"allyears/2003.csv\")\n",
    "# how many data points in 2004\n",
    "@show countlines(\"allyears/2004.csv\")\n",
    "# how many data points in 2005\n",
    "@show countlines(\"allyears/2005.csv\")\n",
    "# how many data points in 2006\n",
    "@show countlines(\"allyears/2006.csv\")\n",
    "# how many data points in 2007\n",
    "@show countlines(\"allyears/2007.csv\")\n",
    "# how many data points in 2008\n",
    "@show countlines(\"allyears/2008.csv\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1(2) \n",
    "\n",
    "We are interested in how the time gain of a flight, defined as `DepDelay - ArrDelay`, depends on the distance traveled (`Distance`), departure delay (`DepDelay`), and carrier (`UniqueCarrier`). \n",
    "\n",
    "We want to fit a linear regression `Gain ~ 1 + Distance + DepDelay + UniqueCarrier` using data from 2003-2008. Note `UniqueCarrier` is a factor with 23 levels: \"9E\", \"AA\", \"AQ\", \"AS\", \"B6\", \"CO\", \"DH\", \"DL\", \"EV\", \"F9\", \"FL\", \"HA\", \"HP\", \"MQ\", \"NW\", \"OH\", \"OO\", \"TZ\", \"UA\", \"US\", \"WN\", \"XE\", and \"YV\". We use the dummy coding with \"9E\" as base level.\n",
    "\n",
    "Will the design matrix (in double precision) fit into the memory of you computer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1(2) Solution:\n",
    "\n",
    "First I checked the dimension of the design matrix we need for the regression. Since our model has an intercept, we have 5 columns and counting the number of rows in each year we have 42,363,271 rows. So our design matrix is a 42,363,271 by 5 matrix, and it can fit on the memory of my computer (but I prefer it didn't)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If each number is stored in double precicion, we need 8,629,998,000 bytes or 8.629998 GiB of memory to store this matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8629998000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculating the number of bytes this data will need\n",
    "41490375*26*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144.277378 seconds (388.15 M allocations: 17.700 GiB, 3.52% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Table with 41490375 rows, 4 columns:\n",
       "DepDelay  ArrDelay  UniqueCarrier  Distance\n",
       "───────────────────────────────────────────\n",
       "-4        -1        \"UA\"           837\n",
       "-1        -3        \"UA\"           837\n",
       "29        23        \"UA\"           837\n",
       "-2        -9        \"UA\"           1835\n",
       "18        52        \"UA\"           1835\n",
       "-4        6         \"UA\"           1835\n",
       "-4        -8        \"UA\"           1835\n",
       "0         2         \"UA\"           1835\n",
       "-4        19        \"UA\"           1835\n",
       "3         4         \"UA\"           413\n",
       "-4        -23       \"UA\"           413\n",
       "-3        -19       \"UA\"           413\n",
       "⋮\n",
       "57        75        \"DL\"           481\n",
       "80        99        \"DL\"           689\n",
       "-2        15        \"DL\"           270\n",
       "-4        6         \"DL\"           425\n",
       "-3        16        \"DL\"           546\n",
       "-1        2         \"DL\"           215\n",
       "3         14        \"DL\"           533\n",
       "-1        -2        \"DL\"           874\n",
       "-5        0         \"DL\"           545\n",
       "11        9         \"DL\"           533\n",
       "7         -5        \"DL\"           874"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data from csv\n",
    "using JuliaDB\n",
    "@time yrtable = loadtable(\n",
    "    \"allyears\", \n",
    "    datacols = [\"DepDelay\", \"ArrDelay\", \"UniqueCarrier\", \"Distance\"])\n",
    "# drop rows with missing values\n",
    "yrtable = dropna(yrtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we drop missing values, I count again how many bytes this design matrix will use. Even after dropping rows with missing values, we will use 8.62999696 GiB of memory.\n",
    "\n",
    "Below is the function that maps from variable names to X columns and generates the [X y] matrix. We will use this function to get our design matrix `xy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generate_xy (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mapping from variable names to X columns\n",
    "# carrier \"9E\" is used as base level\n",
    "const var2col = Dict(\n",
    "        \"Intercept\" => 1,\n",
    "        \"Distance\" => 2,\n",
    "        \"DepDelay\" => 3,\n",
    "        \"AA\" => 4,\n",
    "        \"AQ\" => 5,\n",
    "        \"AS\" => 6,\n",
    "        \"B6\" => 7,\n",
    "        \"CO\" => 8,\n",
    "        \"DH\" => 9,\n",
    "        \"DL\" => 10,\n",
    "        \"EV\" => 11,\n",
    "        \"F9\" => 12,\n",
    "        \"FL\" => 13,\n",
    "        \"HA\" => 14,\n",
    "        \"HP\" => 15,\n",
    "        \"MQ\" => 16,\n",
    "        \"NW\" => 17,\n",
    "        \"OH\" => 18,\n",
    "        \"OO\" => 19,\n",
    "        \"TZ\" => 20,\n",
    "        \"UA\" => 21,\n",
    "        \"US\" => 22,\n",
    "        \"WN\" => 23,\n",
    "        \"XE\" => 24,\n",
    "        \"YV\" => 25,\n",
    "        \"Gain\" => 26)\n",
    "# mapping from column to variable names\n",
    "const col2var = map(reverse, var2col)\n",
    "\n",
    "# a custom function to generate [X y] from data table\n",
    "function generate_xy(tbl::NextTable)\n",
    "    # X matrix\n",
    "    XY = zeros(length(tbl), 26)\n",
    "    # intercept term\n",
    "    @views fill!(XY[:, 1], 1)\n",
    "    # Distance term\n",
    "    @views copy!(XY[:, 2], columns(tbl, :Distance))\n",
    "    # DepDelay term\n",
    "    @views copy!(XY[:, 3], columns(tbl, :DepDelay))\n",
    "    # Dummy coding for airline\n",
    "    @inbounds for i in 1:length(tbl)\n",
    "        yrtable[i][:UniqueCarrier] == \"9E\" && continue # base level\n",
    "        XY[i, var2col[tbl[i][:UniqueCarrier]]] = 1\n",
    "    end\n",
    "    # last column is response: gain = depdelay - arrdelay\n",
    "    XY[:, 26] = select(tbl, \n",
    "        (:DepDelay, :ArrDelay) => p -> Float64(p.DepDelay - p.ArrDelay))\n",
    "    # return\n",
    "    XY\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41490375×26 Array{Float64,2}:\n",
       " 1.0   837.0  -4.0  0.0  0.0  0.0  …  0.0  1.0  0.0  0.0  0.0  0.0   -3.0\n",
       " 1.0   837.0  -1.0  0.0  0.0  0.0     0.0  1.0  0.0  0.0  0.0  0.0    2.0\n",
       " 1.0   837.0  29.0  0.0  0.0  0.0     0.0  1.0  0.0  0.0  0.0  0.0    6.0\n",
       " 1.0  1835.0  -2.0  0.0  0.0  0.0     0.0  1.0  0.0  0.0  0.0  0.0    7.0\n",
       " 1.0  1835.0  18.0  0.0  0.0  0.0     0.0  1.0  0.0  0.0  0.0  0.0  -34.0\n",
       " 1.0  1835.0  -4.0  0.0  0.0  0.0  …  0.0  1.0  0.0  0.0  0.0  0.0  -10.0\n",
       " 1.0  1835.0  -4.0  0.0  0.0  0.0     0.0  1.0  0.0  0.0  0.0  0.0    4.0\n",
       " 1.0  1835.0   0.0  0.0  0.0  0.0     0.0  1.0  0.0  0.0  0.0  0.0   -2.0\n",
       " 1.0  1835.0  -4.0  0.0  0.0  0.0     0.0  1.0  0.0  0.0  0.0  0.0  -23.0\n",
       " 1.0   413.0   3.0  0.0  0.0  0.0     0.0  1.0  0.0  0.0  0.0  0.0   -1.0\n",
       " 1.0   413.0  -4.0  0.0  0.0  0.0  …  0.0  1.0  0.0  0.0  0.0  0.0   19.0\n",
       " 1.0   413.0  -3.0  0.0  0.0  0.0     0.0  1.0  0.0  0.0  0.0  0.0   16.0\n",
       " 1.0   413.0   0.0  0.0  0.0  0.0     0.0  1.0  0.0  0.0  0.0  0.0   12.0\n",
       " ⋮                            ⋮    ⋱       ⋮                          ⋮  \n",
       " 1.0   515.0   0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0    5.0\n",
       " 1.0   481.0  57.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  -18.0\n",
       " 1.0   689.0  80.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  -19.0\n",
       " 1.0   270.0  -2.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  -17.0\n",
       " 1.0   425.0  -4.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  -10.0\n",
       " 1.0   546.0  -3.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  -19.0\n",
       " 1.0   215.0  -1.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0   -3.0\n",
       " 1.0   533.0   3.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  -11.0\n",
       " 1.0   874.0  -1.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0    1.0\n",
       " 1.0   545.0  -5.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0   -5.0\n",
       " 1.0   533.0  11.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0    2.0\n",
       " 1.0   874.0   7.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0   12.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy = generate_xy(yrtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1(3)\n",
    "\n",
    "Review the [Summary of Linear Regression](http://hua-zhou.github.io/teaching/biostatm280-2018spring/slides/12-linreg/linreg.html) and devise a strategy to solve the linear regression.\n",
    "\n",
    "Report the estimated regression coefficients $\\widehat \\beta$, estimated variance $\\widehat \\sigma^2 = \\sum_i (y_i - \\widehat y_i)^2 / (n - 1)$, and coefficient standard errors.\n",
    "\n",
    "Hint: It took my laptop less than 3 minutes to import data and fit linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1(3) Solution:\n",
    "\n",
    "Notice from above that it takes us 144.322472 seconds to read in the flights datasets from years 2003-2008.\n",
    "\n",
    "Next, we want to fit the linear regression for the big data problem using Sweep Operators to give us the desired quantities. Sweep Operators are popular in statistical softwares, namely SAS, for performing linear regression and matrix inversion.\n",
    "\n",
    "To use Sweeep Operators, the matrix that you are sweeping must be symmetric and positive semidefinite. Therefore we will perform Sweep on the symmetric and postive definite ram matrix, $[\\mathbf{{X y}^{T}}][\\mathbf{{X y}]},$ to get the desired quantities from the function `linreg_sweep_all`.\n",
    "\n",
    "$$\n",
    "[\\mathbf{{X y}^{T}}][\\mathbf{{X y}]} = \n",
    "\\begin{pmatrix} \n",
    "    \\mathbf{X}^T \\mathbf{X} & \\mathbf{X}^T \\mathbf{y} \\\\ \n",
    "    \\mathbf{y}^T \\mathbf{X} & \\mathbf{y}^T \\mathbf{y} \n",
    "\\end{pmatrix}\n",
    "$$ \n",
    "\n",
    "calling sweep yields the solution matrix with the desired quantities:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\begin{pmatrix}\n",
    "- (\\mathbf{X}^T \\mathbf{X})^{-1} & (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y} \\\\\n",
    "\\mathbf{y}^T \\mathbf{X} (\\mathbf{X}^T \\mathbf{X})^{-1} & \\mathbf{y}^T \\mathbf{y} - \\mathbf{y}^T \\mathbf{X} (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\n",
    "\\end{pmatrix} = \n",
    "\\begin{pmatrix}\n",
    "- \\sigma^{-2} \\text{Cov}(\\beta) & \\beta \\\\\n",
    "\\beta^T & \\|\\mathbf{y} - \\hat y\\|_2^2\n",
    "\\end{pmatrix}.\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "We notice that the estimated regression coefficients $\\widehat \\beta$ can be found in the last column of the solution matrix.\n",
    "\n",
    "> calling `beta_coeff_est = tableau[1:p, end]` in the function to get this quantity\n",
    "\n",
    "Additionally, the estimated variance $\\widehat \\sigma^2 = \\sum_i (y_i - \\widehat y_i)^2 / (n - 1),$ can be computed from dividing the residual sum of squares in the last cell of the solution matrix by (n - 1)\n",
    " \n",
    "> calling `sigma_hat2 = tableau[end, end] / (n - 1)` in the function to get this quantity\n",
    "\n",
    "The coefficient standard errors $\\text{Cov}(\\beta)$ \n",
    "> calling `SE_beta_coeff_est = sqrt.(Diagonal( .- sigma_hat2 .* tableau[1:p, 1:p]))` in the function to get this quantity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "linreg_sweep_all (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using SweepOperator, BenchmarkTools\n",
    "\n",
    "function linreg_sweep_all(y::Vector, X::Matrix)\n",
    "    n = size(X, 1)\n",
    "    p = size(X, 2)\n",
    "    tableau = [X y]' * [X y]\n",
    "    sweep!(tableau, 1:p)\n",
    "    beta_coeff_est = tableau[1:p, end]\n",
    "    sigma_hat2 = tableau[end, end] / (n - 1) \n",
    "    SE_beta_coeff_est = sqrt.(Diagonal( .- sigma_hat2 .* tableau[1:p, 1:p]))\n",
    "    return beta_coeff_est, sigma_hat2, SE_beta_coeff_est\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_coeff_est, sigma_hat2, SE_beta_coeff_est = linreg_sweep_all(xy[:, 26], xy[:, 1:25]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\widehat \\beta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25-element Array{Float64,1}:\n",
       "  1.14033   \n",
       "  0.00164935\n",
       " -0.0118811 \n",
       " -1.8723    \n",
       " -0.5789    \n",
       " -0.938452  \n",
       " -1.42247   \n",
       " -2.57627   \n",
       "  1.16808   \n",
       " -2.19625   \n",
       "  1.03932   \n",
       " -2.15207   \n",
       " -1.35247   \n",
       " -1.87248   \n",
       " -0.350758  \n",
       " -1.46395   \n",
       " -3.62506   \n",
       " -0.00722279\n",
       " -0.40365   \n",
       " -3.5774    \n",
       " -1.14816   \n",
       " -0.883753  \n",
       "  2.74855   \n",
       " -2.56721   \n",
       " -0.202211  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_coeff_est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\widehat \\sigma^2 = \\sum_i (y_i - \\widehat y_i)^2 / (n - 1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204.4031074256946"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigma_hat2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\text{Cov}(\\beta)$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25×25 Array{Float64,2}:\n",
       " 0.0202318  0.0         0.0         …  0.0        0.0        0.0      \n",
       " 0.0        4.37958e-6  0.0            0.0        0.0        0.0      \n",
       " 0.0        0.0         6.88878e-5     0.0        0.0        0.0      \n",
       " 0.0        0.0         0.0            0.0        0.0        0.0      \n",
       " 0.0        0.0         0.0            0.0        0.0        0.0      \n",
       " 0.0        0.0         0.0         …  0.0        0.0        0.0      \n",
       " 0.0        0.0         0.0            0.0        0.0        0.0      \n",
       " 0.0        0.0         0.0            0.0        0.0        0.0      \n",
       " 0.0        0.0         0.0            0.0        0.0        0.0      \n",
       " 0.0        0.0         0.0            0.0        0.0        0.0      \n",
       " 0.0        0.0         0.0         …  0.0        0.0        0.0      \n",
       " 0.0        0.0         0.0            0.0        0.0        0.0      \n",
       " 0.0        0.0         0.0            0.0        0.0        0.0      \n",
       " 0.0        0.0         0.0            0.0        0.0        0.0      \n",
       " 0.0        0.0         0.0            0.0        0.0        0.0      \n",
       " 0.0        0.0         0.0         …  0.0        0.0        0.0      \n",
       " 0.0        0.0         0.0            0.0        0.0        0.0      \n",
       " 0.0        0.0         0.0            0.0        0.0        0.0      \n",
       " 0.0        0.0         0.0            0.0        0.0        0.0      \n",
       " 0.0        0.0         0.0            0.0        0.0        0.0      \n",
       " 0.0        0.0         0.0         …  0.0        0.0        0.0      \n",
       " 0.0        0.0         0.0            0.0        0.0        0.0      \n",
       " 0.0        0.0         0.0            0.0209201  0.0        0.0      \n",
       " 0.0        0.0         0.0            0.0        0.0222383  0.0      \n",
       " 0.0        0.0         0.0            0.0        0.0        0.0255715"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full(SE_beta_coeff_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1(4)\n",
    "\n",
    "Go to your resume/cv and claim you have experience performing analytics on data with hundred millions of observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1(4) Solution:\n",
    "\n",
    "\n",
    "I have edited my resume/cv and claimed my experience performing analysis on data with hundred millions of observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2 - Google PageRank\n",
    "\n",
    "We are going to try different numerical methods learnt in class on the [Google PageRank problem](https://en.wikipedia.org/wiki/PageRank)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2(1)\n",
    "\n",
    "Let $\\mathbf{A} \\in \\{0,1\\}^{n \\times n}$ be the connectivity matrix of $n$ web pages with entries\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\ta_{ij}= \\begin{cases}\n",
    "\t1 & \\text{if page $i$ links to page $j$} \\\\\n",
    "\t0 & \\text{otherwise}\n",
    "\t\\end{cases}.\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "$r_i = \\sum_j a_{ij}$ is the out-degree of page $i$. That is $r_i$ is the number of links on page $i$. Imagine a random surfer exploring the space of $n$ pages according to the following rules.  \n",
    "\n",
    "- From a page $i$ with $r_i>0$\n",
    "    * with probability $p$, (s)he randomly chooses a link on page $i$ (uniformly) and follows that link to the next page  \n",
    "    * with probability $1-p$, (s)he randomly chooses one page from the set of all $n$ pages (uniformly) and proceeds to that page  \n",
    "- From a page $i$ with $r_i=0$ (a dangling page), (s)he randomly chooses one page from the set of all $n$ pages (uniformly) and proceeds to that page  \n",
    "    \n",
    "The process defines a Markov chain on the space of $n$ pages. Write down the transition matrix $\\mathbf{P}$ of the Markov chain as a diagonal matrix plus rank-1 matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2(1) Solution:\n",
    "\n",
    ">Since with probability $p$, (s)he randomly chooses a link on page $i$ (uniformly) and follows that link to the next page, if there are ${\\sum_j a_{ij}}$ pages the probability of going to each one would be $\\frac{1}{{\\sum_j a_{ij}}}$.<br> \n",
    "\n",
    ">Additionally, we are given: if there are no links on the current page, with probability $1-p$, (s)he randomly chooses one page from the set of all $n$ pages (uniformly) and proceeds to that page. If there are n total pages in the world, the probability of going to each one would be $1/n$.<br><br>\n",
    "\n",
    ">We note that the transition probability of going from page i to page j is:\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "    p_{ij} = \\begin{cases}\n",
    "    \\frac{1}{\\sum_j a_{ij}}*p + \\frac{1}{n}*(1 - p) & \\text{if page $i$ has links on the page, $r_i$ > 0} \\\\\n",
    "    \\frac{1}{n} & \\text{if there are no links on the page $i$, $r_i$ = 0}\n",
    "    \\end{cases}\n",
    "\\end{eqnarray*}\n",
    "$$<br>\n",
    "> Replacing $r_i = \\sum_j a_{ij}$ we have:\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "    p_{ij} = \\begin{cases}\n",
    "    \\frac{a_{ij}}{r_i}*p + \\frac{1}{n}*(1 - p) & \\text{if page $i$ has links on the page, $r_i$ > 0} \\\\\n",
    "    \\frac{1}{n} & \\text{if there are no links on the page $i$, $r_i$ = 0}\n",
    "    \\end{cases}\n",
    "\\end{eqnarray*}\n",
    "$$<br>\n",
    "\n",
    ">We make note that the structure of transition matrix $P$ is the sum of two terms, a diagonal matrix + a rank-1 matrix, <br> where $\\mathbf{\\frac{1}{n}} = \\frac{1}{n}\\mathbf{1^T}$ and $\\mathbf{\\frac{1-p}{n}} = \\frac{1-p}{n}\\mathbf{1^T}$ are vectors. Then the resulting Transition Probability Matrix is:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "    \\mathbf{P} = \\begin{cases}\n",
    "     diag(\\frac{p}{r_i})\\mathbf{A} + \\mathbf{\\frac{1-p}{n}}\\mathbf{1} & \\text{if page $i$ has links on the page, $r_i$ > 0} \\\\\n",
    "    diag(0)\\mathbf{A} + \\mathbf{\\frac{1}{n}}\\mathbf{1} & \\text{if there are no links on the page $i$, $r_i$ = 0}\n",
    "    \\end{cases}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Q2(2)\n",
    "\n",
    "According to standard Markov chain theory, the (random) position of the surfer converges to the stationary distribution $\\mathbf{x} = (x_1,\\ldots,x_n)^T$ of the Markov chain. $x_i$ has the natural interpretation of the proportion of times the surfer visits page $i$ in the long run. Therefore $\\mathbf{x}$ serves as page ranks: a higher $x_i$ means page $i$ is more visited. It is well-known that $\\mathbf{x}$ is the left eigenvector corresponding to the top eigenvalue 1 of the transition matrix $\\mathbf{P}$. That is $\\mathbf{P}^T \\mathbf{x} = \\mathbf{x}$. Therefore $\\mathbf{x}$ can be solved as an eigen-problem. Show that it can also be cast as solving a linear system. Since the row sums of $\\mathbf{P}$ are 1, $\\mathbf{P}$ is rank deficient. We can replace the first equation by the $\\sum_{i=1}^n x_i = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2(2) Solution:\n",
    "\n",
    ">First we have:\n",
    "$$\\mathbf{P}^T \\mathbf{x} = \\mathbf{x}$$<br>\n",
    ">Moving everything to the left side we have:\n",
    "$$\\mathbf{P}^T \\mathbf{x} - \\mathbf{x} = \\mathbf{0}$$<br>\n",
    ">We can re-write this as:\n",
    "$$(\\mathbf{I} - \\mathbf{P}^T) \\mathbf{x} = \\mathbf{0}$$<br>\n",
    ">Now, we see that this problem can also be cast as solving a linear system. What we want to find is the solution `x`.\n",
    ">If we let $\\mathbf{C} = (\\mathbf{I} - \\mathbf{P}^T),$ we know $\\mathbf{C}$ is not full rank. Since the row sums of $\\mathbf{P}$ are 1, $\\mathbf{P}$ is rank deficient and we know each column of $\\mathbf{C}$ will also sum to 1.<br><br>\n",
    ">Thus, we can replace the first equation by the $\\sum_{i=1}^n x_i = 1$ by replacing the first row of $\\mathbf{C}$ with a vector of ones, and create a new vector for the solution, where the first postition is a 1 (indicating we changed the 1st row of matrix $\\mathbf{C}$ to ones) and the rest are zeros. <br><br>\n",
    ">Note that we could choose any of the n rows of matrix $\\mathbf{C}$ to be ones, adjusting the corresponding solution vector with a 1 in the position of the row number that was changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2(3)\n",
    "\n",
    "Download the [`ucla.zip`](http://hua-zhou.github.io/teaching/biostatm280-2018spring/hw/hw3/ucla.zip) package from course webpage. Unzip the package, which contains two files `U.txt` and `A.txt`. `U.txt` lists the 500 URL names. `A.txt` is the $500 \\times 500$ connectivity matrix. Read data into Julia. Compute summary statistics:\n",
    "* number of pages\n",
    "* number of edges\n",
    "* number of dangling nodes (pages with no out links)\n",
    "* which page has max in-degree?\n",
    "* which page has max out-degree?\n",
    "* visualize the sparsity pattern of $\\mathbf{A}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2(3) Solution:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_pages = size(A, 1) = 500\n",
      "n_edges = countnz(A) = 10853\n",
      "n_dangling_nodes = n_pages - countnz(r) = 103\n"
     ]
    }
   ],
   "source": [
    "# connectivity matrix\n",
    "A = readcsv(\"A.txt\")\n",
    "# U = readcsv(\"U.txt\")\n",
    "U = readlines(\"U.txt\")\n",
    "# Make diagonals 0 so that probability of going from page i to page i is 0 !\n",
    "A = A - Diagonal(A)\n",
    "# Let Julia know A is a sparse matrix since most entries are 0's!\n",
    "A = sparse(A);\n",
    "# number of pages\n",
    "@show n_pages = size(A, 1)\n",
    "# number of edges (page links)\n",
    "@show n_edges = countnz(A)\n",
    "# number of dangling nodes (pages with no out links)\n",
    "r = sum(A, 2) #number of outlinks per page\n",
    "@show n_dangling_nodes = n_pages - countnz(r); # total number of pages minus those with out links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are `n_pages = 500` pages, there are `n_edges = 10,853` edges, and there are `n_dangling_nodes = 103` dangling nodes (pages with no out links) in the connectivity matrix in `A.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max in/out-degree\n",
    "\n",
    "Next we find which page has the max in-degree and which page has the max out-degree. The in-degree of page `i` is defined to be the number of pages that link to page `i`. Similarly, the out-degree of page `i` is defined to be the number of page links on page `i`. Thus, we are interested in which page has the most traffic coming in, and which page has the most traffic going out by calculing the maximum in/out degree.\n",
    "\n",
    "We see that the page that has the most traffic coming in is \"http://www.ucla.edu\", with 171 pages linking in to it. \n",
    "We see that the page that has the most traffic going out is \"http://giveto.ucla.edu\", with 87 pages linking in to it. Makes total sense to me lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "which_max_nidgl = indmax(n_in_degree_links) = 1\n",
      "max_in_degree_link = U[which_max_nidgl] = \"http://www.ucla.edu\"\n",
      "max_n_in_degree_links = n_in_degree_links[which_max_nidgl] = 171.0\n"
     ]
    }
   ],
   "source": [
    "# which page has max in-degree?\n",
    "n_in_degree_links = sum(A, 1)\n",
    "@show which_max_nidgl = indmax(n_in_degree_links) #indicates first link has the maximum # of in-degree links\n",
    "@show max_in_degree_link = U[which_max_nidgl] #print the first link in URL file\n",
    "@show max_n_in_degree_links = n_in_degree_links[which_max_nidgl]; # number of in-degree links to this page is 171!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U[which_max_nodgl] = \"http://giveto.ucla.edu\"\n",
      "r[which_max_nodgl] = 87.0\n"
     ]
    }
   ],
   "source": [
    "# which page has max out-degree?\n",
    "which_max_nodgl = indmax(r)\n",
    "@show U[which_max_nodgl]\n",
    "@show r[which_max_nodgl];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Sparsity Pattern of Connectivity Matrix\n",
    "\n",
    "Below, we use the spy() function in the `Plots.jl` package to visualize the sparsity of the connectivity matrix A in `A.txt`. The shaded cells of the figure below represent where A[i,j] = 1 and there exists a link from page `i` to `j`. The unshaded cells show where A[i, j] = 0 and there does not exists a link from page `i` to `j`.\n",
    "\n",
    "Looking across the graph, each row `i` without any shading indicate pages where there are no out-links on page `i`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 600 400\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip5100\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"600\" height=\"400\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polygon clip-path=\"url(#clip5100)\" points=\"\n",
       "0,400 600,400 600,0 0,0 \n",
       "  \" fill=\"#ffffff\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip5101\">\n",
       "    <rect x=\"120\" y=\"0\" width=\"421\" height=\"400\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polygon clip-path=\"url(#clip5100)\" points=\"\n",
       "126.039,375.869 490.096,375.869 490.096,11.811 126.039,11.811 \n",
       "  \" fill=\"#ffffff\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip5102\">\n",
       "    <rect x=\"126\" y=\"11\" width=\"365\" height=\"365\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip5100)\" style=\"stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none\" points=\"\n",
       "  126.039,375.869 490.096,375.869 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5100)\" style=\"stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none\" points=\"\n",
       "  126.039,11.811 126.039,375.869 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5100)\" style=\"stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none\" points=\"\n",
       "  198.486,375.869 198.486,370.408 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5100)\" style=\"stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none\" points=\"\n",
       "  271.298,375.869 271.298,370.408 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5100)\" style=\"stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none\" points=\"\n",
       "  344.109,375.869 344.109,370.408 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5100)\" style=\"stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none\" points=\"\n",
       "  416.921,375.869 416.921,370.408 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5100)\" style=\"stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none\" points=\"\n",
       "  489.732,375.869 489.732,370.408 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5100)\" style=\"stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none\" points=\"\n",
       "  126.039,84.2585 131.499,84.2585 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5100)\" style=\"stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none\" points=\"\n",
       "  126.039,157.07 131.499,157.07 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5100)\" style=\"stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none\" points=\"\n",
       "  126.039,229.882 131.499,229.882 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5100)\" style=\"stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none\" points=\"\n",
       "  126.039,302.693 131.499,302.693 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5100)\" style=\"stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none\" points=\"\n",
       "  126.039,375.505 131.499,375.505 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip5100)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:12; text-anchor:middle;\" transform=\"rotate(0, 198.486, 389.669)\" x=\"198.486\" y=\"389.669\">100</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5100)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:12; text-anchor:middle;\" transform=\"rotate(0, 271.298, 389.669)\" x=\"271.298\" y=\"389.669\">200</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5100)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:12; text-anchor:middle;\" transform=\"rotate(0, 344.109, 389.669)\" x=\"344.109\" y=\"389.669\">300</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5100)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:12; text-anchor:middle;\" transform=\"rotate(0, 416.921, 389.669)\" x=\"416.921\" y=\"389.669\">400</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5100)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:12; text-anchor:middle;\" transform=\"rotate(0, 489.732, 389.669)\" x=\"489.732\" y=\"389.669\">500</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5100)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:12; text-anchor:end;\" transform=\"rotate(0, 120.039, 88.7585)\" x=\"120.039\" y=\"88.7585\">100</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5100)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:12; text-anchor:end;\" transform=\"rotate(0, 120.039, 161.57)\" x=\"120.039\" y=\"161.57\">200</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5100)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:12; text-anchor:end;\" transform=\"rotate(0, 120.039, 234.382)\" x=\"120.039\" y=\"234.382\">300</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5100)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:12; text-anchor:end;\" transform=\"rotate(0, 120.039, 307.193)\" x=\"120.039\" y=\"307.193\">400</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5100)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:12; text-anchor:end;\" transform=\"rotate(0, 120.039, 380.005)\" x=\"120.039\" y=\"380.005\">500</text>\n",
       "</g>\n",
       "<polyline clip-path=\"url(#clip5102)\" style=\"stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none\" points=\"\n",
       "  126.039,11.811 490.096,11.811 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5102)\" style=\"stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none\" points=\"\n",
       "  490.096,11.811 490.096,375.869 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip5102)\">\n",
       "<image width=\"364\" height=\"364\" xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAWwAAAFsCAYAAADon4O5AAAQH0lEQVR4nO3dP4wc130H8HcCAUqd\n",
       "uygdK4UlK0OlEYc5XnDnY3OAADeMLSeyEMdn5QoBgX0C3KgkgsD/GCBXumSZuCKDGHARIEICBCpp\n",
       "0jQpyZK3vELApNjb2+Vwdnd25997bz4f4HCLvb2d2Zk333n7mzcz4byYuv/05ODjH7xxODkN4fi9\n",
       "Nw6Pn54cTE5DuP/05OD9v357b3IawuQ0hN/cerg7e/xvT08un3/rPye7Z/8x2Z2chnD+3Zt7d9+9\n",
       "uTc5DeHNH7xxeOcf9w5mr3v0+dHh5DSEoiiKG996+2D2OIQQXr342+3Pjw5/+dNf7U9OQ7j7i//a\n",
       "ff7+1fn0v/yTy/e6dWc+X+dFUdz+3XSeb/16svvo19N5+cofjg5v/2H6vpPTEO589+bl/9z9/Ojy\n",
       "8bWnJ3vne9PPdvevHu5ev5jO2Wu/2X/r+I3L/3/9X+/tzx5/uPC+H/zu5ODBxfTPi6KY/O9rh5PT\n",
       "EN7/7OjwwWfzzxwuTE5DmDyZL7/wzvSzFEVRPPq/1w5n6+XsycnB3SfT9/3oycnB82/Ml//z788/\n",
       "y63ThWX02dHenU+nn+28KIqzv5zP17XvXb2cl/unVy/n/6O/mS/LcHey++XF9EMI4frj6Xw+eHyy\n",
       "N5uvyWkIv/zR1ReW/+z586Iobjy5cjg5DeHLhc/8ZenzH//T5PKzfPCnjy6X6+3v39z72cX83/js\n",
       "aO/6z6fL/MHP7+2/+aOrl+v/g0/ny//1B/P3eueTo73wfPr/b+0/3J3sT5fZ+adHh1+7WBevfuft\n",
       "vbPvzJf57b8Nl8vl7OE3L6cx+cW9/dlnO/73Pzs8ezxdlo8enxzc/+3J5euu/3C+LJ5/Mp+vD793\n",
       "c+/a30/X0+s/vHp4/Bef7s+W0dcez///o6+/d/n47Pn8/yd/d3Pv+o/Pp5//x+f7H96bLotr/3Jv\n",
       "//x8vi28+fyocl387OFk98ZsuyyK4tGji2k+Pzq88cnL7bLcRq994+Hu+aPp+r/725O9xXZx+V4X\n",
       "63n2/OLjN/84b+N3nh0dnj2bTvOdT44O719Mv9wu7v/DfFne/e/pvBdFURz/z62Dxff++M+n6/X4\n",
       "6w933/nn+Xb5evHVeRv544vzOJvOx8+ODq89r57+jZ9M36soimLy3nx7Of7i5IXpv/rF9LOdfXGy\n",
       "9/G7bx9ULf/zd+fb1QvT/9bNveNvT9vFrd8fHd79/XydHz+br8uPvpjP/1eeHR0GADZWNPz7VtO6\n",
       "0uKbAoxNEULYqfGaVqb1SktvBEA3dkK7PXaAUSlKv/uYVtDDBtjeunLIMlsFvcAG2F6THvbG/yuw\n",
       "Aba3bQ973f9WhnmTicWmztFagDbM8mZZ7iw+v0lP2gFGgJY56AiQmDa/1a8Nf4ENEIe14S+wAbbX\n",
       "a71ZYAO0r5MgF9gA7Wta2zZSBKAlTQN1k/83SgSgBdsG97rx1pV/E9gAiRDYAABkq8+Dgg5AAjTQ\n",
       "5y3CAGigz2uJANDAIEHtoCPA9pYFtzMdARLRybX5cwpstSQgazkFNkDWBDYAANkySqQhNWygb3IH\n",
       "ABieHgls6ZVgAwKA0bDTh550sbHZgAEgQnbQwEpCAhiV2Y0g27xQySxIO7n4CVGr2olqB5CQrnrC\n",
       "fd5mHiAKXQeXYKRr2hgAEJc2eidFxU/X04QQXm5L2hYAMKyhj+CXR6hs2jsaev7ZXNujkmA0Yry8\n",
       "qo05b9YvdRWlx7GOOOtNjIENsImmgZtMJ0JgA6mrG7jJ9KSXEdjAWCTTk15myA9QdQq7g47ApkZz\n",
       "IHvIHvYoFjAvSf5rKY10sf5HkyUxlkRGs/BHyvodt03Wf9ujRJLvLCiJACQixpKIEAZC6G8cdjJi\n",
       "LImMfqVkzvqlrp3S49F35oYMbBvuOI1+o6M32WWMkgiQi3LZJLssibEkArCt2W0PsySwgVxkHdYh\n",
       "CGwgH1mHdQghXBl6Bipkv9BHbjSnEdO77NuVUSL0LfuNit6NJkuURICY1Qnj0XQCYhzWR95G0xui\n",
       "FXJigZIIfbMB0oZR5oeSCJCiUe74YyyJjHJFAC8pX/xp9PSwgRRkf1IMABkZuvxQPonCDQzy58QZ\n",
       "AMiAGxWsEMMtwprQUwMAAAASt2mJRJ0L8mF7rsE4bCAGxlnXILCBWBhEUEPTvdq600eLmq9rMv1V\n",
       "P21PD2AQetgAiRDYAIkQ2ACJGLrIX3UtkU2OFg89/0B3FvNhlglNtnnXsQFokWuJAJCHNvZmy4bT\n",
       "dTlNCOHltqRtka1ZvbiPuk4M9aNYN+Zyna7Oa4GRuTL0DABRa9LJ6bNDOAqG9QFdcG2QDghsoG3C\n",
       "uiMCG2iTsO6QGnZcNHZis67+XFWjVrPuiMCOi7AmNnXa5LozlAV4SwR2XPSwiU3VpSPqvJYOqGED\n",
       "ddUpj9AhgQ20RQ+7YwIbIBECGyARAhsgEQIbIBECGyARAhuoq6jxGAAAAAAAAAAAAAAAopHy5RDX\n",
       "XUy96/+HTfVxgkndNj2bl3U3zbCNRGasZymN9XMDzQ2SHynvPfWwSU1MPexFetiJcC0RyF/VdT98\n",
       "wyQpGiyQlJTvmq6kQWqGKokUK/62+Pe67wcb00MGRiXlHnZTeuj0bciDjqvaux52Ihx0hHEQvBkQ\n",
       "2JAnJcMMCWzIkx51hgQ2QCIENoxTEZRNkjPmUSKQm03KIEomCdLDhjysu+reuv+d/YQgzKOlhw3p\n",
       "axLWVe+1+JuICGzoTh8nZ7UR1k6cSYSSCHQn9rB20DExAhvS1EbPWu85MQIb0tNmzZqECGxIi7Ae\n",
       "MYEN6egqrA3rS4RRIpCGLnvW5ffVg4+UHjbEr+vRIHrYidDDhrhtE9bl8d/rAlgPOxF62BCvbXvW\n",
       "dXrILv6UIIFNnwREfV3VrBdvyKv0kRiBTZ8ERD1dHmBcXAd2oIkR2BCXPsdZ24EmRmBDPJwUw0oC\n",
       "G+IgrFmrr8DWEGE5YU0tfQW2WhlUE9bUpiQCwxHWAAAAAAAAK/RxIMXBGvpUdUU7bRAAAAAAAAAG\n",
       "4EAdANAbHQ8AAGBAvpIAo+I61fCiTTsCdbehYoPXQq+cmk5uqk5NhyxozIyBdg4AAADUpIwAI2TD\n",
       "JyXaK9HSOAEAAAAAAAAAYlOUfq97bdU1Fap+6kxz3XNUq7OM+7Rt2+mCtkXW2mzMTQJ7m2kB8bOt\n",
       "tmQntHvZx9mKcRlJhrRJQMzaqrbL6NTdUOxxAQDIk5IIOarzDW6x7Wu3JOGVoWcABrAY0ouU6oia\n",
       "wGZsloU1RE9gMybCmqQJbHK3s/C7KD1frlmrYRM1gU3uygcWF5/X2yYpApvcKYOQjb4Ce5PrlZCn\n",
       "IXq0wpqsXOlpOjul34zPEOteWJMVJRFi00bI6hgAAMMpX6msi/cGoGVNQnvTGxgwPsWSxynL5XOQ\n",
       "CDVs+rKz5HHKcvkcJCTVXkKq8w0QBSEKAABd9Iq3eU+983Qtnsnq4DM5Kbftxed619eZjnU4gJOu\n",
       "OuvOXV1IUdVZ2oO14cUJN7lVWNUex4YJ0CLD+gASIbABEiGwARIhsAESIbABEiGwARIhsAES0fc9\n",
       "HQEAAAAi0Map6cvKHU5NB2iRg470ZchbhNW5yprjLETPxZ8ApppkYC92QvOZXAzrneAymmO2SVvq\n",
       "sker7QEAbEPNEQA6YidLDrRjAAAAgP60NfzJOGwAAAAAACBGxqUCAAAAwNaKoMS2lBsYACSk6d5s\n",
       "3Z1Equ720eYedHGPXFT8tD09gEHoYQMAAAAjtWl9t/z6WGrYy37anh7AILapYbsK3zgYXgWRuTL0\n",
       "DBAtO2YAgG3V/dq77HVq2PlQBgE6IVgAAIhTnZ7qqtcoiaTP8oFEbBrYdcO7a0KGGW2BUdgJ08be\n",
       "ZAjX4v9XvdfsuXWvazL9Vdr4jDFqK6RyWy6QLeOw0yVoYWRcrQ8AAID+OKBJVHK544xhfUDW1LAB\n",
       "EiGw2YRvKjAggc0mDCUEAKjDQUeABCiJACRCYAMAlZSnoAE1bIAEKIkAJGLocbXl61QvXju7jqHn\n",
       "H+hO+Rr6IdjmlUQAUuAGBmlz1xkYEYGdNkELI+KgIwAAADBCsyF0fdRC+5rOunmIUXloY9XfDWuC\n",
       "kWu7hh1rIKZsk3Hp27LeAADaoiQSh6qSyLKe9dDLEBiIYX1x6qMMAgBAF5RE4rBulMiy10LsYtju\n",
       "szGmkkisYQ05E9YAAAAAAAAAALAR47DjYBw2qYlhex6dMY3DHprGTS5mYV2nAxRrJylJArsfdRq3\n",
       "hk0qdkq/67yWFgjs7tXtiWjYwEoCu1uuuge0RmB3Z5uwFu7kRptukcDuxrY9a2URcqNNt0hgt08Z\n",
       "hNxUtWdtfAACu11NwtoGQKyqesmz57TbHgns9jTtWfvqSIq02x4J7HYogzBWxZLHdEBgNyesGbOd\n",
       "JY/pgMBuRlgzZtp+zwT29oQ1Y6dH3TOBvR1hzdho7xEQ2JsT1oyR3nQEBPZmugxrOwFSpN32SGDX\n",
       "13XPWg+GVCxuB9ptjwR2PcogMLcspG0jHRPY6wlrqEdvu2MCezVhDURDYC8nrIGoCOxqwhqIjsB+\n",
       "mbAGoiSwXzR0WNtRkBttukUCe27osJ7NA+REm26RwJ6KIawBVhLYwhq2VQTbDgAAAAD5UOsCACB/\n",
       "sxESfYyV7Gs66+YhZnVGrAy9DAE2Fnv48jLrDBpquhEVSx6Xn1v3um2nve6nzekBDCb1E2fWlQeU\n",
       "D7rZUQIDSD2wV3EG49TOksdAYnINbGENZCfHwBbWQJZyC2xhDWRLTRNgc4N0DBcDu8mJLVUzb2fA\n",
       "ohhOnIJVNglhbRlgIEmUUts4NX3x/6vea/bcutc1mX4d9ogAAHRv6F5nuae92Btfpc+LVgHDKH8r\n",
       "D6H9bb6r9+1EisP6DN0DFm2bB7MdQhJhHUJ6gS2sgbK6gVu+rk4yQT0z5AxXfRVZFcZVYZ3cAgda\n",
       "t0n4JhnUM0P2sJcttKrn9axhfMo94mUZUCeAZ/+fbFiHkEZJRFgD21r8Jp90WIcQf0lkXVgnvwKA\n",
       "TiQ1+qOuKwNOe1kYLw7Z07MG6hjF8a0hA3tZGAtrYGZV3Xnxb1kGdFmMNWxhDdQxipBeFFtgC2tg\n",
       "UVUorxoxAgDDG/orxbJrBbgCH+A6+yVDHnQsmwW1rzp5S/7kBXqhnQAAQBt8y17BPR0BAKBNm/SC\n",
       "151xtOn7Vb3npmc56sVT5mBVvOps19bdCmO71VastbHy0MY6r4U+tLHNaLMAjMsmPbtt3xsAAMaj\n",
       "yTC+Orefd/MBgJaM7aAjrNPlgWnbGY20cS2Rcm9bo2RoTUJX+yVabV38qepKexr++NTdYXc9vFLb\n",
       "I1t9j02OdSw0/bD+AYC8dXHQUUlkHKrWs4uAQYdiuoEB+VoMcgEOWxLYdM3IIWhR04NAxZLH5efW\n",
       "va7J9IvS4/L7O9DVXBGWL1+gBzn1sJeFtN5dc3rJEIFXhp4BkiCsIQI59bBpzsFBiJjAJgTlI0iC\n",
       "wE5Xmwf+BDUkQGCnyUFAGCEHHdMjrGGkUg/sOmWB3MYMC2sYqSE3fndjBthAl7cIWxfIwhYAAAAY\n",
       "yP8DJP+bpvoWbeQAAAAASUVORK5CYII=\n",
       "\" transform=\"translate(126, 12)\"/>\n",
       "</g>\n",
       "</svg>\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Plots\n",
    "spy(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2(4)\n",
    "\n",
    "Set the _teleportation_ parameter at $p = 0.85$. Try the following methods to solve for $\\mathbf{x}$ using the `ucla.zip` data.\n",
    "\n",
    "0. A dense linear system solver such as LU decomposition.  \n",
    "0. A simple iterative linear system solver such as Jacobi or Gauss-Seidel.   \n",
    "0. A dense eigen-solver.  \n",
    "0. A simple iterative eigen-solver such as the power method.  \n",
    "\n",
    "For iterative methods, you can use the [`IterativeSolvers.jl`](https://github.com/JuliaMath/IterativeSolvers.jl) package. Make sure to utilize the special structure of $\\mathbf{P}$ (diagonal + rank 1) to speed up the matrix-vector multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2(4) Solution:\n",
    "\n",
    "Below we use four different methods to solve for the stationary distribution, `x`, the long run proportion of time spent on each page. I have written a function for each method of solving this problem, where by default the teleportation parameter is set at p = 0.85. For the iterative solving methods, our functions set by default the maximum number of iterations to 10,000 and we use a tolerance of 1e-7. Since we are given that each page is uniformly likely to be visited, we initialize the stationary distribution at `x_old`, which is a vector of length n of the value $/mathfrac{1}/{n}$.\n",
    "\n",
    "Using the `ucla.zip` data, we benchmark these methods to compare their efficiencies. \n",
    "\n",
    "### Using LU-decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LU_Decomposition_Linear_Solver (generic function with 2 methods)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function LU_Decomposition_Linear_Solver(A::AbstractMatrix, telep::Float64 = 0.85)\n",
    "#first make P\n",
    "# Matrix P is the sum of a sparse matrix + a rank 1 matrix\n",
    "n = size(A, 1)\n",
    "r = sum(A, 2) #sum each row of A to get r_i for each page i\n",
    "P = zeros(n, n)\n",
    "term1 = zeros(n)\n",
    "term2 = repeat([1 / n], outer = [n]) \n",
    "for i in 1:n\n",
    "    if r[i] > 0 \n",
    "        term1[i] = telep / r[i]\n",
    "        term2[i] = (1 - telep) / n\n",
    "    end\n",
    "end\n",
    "#first make P^T\n",
    "P_transpose = (Diagonal(term1) * full(A) .+ term2).'\n",
    "\n",
    "#then make C = (I- P^t)\n",
    "C = I - P_transpose\n",
    "C[1, : ] = ones(n)      \n",
    "b = zeros(n)\n",
    "b[1] = 1.0\n",
    "\n",
    "#use the fact that each column sums to 1 and replace the first row of C with ones\n",
    "#then solve x = C \\ [1, 0, ... , 0] \n",
    "    return C \\ b\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500-element Array{Float64,1}:\n",
       " 0.0127701 \n",
       " 0.00130393\n",
       " 0.00405405\n",
       " 0.00398338\n",
       " 0.00130393\n",
       " 0.00130393\n",
       " 0.00130393\n",
       " 0.00130393\n",
       " 0.00130393\n",
       " 0.00130393\n",
       " 0.00130393\n",
       " 0.00145083\n",
       " 0.00141309\n",
       " ⋮         \n",
       " 0.0011959 \n",
       " 0.00149245\n",
       " 0.0011959 \n",
       " 0.00345944\n",
       " 0.00211007\n",
       " 0.0011959 \n",
       " 0.0011959 \n",
       " 0.0084935 \n",
       " 0.0011959 \n",
       " 0.00149103\n",
       " 0.0011959 \n",
       " 0.0011959 "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_LU = LU_Decomposition_Linear_Solver(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jacobi Iterative Method\n",
    "\n",
    ">Note that for our problem we are finding the solution $\\mathbf{x}$ to the equation $\\mathbf{C}\\mathbf{x} = \\mathbf{b},$\n",
    "where $\\mathbf{C} = (\\mathbf{I} - \\mathbf{P^{T}})$\n",
    "\n",
    "$$x_i^{(t+1)} = \\frac{b_i - \\sum_{j=1}^{i-1} c_{ij} x_j^{(t)} - \\sum_{j=i+1}^n c_{ij} x_j^{(t)}}{c_{ii}}$$\n",
    "\n",
    "Splitting up Matrix A into the sum of a Lower Triangular, Diagonal and Upper triangular matrix we have: \n",
    "$$\\mathbf{C} = \\mathbf{L} + \\mathbf{D} + \\mathbf{U}$$<br>\n",
    "$$x_i^{(t+1)} = -\\mathbf{D^{-1}}\\mathbf{C}x_i^{(t)} + x_i^{(t)} + \\mathbf{D^{-1}}\\mathbf{b}$$\n",
    "\n",
    ">Now substituting back in $\\mathbf{C} = (\\mathbf{I} - \\mathbf{P^{T}})$ we have:\n",
    "$$x_i^{(t+1)} = -\\mathbf{D^{-1}}(\\mathbf{I} - \\mathbf{P^{T}})x_i^{(t)} + x_i^{(t)} + \\mathbf{D^{-1}}\\mathbf{b}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Jacobi_Linear_Solver (generic function with 2 methods)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function Jacobi_Linear_Solver(A::AbstractMatrix, telep::Float64 = 0.85; \n",
    "        maxiter::Int = 10000, tolerance::Float64 = 1e-9, \n",
    "        x_old::Vector = repeat([1 / size(A, 1)], outer = [size(A, 1)]))\n",
    "#first make P\n",
    "# Matrix P is the sum of a sparse matrix + a rank 1 matrix\n",
    "n = size(A, 1)\n",
    "r = sum(A, 2) #sum each row of A to get r_i for each page i\n",
    "P = zeros(n, n)\n",
    "term1 = zeros(n)\n",
    "term2 = repeat([1 / n], outer = [n]) \n",
    "for i in 1:n\n",
    "    if r[i] > 0 \n",
    "        term1[i] = telep / r[i]\n",
    "        term2[i] = (1 - telep) / n\n",
    "    end\n",
    "end\n",
    "#first make P^T\n",
    "#P_transpose = (Diagonal(term1) * sparse(A) .+ term2).'\n",
    "\n",
    "P_term1 = sparse(A') * Diagonal(term1)\n",
    "P_term2 = ones(n, 1) * term2'\n",
    "P_t2vector = P_term2[1, :]' #want to make this a vector to speed up the process and reduce memory size\n",
    "    \n",
    "x_new = vec(zeros(n))\n",
    "\n",
    "#then make C = (I - P^t)\n",
    "C = I - P_term1 - P_term2\n",
    "b = zeros(n)\n",
    "    \n",
    "x_new = zeros(n)\n",
    "\n",
    "D_inv = inv(Diagonal(C)) \n",
    "i = 0\n",
    "for i in 1:maxiter\n",
    "x_new = -D_inv * (C * x_old) .+ x_old .+ (D_inv * b)\n",
    "if vecnorm(x_new - x_old) < tolerance\n",
    "        break\n",
    "    end\n",
    "x_old = x_new       \n",
    "end\n",
    "return x_new\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500-element Array{Float64,1}:\n",
       " 0.012771  \n",
       " 0.00130402\n",
       " 0.00405434\n",
       " 0.00398366\n",
       " 0.00130402\n",
       " 0.00130402\n",
       " 0.00130402\n",
       " 0.00130402\n",
       " 0.00130402\n",
       " 0.00130402\n",
       " 0.00130402\n",
       " 0.00145093\n",
       " 0.00141319\n",
       " ⋮         \n",
       " 0.00119598\n",
       " 0.00149255\n",
       " 0.00119598\n",
       " 0.00345969\n",
       " 0.00211022\n",
       " 0.00119598\n",
       " 0.00119598\n",
       " 0.00849411\n",
       " 0.00119598\n",
       " 0.00149114\n",
       " 0.00119598\n",
       " 0.00119598"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_jacobi = Jacobi_Linear_Solver(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense Eigen-Solver\n",
    "\n",
    "It is well-known that $\\mathbf{x}$ is the left eigenvector corresponding to the top eigenvalue 1 of the transition matrix $\\mathbf{P}$. That is $\\mathbf{P}^T \\mathbf{x} = \\mathbf{x}$. Therefore $\\mathbf{x}$ can be solved as an eigen-problem.\n",
    " \n",
    "We first perform eigen decomposition on $P^{T}$ to get the left eigenvector corresponding to the top eigenvalue 1, and find vector $x_i$, the proportion of times the surfer visits page $i$ in the long run. Therefore `x_dense_eigen_solvers` serves as page ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dense_Eigen_Solver (generic function with 2 methods)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function Dense_Eigen_Solver(A::AbstractMatrix, telep::Float64 = 0.85)\n",
    "#first make P\n",
    "# Matrix P is the sum of a sparse matrix + a rank 1 matrix\n",
    "n = size(A, 1)\n",
    "r = sum(A, 2) #sum each row of A to get r_i for each page i\n",
    "P = zeros(n, n)\n",
    "term1 = zeros(n)\n",
    "term2 = repeat([1 / n], outer = [n]) \n",
    "for i in 1:n\n",
    "    if r[i] > 0 \n",
    "        term1[i] = telep / r[i]\n",
    "        term2[i] = (1 - telep) / n\n",
    "    end\n",
    "end\n",
    "eigenvals_weights = zeros(n)\n",
    "#first make P^T\n",
    "P_transpose = (Diagonal(term1) * full(A) .+ term2).'\n",
    "# eig-decomposition C\n",
    "C = eigfact((P_transpose)) # this gets the eigen decomp of P^T which has the eigen values and vectors\n",
    "    \n",
    "# we get the real elements of the first column of the eigenvectors of P_transpose\n",
    "eigenvals_weights = real(C[:vectors][:, 1]) \n",
    "    \n",
    "# use eigen solvers to get the long run frequency of each page\n",
    "return eigenvals_weights / sum(eigenvals_weights) # find the proportion of time spent in each page over total time\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500-element Array{Float64,1}:\n",
       " 0.0127701 \n",
       " 0.00130393\n",
       " 0.00405405\n",
       " 0.00398338\n",
       " 0.00130393\n",
       " 0.00130393\n",
       " 0.00130393\n",
       " 0.00130393\n",
       " 0.00130393\n",
       " 0.00130393\n",
       " 0.00130393\n",
       " 0.00145083\n",
       " 0.00141309\n",
       " ⋮         \n",
       " 0.0011959 \n",
       " 0.00149245\n",
       " 0.0011959 \n",
       " 0.00345944\n",
       " 0.00211007\n",
       " 0.0011959 \n",
       " 0.0011959 \n",
       " 0.0084935 \n",
       " 0.0011959 \n",
       " 0.00149103\n",
       " 0.0011959 \n",
       " 0.0011959 "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_dense_eigen_solvers = Dense_Eigen_Solver(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Power_Eigen_Solver69 (generic function with 2 methods)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function Power_Eigen_Solver69(A::AbstractMatrix, telep::Float64 = 0.85; \n",
    "        maxiter::Int = 10000, tolerance::Float64 = 1e-7, \n",
    "        x_old::Vector = repeat([1 / size(A, 1)], outer = [size(A, 1)]))\n",
    "#first make P\n",
    "# Matrix P is the sum of a sparse matrix + a rank 1 matrix\n",
    "n = size(A, 1)\n",
    "r = sum(A, 2) #sum each row of A to get r_i for each page i\n",
    "P = zeros(n, n)\n",
    "term1 = zeros(n)\n",
    "term2 = repeat([1 / n], outer = [n]) \n",
    "for i in 1:n\n",
    "    if r[i] > 0 \n",
    "        term1[i] = telep / r[i]\n",
    "        term2[i] = (1 - telep) / n\n",
    "    end\n",
    "end\n",
    "\n",
    "#first make P^T\n",
    "#P_transpose = (Diagonal(term1) * sparse(A) .+ term2).' # blas2 here so use rank one update\n",
    "P_term1 = sparse(A') * Diagonal(term1)\n",
    "P_term2 = ones(n, 1) * term2'\n",
    "P_t2vector = P_term2[1, :]' #want to make this a vector to speed up the process and reduce memory size\n",
    "    \n",
    "x_new = vec(zeros(n))\n",
    "\n",
    "for i in 1:maxiter\n",
    "x_new = P_term1*x_old .+ (P_t2vector * x_old) \n",
    "        \n",
    "if vecnorm(x_new - x_old) < tolerance\n",
    "        break\n",
    "    end\n",
    "x_old = x_new       \n",
    "end\n",
    "return x_new / sum(x_new)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500-element Array{Float64,1}:\n",
       " 0.0127701 \n",
       " 0.00130394\n",
       " 0.00405407\n",
       " 0.0039834 \n",
       " 0.00130394\n",
       " 0.00130394\n",
       " 0.00130394\n",
       " 0.00130394\n",
       " 0.00130394\n",
       " 0.00130394\n",
       " 0.00130394\n",
       " 0.00145083\n",
       " 0.00141309\n",
       " ⋮         \n",
       " 0.0011959 \n",
       " 0.00149245\n",
       " 0.0011959 \n",
       " 0.00345947\n",
       " 0.00211009\n",
       " 0.0011959 \n",
       " 0.0011959 \n",
       " 0.00849357\n",
       " 0.0011959 \n",
       " 0.00149104\n",
       " 0.0011959 \n",
       " 0.0011959 "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_Power69 = Power_Eigen_Solver69(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  13.42 MiB\n",
       "  allocs estimate:  1462\n",
       "  --------------\n",
       "  minimum time:     4.615 ms (0.00% GC)\n",
       "  median time:      7.593 ms (28.86% GC)\n",
       "  mean time:        7.259 ms (22.65% GC)\n",
       "  maximum time:     12.044 ms (40.32% GC)\n",
       "  --------------\n",
       "  samples:          688\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using BenchmarkTools\n",
    "@benchmark LU_Decomposition_Linear_Solver(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  9.52 MiB\n",
       "  allocs estimate:  5172\n",
       "  --------------\n",
       "  minimum time:     5.086 ms (0.00% GC)\n",
       "  median time:      7.662 ms (0.00% GC)\n",
       "  mean time:        7.740 ms (14.73% GC)\n",
       "  maximum time:     25.605 ms (16.18% GC)\n",
       "  --------------\n",
       "  samples:          646\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@benchmark Jacobi_Linear_Solver(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  17.40 MiB\n",
       "  allocs estimate:  1928\n",
       "  --------------\n",
       "  minimum time:     95.726 ms (0.00% GC)\n",
       "  median time:      104.308 ms (2.46% GC)\n",
       "  mean time:        106.508 ms (2.02% GC)\n",
       "  maximum time:     138.575 ms (2.11% GC)\n",
       "  --------------\n",
       "  samples:          47\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@benchmark Dense_Eigen_Solver(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  4.75 MiB\n",
       "  allocs estimate:  3232\n",
       "  --------------\n",
       "  minimum time:     1.469 ms (0.00% GC)\n",
       "  median time:      2.014 ms (0.00% GC)\n",
       "  mean time:        2.544 ms (23.11% GC)\n",
       "  maximum time:     6.541 ms (57.27% GC)\n",
       "  --------------\n",
       "  samples:          1958\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@benchmark Power_Eigen_Solver69(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2(5)\n",
    "\n",
    "List the top 20 ranked URLs you found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2(5) Solution:\n",
    "\n",
    "First, we show that the 4 different methods we implement to solve the Google page rank problem give us roughly the same stationary distributions, thus the same page rankings. We compare the values of each of the methods below using the `vecnorm()` function. The values of each method seem to agree, thus we sort the data by the stationary distribution column `x_LU` using the function `sortperm()`. This function is useful because we are able to get the indices of the top 20 (22 for us) pages! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vecnorm(x_LU - x_jacobi) = 4.490383853180799e-6\n",
      "vecnorm(x_LU - x_dense_eigen_solvers) = 2.9112447522968875e-16\n",
      "vecnorm(x_LU - x_Power69) = 8.866001341035277e-7\n",
      "vecnorm(x_jacobi - x_Power69) = 4.696010753965762e-6\n",
      "vecnorm(x_dense_eigen_solvers - x_Power69) = 8.866001339943139e-7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8.866001339943139e-7"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@show vecnorm(x_LU - x_jacobi)\n",
    "@show vecnorm(x_LU - x_dense_eigen_solvers)\n",
    "@show vecnorm(x_LU - x_Power69)\n",
    "@show vecnorm(x_jacobi - x_Power69)\n",
    "@show vecnorm(x_dense_eigen_solvers - x_Power69)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that there is a 4 way tie between the 19th - 22nd sites, so we print out the top 22 links!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22-element Array{Int64,1}:\n",
       " 459\n",
       "  64\n",
       "   1\n",
       "  27\n",
       "  29\n",
       " 496\n",
       " 462\n",
       "  36\n",
       "  28\n",
       "  43\n",
       "  57\n",
       "  60\n",
       "  62\n",
       "  63\n",
       "  61\n",
       "  54\n",
       "  55\n",
       "  59\n",
       "  56\n",
       "  58\n",
       "  51\n",
       "  53"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top22 = sortperm(x_LU, rev = true)[1:22]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the top 22 links! I think it's funny that the second most popular link is just \"http://www\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22-element Array{String,1}:\n",
       " \"http://schema.org\"                                                                     \n",
       " \"http://www\"                                                                            \n",
       " \"http://www.ucla.edu\"                                                                   \n",
       " \"http://www.directory.ucla.edu\"                                                         \n",
       " \"http://www.universityofcalifornia.edu\"                                                 \n",
       " \"http://schema.org/CreativeWork\"                                                        \n",
       " \"http://schema.org/Text\"                                                                \n",
       " \"http://giveto.ucla.edu\"                                                                \n",
       " \"http://www.registrar.ucla.edu/calendar\"                                                \n",
       " \"http://www.uclalumni.net\"                                                              \n",
       " \"http://www.magazine.ucla.edu/depts/lifesigns/honoring-adolescence\"                     \n",
       " \"http://www.magazine.ucla.edu/depts/lifesigns/breaking-the-bad\"                         \n",
       " \"http://www.magazine.ucla.edu/depts/atissue/local-thought-global-action\"                \n",
       " \"http://www.magazine.ucla.edu/depts/hailhills/uncle-welton-in-westwood\"                 \n",
       " \"http://www.magazine.ucla.edu/depts/forward-thinker/homes-with-heart\"                   \n",
       " \"http://www.magazine.ucla.edu/features/patient-test-thyself\"                            \n",
       " \"http://www.magazine.ucla.edu/depts/style/the-dancing-scientist\"                        \n",
       " \"http://www.magazine.ucla.edu/depts/style/welcome-to-the-jungle\"                        \n",
       " \"http://www.magazine.ucla.edu/features/splendor-in-the-trash\"                           \n",
       " \"http://www.magazine.ucla.edu/features/not-your-fathers-mba\"                            \n",
       " \"http://www.magazine.ucla.edu/features/hope-is-real-the-ucla-depression-grand-challenge\"\n",
       " \"http://www.magazine.ucla.edu/exclusives/brotherly-love\"                                "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top22_pages = U[top22]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2(6)\n",
    "\n",
    "As of Monday May 11 2018, there are at least 1.83 billion indexed webpages on internet according to <http://www.worldwidewebsize.com/>. Explain whether each of these methods works for the PageRank problem at this scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2(6) Solution:\n",
    "\n",
    "\n",
    "The Google PageRank problem as of May 11, 2008 must rank at least 1.83 billion webpages on the internet.\n",
    "\n",
    "To solve this problem, want to find the top left eigenvector of the transition matrix  P . Direct methods such as (unsymmetric) QR or SVD takes forever. Iterative methods such as power method is feasible. However power method may take a large number of iterations to converge.\n",
    "\n",
    "Note that the Indexed Web currently contains at least 3.07 billion pages as of Thursday, 24 May, 2018. The Google page rank problem is growing rapidly in scale, as we speak! Thus finding feasible methods to analyze this problem becomes increasingly important as the size of the data grows.\n",
    "\n",
    "### Direct methods (flops fixed a priori) vs iterative methods:\n",
    "\n",
    "0. Direct method (GE/LU, Cholesky, QR, SVD): good for dense, small to moderate sized  A matrices\n",
    "0. Iterative methods (Jacobi, Gauss-Seidal, SOR, conjugate-gradient, GMRES): good for large, sparse, structured linear system, parallel computing, warm start\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. A dense linear system solver - LU decomposition  (doesn't work)\n",
    "> A dense linear system solver such as LU decomposition would not work for the Google PageRank problem at this scale. Since direct methods The number of flops required are fixed a priori, and would take millions of years to compute.\n",
    "\n",
    "0. A simple iterative linear system solver - Jacobi  (works!)\n",
    "> A simple iterative linear system solver such as the Jacobi method would work for the Google PageRank problem at this scale. In general, iterative methods are good for large, sparse, structured linear system, parallel computing, and can benefit from a \"warm start\". A warm start is a good guess to initialize the algorithm. Utilizing the sparsity of the connectivity matrix $\\mathbf{A},$ we can optimize our code efficiency. \n",
    "\n",
    "0. A dense eigen solver - (doesn't work)\n",
    "> A dense eigen solver would not work for the Google PageRank problem at this scale. Since direct methods fix the number of flops required a priori, it would take millions of years to compute.\n",
    "\n",
    "0. A simple iterative eigen-solver - the power method  (works!)\n",
    "> A simple iterative eigen-solver such as the power method would work for the Google PageRank problem at this scale. In general, iterative methods are good for large, sparse, structured linear system, parallel computing, and can benefit from a \"warm start\". A warm start is a good guess to initialize the algorithm. Utilizing the sparsity of the connectivity matrix $\\mathbf{A},$ we can optimize our code efficiency. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "87px",
    "width": "252px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
