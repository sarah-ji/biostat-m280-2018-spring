{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biostat M280 Homework 3\n",
    "\n",
    "**Due Friday, May 25 @ 11:59PM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 - Big $n$ regression\n",
    "\n",
    "Those who took my _203B: Introduction to Data Science_ last quarter had a (painful) experience of wrangling an Apache Spark cluster to do linear regression on a dataset with more than 100 million observations. Now we learnt various methods for solving linear regression and should realize that, with right choice of algorithm, it is a problem that can be handled by any moderate computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1(1)\n",
    "\n",
    "Download the flight data from <http://stat-computing.org/dataexpo/2009/the-data.html>. For this exercise, we only need data from years 2003-2008. If you are using Mac or Linux, you can run the following Bash script, which downloads and unzips files for all years.\n",
    "```bash\n",
    "# Download flight data by year\n",
    "for i in {1987..2008}\n",
    "  do\n",
    "    echo \"$(date) $i Download\"\n",
    "    fnam=$i.csv.bz2\n",
    "    wget -O ./$fnam http://stat-computing.org/dataexpo/2009/$fnam\n",
    "    echo \"$(date) $i unzip\"\n",
    "    bzip2 -d ./$fnam\n",
    "  done\n",
    "\n",
    "# Download airline carrier data\n",
    "wget -O ./airlines.csv http://www.transtats.bts.gov/Download_Lookup.asp?Lookup=L_UNIQUE_CARRIERS\n",
    "\n",
    "# Download airports data\n",
    "wget -O ./airports.csv https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat\n",
    "```\n",
    "Find out how many data points in each year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1(1) Solution:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "countlines(\"2003.csv\") = 6488541\n",
      "countlines(\"2004.csv\") = 7129271\n",
      "countlines(\"2005.csv\") = 7140597\n",
      "countlines(\"2006.csv\") = 7141923\n",
      "countlines(\"2007.csv\") = 7453216\n",
      "countlines(\"2008.csv\") = 7009729\n"
     ]
    }
   ],
   "source": [
    "# how many data points\n",
    "@show countlines(\"2003.csv\")\n",
    "# how many data points\n",
    "@show countlines(\"2004.csv\")\n",
    "# how many data points\n",
    "@show countlines(\"2005.csv\")\n",
    "# how many data points\n",
    "@show countlines(\"2006.csv\")\n",
    "# how many data points\n",
    "@show countlines(\"2007.csv\")\n",
    "# how many data points\n",
    "@show countlines(\"2008.csv\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1(2) \n",
    "\n",
    "We are interested in how the time gain of a flight, defined as `DepDelay - ArrDelay`, depends on the distance traveled (`Distance`), departure delay (`DepDelay`), and carrier (`UniqueCarrier`). \n",
    "\n",
    "We want to fit a linear regression `Gain ~ 1 + Distance + DepDelay + UniqueCarrier` using data from 2003-2008. Note `UniqueCarrier` is a factor with 23 levels: \"9E\", \"AA\", \"AQ\", \"AS\", \"B6\", \"CO\", \"DH\", \"DL\", \"EV\", \"F9\", \"FL\", \"HA\", \"HP\", \"MQ\", \"NW\", \"OH\", \"OO\", \"TZ\", \"UA\", \"US\", \"WN\", \"XE\", and \"YV\". We use the dummy coding with \"9E\" as base level.\n",
    "\n",
    "Will the design matrix (in double precision) fit into the memory of you computer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1(2) Solution:\n",
    "\n",
    "First I checked the dimension of the design matrix we need for the regression. Since our model has an intercept, we have 5 columns and counting the number of rows in each year we have 41,490,375 rows. So our design matrix is a 41,490,375 by 5 matrix, and it can fit on the memory of my computer but I prefer it didn't... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If each number is stored in double precicion, we need 8,629,998,000 bytes or 8.629998 GiB of memory to store this matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8629998000"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "41490375*26*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 48.374033 seconds (153.87 M allocations: 8.183 GiB, 26.93% gc time)\n",
      " 20.021477 seconds (31.53 M allocations: 4.276 GiB, 7.28% gc time)\n",
      " 19.828827 seconds (31.29 M allocations: 4.364 GiB, 7.48% gc time)\n",
      " 21.101573 seconds (31.41 M allocations: 4.530 GiB, 18.85% gc time)\n",
      " 23.794135 seconds (41.08 M allocations: 4.698 GiB, 21.54% gc time)\n",
      " 36.608247 seconds (121.61 M allocations: 7.291 GiB, 31.04% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Table with 6855029 rows, 4 columns:\n",
       "DepDelay  ArrDelay  UniqueCarrier  Distance\n",
       "───────────────────────────────────────────\n",
       "8         -14       \"WN\"           810\n",
       "19        2         \"WN\"           810\n",
       "8         14        \"WN\"           515\n",
       "-4        -6        \"WN\"           515\n",
       "34        34        \"WN\"           515\n",
       "25        11        \"WN\"           688\n",
       "67        57        \"WN\"           1591\n",
       "-1        -18       \"WN\"           1591\n",
       "2         2         \"WN\"           451\n",
       "0         -16       \"WN\"           451\n",
       "6         1         \"WN\"           828\n",
       "94        80        \"WN\"           828\n",
       "⋮\n",
       "57        75        \"DL\"           481\n",
       "80        99        \"DL\"           689\n",
       "-2        15        \"DL\"           270\n",
       "-4        6         \"DL\"           425\n",
       "-3        16        \"DL\"           546\n",
       "-1        2         \"DL\"           215\n",
       "3         14        \"DL\"           533\n",
       "-1        -2        \"DL\"           874\n",
       "-5        0         \"DL\"           545\n",
       "11        9         \"DL\"           533\n",
       "7         -5        \"DL\"           874"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data from csv\n",
    "using JuliaDB\n",
    "# only need columns: DepDelay, ArrDelay, UniqueCarrier, Distance\n",
    "@time table2003 = loadtable(\n",
    "    \"2003.csv\", \n",
    "    datacols = [\"DepDelay\", \"ArrDelay\", \"UniqueCarrier\", \"Distance\"])\n",
    "# drop rows with missing values\n",
    "table2003 = dropna(table2003)\n",
    "\n",
    "@time table2004 = loadtable(\n",
    "    \"2004.csv\", \n",
    "    datacols = [\"DepDelay\", \"ArrDelay\", \"UniqueCarrier\", \"Distance\"])\n",
    "# drop rows with missing values\n",
    "table2004 = dropna(table2004)\n",
    "\n",
    "@time table2005 = loadtable(\n",
    "    \"2005.csv\", \n",
    "    datacols = [\"DepDelay\", \"ArrDelay\", \"UniqueCarrier\", \"Distance\"])\n",
    "# drop rows with missing values\n",
    "table2005 = dropna(table2005)\n",
    "\n",
    "@time table2006 = loadtable(\n",
    "    \"2006.csv\", \n",
    "    datacols = [\"DepDelay\", \"ArrDelay\", \"UniqueCarrier\", \"Distance\"])\n",
    "# drop rows with missing values\n",
    "table2006 = dropna(table2006)\n",
    "\n",
    "@time table2007 = loadtable(\n",
    "    \"2007.csv\", \n",
    "    datacols = [\"DepDelay\", \"ArrDelay\", \"UniqueCarrier\", \"Distance\"])\n",
    "# drop rows with missing values\n",
    "table2007 = dropna(table2007)\n",
    "\n",
    "@time table2008 = loadtable(\n",
    "    \"2008.csv\", \n",
    "    datacols = [\"DepDelay\", \"ArrDelay\", \"UniqueCarrier\", \"Distance\"])\n",
    "# drop rows with missing values\n",
    "table2008 = dropna(table2008)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131.622394 seconds (388.14 M allocations: 17.700 GiB, 3.80% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Table with 41490375 rows, 4 columns:\n",
       "DepDelay  ArrDelay  UniqueCarrier  Distance\n",
       "───────────────────────────────────────────\n",
       "-4        -1        \"UA\"           837\n",
       "-1        -3        \"UA\"           837\n",
       "29        23        \"UA\"           837\n",
       "-2        -9        \"UA\"           1835\n",
       "18        52        \"UA\"           1835\n",
       "-4        6         \"UA\"           1835\n",
       "-4        -8        \"UA\"           1835\n",
       "0         2         \"UA\"           1835\n",
       "-4        19        \"UA\"           1835\n",
       "3         4         \"UA\"           413\n",
       "-4        -23       \"UA\"           413\n",
       "-3        -19       \"UA\"           413\n",
       "⋮\n",
       "57        75        \"DL\"           481\n",
       "80        99        \"DL\"           689\n",
       "-2        15        \"DL\"           270\n",
       "-4        6         \"DL\"           425\n",
       "-3        16        \"DL\"           546\n",
       "-1        2         \"DL\"           215\n",
       "3         14        \"DL\"           533\n",
       "-1        -2        \"DL\"           874\n",
       "-5        0         \"DL\"           545\n",
       "11        9         \"DL\"           533\n",
       "7         -5        \"DL\"           874"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data from csv\n",
    "using JuliaDB\n",
    "@time yrtable = loadtable(\n",
    "    \"allyears\", \n",
    "    datacols = [\"DepDelay\", \"ArrDelay\", \"UniqueCarrier\", \"Distance\"])\n",
    "# drop rows with missing values\n",
    "yrtable = dropna(yrtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we drop missing values, I count again how many bytes this design matrix will use. Even after dropping rows with missing values, we will use 8.62999696 GiB of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "\u001b[91mUndefVarError: table2003 not defined\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mUndefVarError: table2003 not defined\u001b[39m",
      ""
     ]
    }
   ],
   "source": [
    "# how many data points\n",
    "@show length(table2003)\n",
    "# how many data points\n",
    "@show length(table2004)\n",
    "# how many data points\n",
    "@show length(table2005)\n",
    "# how many data points\n",
    "@show length(table2006)\n",
    "# how many data points\n",
    "@show length(table2007)\n",
    "# how many data points\n",
    "@show length(table2008)\n",
    "\n",
    "@show 41490375*26*8;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "linreg_sweep_all (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mapping from variable names to X columns\n",
    "# carrier \"9E\" is used as base level\n",
    "const var2col = Dict(\n",
    "        \"Intercept\" => 1,\n",
    "        \"Distance\" => 2,\n",
    "        \"DepDelay\" => 3,\n",
    "        \"AA\" => 4,\n",
    "        \"AQ\" => 5,\n",
    "        \"AS\" => 6,\n",
    "        \"B6\" => 7,\n",
    "        \"CO\" => 8,\n",
    "        \"DH\" => 9,\n",
    "        \"DL\" => 10,\n",
    "        \"EV\" => 11,\n",
    "        \"F9\" => 12,\n",
    "        \"FL\" => 13,\n",
    "        \"HA\" => 14,\n",
    "        \"HP\" => 15,\n",
    "        \"MQ\" => 16,\n",
    "        \"NW\" => 17,\n",
    "        \"OH\" => 18,\n",
    "        \"OO\" => 19,\n",
    "        \"TZ\" => 20,\n",
    "        \"UA\" => 21,\n",
    "        \"US\" => 22,\n",
    "        \"WN\" => 23,\n",
    "        \"XE\" => 24,\n",
    "        \"YV\" => 25,\n",
    "        \"Gain\" => 26)\n",
    "# mapping from column to variable names\n",
    "const col2var = map(reverse, var2col)\n",
    "\n",
    "# a custom function to generate [X y] from data table\n",
    "function generate_xy(tbl::NextTable)\n",
    "    # X matrix\n",
    "    XY = zeros(length(tbl), 26)\n",
    "    # intercept term\n",
    "    @views fill!(XY[:, 1], 1)\n",
    "    # Distance term\n",
    "    @views copy!(XY[:, 2], columns(tbl, :Distance))\n",
    "    # DepDelay term\n",
    "    @views copy!(XY[:, 3], columns(tbl, :DepDelay))\n",
    "    # Dummy coding for airline\n",
    "    @inbounds for i in 1:length(tbl)\n",
    "        yrtable[i][:UniqueCarrier] == \"9E\" && continue # base level\n",
    "        XY[i, var2col[tbl[i][:UniqueCarrier]]] = 1\n",
    "    end\n",
    "    # last column is response: gain = depdelay - arrdelay\n",
    "    XY[:, 26] = select(tbl, \n",
    "        (:DepDelay, :ArrDelay) => p -> Float64(p.DepDelay - p.ArrDelay))\n",
    "    # return\n",
    "    XY\n",
    "end\n",
    "\n",
    "function linreg_sweep_all(y::Vector, X::Matrix)\n",
    "    n = size(X, 1)\n",
    "    p = size(X, 2)\n",
    "    \n",
    "    tableau = [X y]' * [X y]\n",
    "    sweep!(tableau, 1:p)\n",
    "    beta_coeff_est = tableau[1:p, end]\n",
    "    sigma_hat2 = tableau[end, end] / (n - 1) \n",
    "    SE_beta_coeff_est = sqrt.(Diagonal( .- sigma_hat2 .* tableau[1:p, 1:p]))\n",
    "    return beta_coeff_est, sigma_hat2, SE_beta_coeff_est\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41490375×26 Array{Float64,2}:\n",
       " 1.0   837.0  -4.0  0.0  0.0  0.0  …  0.0  1.0  0.0  0.0  0.0  0.0   -3.0\n",
       " 1.0   837.0  -1.0  0.0  0.0  0.0     0.0  1.0  0.0  0.0  0.0  0.0    2.0\n",
       " 1.0   837.0  29.0  0.0  0.0  0.0     0.0  1.0  0.0  0.0  0.0  0.0    6.0\n",
       " 1.0  1835.0  -2.0  0.0  0.0  0.0     0.0  1.0  0.0  0.0  0.0  0.0    7.0\n",
       " 1.0  1835.0  18.0  0.0  0.0  0.0     0.0  1.0  0.0  0.0  0.0  0.0  -34.0\n",
       " 1.0  1835.0  -4.0  0.0  0.0  0.0  …  0.0  1.0  0.0  0.0  0.0  0.0  -10.0\n",
       " 1.0  1835.0  -4.0  0.0  0.0  0.0     0.0  1.0  0.0  0.0  0.0  0.0    4.0\n",
       " 1.0  1835.0   0.0  0.0  0.0  0.0     0.0  1.0  0.0  0.0  0.0  0.0   -2.0\n",
       " 1.0  1835.0  -4.0  0.0  0.0  0.0     0.0  1.0  0.0  0.0  0.0  0.0  -23.0\n",
       " 1.0   413.0   3.0  0.0  0.0  0.0     0.0  1.0  0.0  0.0  0.0  0.0   -1.0\n",
       " 1.0   413.0  -4.0  0.0  0.0  0.0  …  0.0  1.0  0.0  0.0  0.0  0.0   19.0\n",
       " 1.0   413.0  -3.0  0.0  0.0  0.0     0.0  1.0  0.0  0.0  0.0  0.0   16.0\n",
       " 1.0   413.0   0.0  0.0  0.0  0.0     0.0  1.0  0.0  0.0  0.0  0.0   12.0\n",
       " ⋮                            ⋮    ⋱       ⋮                          ⋮  \n",
       " 1.0   515.0   0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0    5.0\n",
       " 1.0   481.0  57.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  -18.0\n",
       " 1.0   689.0  80.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  -19.0\n",
       " 1.0   270.0  -2.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  -17.0\n",
       " 1.0   425.0  -4.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  -10.0\n",
       " 1.0   546.0  -3.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  -19.0\n",
       " 1.0   215.0  -1.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0   -3.0\n",
       " 1.0   533.0   3.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  -11.0\n",
       " 1.0   874.0  -1.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0    1.0\n",
       " 1.0   545.0  -5.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0   -5.0\n",
       " 1.0   533.0  11.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0    2.0\n",
       " 1.0   874.0   7.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0   12.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy = generate_xy(yrtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "call sweep directly and you will get the desired quantities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "linreg_sweep_all (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using SweepOperator, BenchmarkTools\n",
    "\n",
    "function linreg_sweep_all(y::Vector, X::Matrix)\n",
    "    n = size(X, 1)\n",
    "    p = size(X, 2)\n",
    "    \n",
    "    tableau = [X y]' * [X y]\n",
    "    sweep!(tableau, 1:p)\n",
    "    beta_coeff_est = tableau[1:p, end]\n",
    "    sigma_hat2 = tableau[end, end] / (n - 1) \n",
    "    SE_beta_coeff_est = sqrt.(Diagonal( .- sigma_hat2 .* tableau[1:p, 1:p]))\n",
    "    return beta_coeff_est, sigma_hat2, SE_beta_coeff_est\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1.14033, 0.00164935, -0.0118811, -1.8723, -0.5789, -0.938452, -1.42247, -2.57627, 1.16808, -2.19625  …  -1.46395, -3.62506, -0.00722279, -0.40365, -3.5774, -1.14816, -0.883753, 2.74855, -2.56721, -0.202211], 204.4031074256946, \n",
       "  [1 ,  1]  =  0.0202318\n",
       "  [2 ,  2]  =  4.37958e-6\n",
       "  [3 ,  3]  =  6.88878e-5\n",
       "  [4 ,  4]  =  0.0215571\n",
       "  [5 ,  5]  =  0.0521477\n",
       "  [6 ,  6]  =  0.0250361\n",
       "  [7 ,  7]  =  0.0259154\n",
       "  [8 ,  8]  =  0.0229534\n",
       "  [9 ,  9]  =  0.0266552\n",
       "  [10, 10]  =  0.021659\n",
       "  ⋮\n",
       "  [15, 15]  =  0.0276918\n",
       "  [16, 16]  =  0.0218055\n",
       "  [17, 17]  =  0.0220088\n",
       "  [18, 18]  =  0.023446\n",
       "  [19, 19]  =  0.0217463\n",
       "  [20, 20]  =  0.0375073\n",
       "  [21, 21]  =  0.0219444\n",
       "  [22, 22]  =  0.0220022\n",
       "  [23, 23]  =  0.0209201\n",
       "  [24, 24]  =  0.0222383\n",
       "  [25, 25]  =  0.0255715)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_coeff_est, sigma_hat2, SE_beta_coeff_est = linreg_sweep_all(xy[:, 26], xy[:, 1:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1(3)\n",
    "\n",
    "Review the [Summary of Linear Regression](http://hua-zhou.github.io/teaching/biostatm280-2018spring/slides/12-linreg/linreg.html) and devise a strategy to solve the linear regression.\n",
    "\n",
    "Report the estimated regression coefficients $\\widehat \\beta$, estimated variance $\\widehat \\sigma^2 = \\sum_i (y_i - \\widehat y_i)^2 / (n - 1)$, and coefficient standard errors.\n",
    "\n",
    "Hint: It took my laptop less than 3 minutes to import data and fit linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1(3) Solution:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25-element Array{Float64,1}:\n",
       "  1.14033   \n",
       "  0.00164935\n",
       " -0.0118811 \n",
       " -1.8723    \n",
       " -0.5789    \n",
       " -0.938452  \n",
       " -1.42247   \n",
       " -2.57627   \n",
       "  1.16808   \n",
       " -2.19625   \n",
       "  1.03932   \n",
       " -2.15207   \n",
       " -1.35247   \n",
       " -1.87248   \n",
       " -0.350758  \n",
       " -1.46395   \n",
       " -3.62506   \n",
       " -0.00722279\n",
       " -0.40365   \n",
       " -3.5774    \n",
       " -1.14816   \n",
       " -0.883753  \n",
       "  2.74855   \n",
       " -2.56721   \n",
       " -0.202211  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_coeff_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204.4031074256946"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigma_hat2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25×25 SparseMatrixCSC{Float64,Int64} with 25 stored entries:\n",
       "  [1 ,  1]  =  0.0202318\n",
       "  [2 ,  2]  =  4.37958e-6\n",
       "  [3 ,  3]  =  6.88878e-5\n",
       "  [4 ,  4]  =  0.0215571\n",
       "  [5 ,  5]  =  0.0521477\n",
       "  [6 ,  6]  =  0.0250361\n",
       "  [7 ,  7]  =  0.0259154\n",
       "  [8 ,  8]  =  0.0229534\n",
       "  [9 ,  9]  =  0.0266552\n",
       "  [10, 10]  =  0.021659\n",
       "  ⋮\n",
       "  [15, 15]  =  0.0276918\n",
       "  [16, 16]  =  0.0218055\n",
       "  [17, 17]  =  0.0220088\n",
       "  [18, 18]  =  0.023446\n",
       "  [19, 19]  =  0.0217463\n",
       "  [20, 20]  =  0.0375073\n",
       "  [21, 21]  =  0.0219444\n",
       "  [22, 22]  =  0.0220022\n",
       "  [23, 23]  =  0.0209201\n",
       "  [24, 24]  =  0.0222383\n",
       "  [25, 25]  =  0.0255715"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SE_beta_coeff_est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1(4)\n",
    "\n",
    "Go to your resume/cv and claim you have experience performing analytics on data with hundred millions of observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1(4) Solution:\n",
    "\n",
    "\n",
    "I have edited my resume/cv and claimed my experience performing analysis on data with hundred millions of observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2 - Google PageRank\n",
    "\n",
    "We are going to try different numerical methods learnt in class on the [Google PageRank problem](https://en.wikipedia.org/wiki/PageRank)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2(1)\n",
    "\n",
    "Let $\\mathbf{A} \\in \\{0,1\\}^{n \\times n}$ be the connectivity matrix of $n$ web pages with entries\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\ta_{ij}= \\begin{cases}\n",
    "\t1 & \\text{if page $i$ links to page $j$} \\\\\n",
    "\t0 & \\text{otherwise}\n",
    "\t\\end{cases}.\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "$r_i = \\sum_j a_{ij}$ is the out-degree of page $i$. That is $r_i$ is the number of links on page $i$. Imagine a random surfer exploring the space of $n$ pages according to the following rules.  \n",
    "\n",
    "- From a page $i$ with $r_i>0$\n",
    "    * with probability $p$, (s)he randomly chooses a link on page $i$ (uniformly) and follows that link to the next page  \n",
    "    * with probability $1-p$, (s)he randomly chooses one page from the set of all $n$ pages (uniformly) and proceeds to that page  \n",
    "- From a page $i$ with $r_i=0$ (a dangling page), (s)he randomly chooses one page from the set of all $n$ pages (uniformly) and proceeds to that page  \n",
    "    \n",
    "The process defines a Markov chain on the space of $n$ pages. Write down the transition matrix $\\mathbf{P}$ of the Markov chain as a diagonal matrix plus rank-1 matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2(1) Solution:\n",
    "\n",
    ">Since with probability $p$, (s)he randomly chooses a link on page $i$ (uniformly) and follows that link to the next page, if there are ${\\sum_j a_{ij}}$ pages the probability of going to each one would be $\\frac{1}{{\\sum_j a_{ij}}}$.<br> \n",
    "\n",
    ">Additionally, we are given: if there are no links on the current page, with probability $1-p$, (s)he randomly chooses one page from the set of all $n$ pages (uniformly) and proceeds to that page. If there are n total pages in the world, the probability of going to each one would be $1/n$.<br><br>\n",
    "\n",
    ">We note that the transition probability of going from page i to page j is:\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "    p_{ij} = \\begin{cases}\n",
    "    \\frac{1}{\\sum_j a_{ij}}*p + \\frac{1}{n}*(1 - p) & \\text{if page $i$ has links on the page, $r_i$ > 0} \\\\\n",
    "    \\frac{1}{n} & \\text{if there are no links on the page $i$, $r_i$ = 0}\n",
    "    \\end{cases}\n",
    "\\end{eqnarray*}\n",
    "$$<br>\n",
    "> Replacing $r_i = \\sum_j a_{ij}$ we have:\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "    p_{ij} = \\begin{cases}\n",
    "    \\frac{a_{ij}}{r_i}*p + \\frac{1}{n}*(1 - p) & \\text{if page $i$ has links on the page, $r_i$ > 0} \\\\\n",
    "    \\frac{1}{n} & \\text{if there are no links on the page $i$, $r_i$ = 0}\n",
    "    \\end{cases}\n",
    "\\end{eqnarray*}\n",
    "$$<br>\n",
    "\n",
    ">We make note that the structure of transition matrix $P$ is the sum of two terms, a diagonal matrix + a rank-1 matrix, <br> where $\\mathbf{\\frac{1}{n}} = \\frac{1}{n}\\mathbf{1^T}$ and $\\mathbf{\\frac{1-p}{n}} = \\frac{1-p}{n}\\mathbf{1^T}$ are vectors. Then the resulting Transition Probability Matrix is:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "    \\mathbf{P} = \\begin{cases}\n",
    "     diag(\\frac{p}{r_i})\\mathbf{A} + \\mathbf{\\frac{1-p}{n}}\\mathbf{1} & \\text{if page $i$ has links on the page, $r_i$ > 0} \\\\\n",
    "    diag(0)\\mathbf{A} + \\mathbf{\\frac{1}{n}}\\mathbf{1} & \\text{if there are no links on the page $i$, $r_i$ = 0}\n",
    "    \\end{cases}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Q2(2)\n",
    "\n",
    "According to standard Markov chain theory, the (random) position of the surfer converges to the stationary distribution $\\mathbf{x} = (x_1,\\ldots,x_n)^T$ of the Markov chain. $x_i$ has the natural interpretation of the proportion of times the surfer visits page $i$ in the long run. Therefore $\\mathbf{x}$ serves as page ranks: a higher $x_i$ means page $i$ is more visited. It is well-known that $\\mathbf{x}$ is the left eigenvector corresponding to the top eigenvalue 1 of the transition matrix $\\mathbf{P}$. That is $\\mathbf{P}^T \\mathbf{x} = \\mathbf{x}$. Therefore $\\mathbf{x}$ can be solved as an eigen-problem. Show that it can also be cast as solving a linear system. Since the row sums of $\\mathbf{P}$ are 1, $\\mathbf{P}$ is rank deficient. We can replace the first equation by the $\\sum_{i=1}^n x_i = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2(2) Solution:\n",
    "\n",
    ">First we have:\n",
    "$$\\mathbf{P}^T \\mathbf{x} = \\mathbf{x}$$<br>\n",
    ">Moving everything to the left side we have:\n",
    "$$\\mathbf{P}^T \\mathbf{x} - \\mathbf{x} = \\mathbf{0}$$<br>\n",
    ">We can re-write this as:\n",
    "$$(\\mathbf{I} - \\mathbf{P}^T) \\mathbf{x} = \\mathbf{0}$$<br>\n",
    ">Now, we see that this problem can also be cast as solving a linear system. What we want to find is the solution `x`.\n",
    ">If we let $\\mathbf{C} = (\\mathbf{I} - \\mathbf{P}^T),$ we know $\\mathbf{C}$ is not full rank. Since the row sums of $\\mathbf{P}$ are 1, $\\mathbf{P}$ is rank deficient and we know each column of $\\mathbf{C}$ will also sum to 1.<br><br>\n",
    ">Thus, we can replace the first equation by the $\\sum_{i=1}^n x_i = 1$ by replacing the first row of $\\mathbf{C}$ with a vector of ones, and create a new vector for the solution, where the first postition is a 1 (indicating we changed the 1st row of matrix $\\mathbf{C}$ to ones) and the rest are zeros. <br><br>\n",
    ">Note that we could choose any of the n rows of matrix $\\mathbf{C}$ to be ones, adjusting the corresponding solution vector with a 1 in the position of the row number that was changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2(3)\n",
    "\n",
    "Download the [`ucla.zip`](http://hua-zhou.github.io/teaching/biostatm280-2018spring/hw/hw3/ucla.zip) package from course webpage. Unzip the package, which contains two files `U.txt` and `A.txt`. `U.txt` lists the 500 URL names. `A.txt` is the $500 \\times 500$ connectivity matrix. Read data into Julia. Compute summary statistics:\n",
    "* number of pages\n",
    "* number of edges\n",
    "* number of dangling nodes (pages with no out links)\n",
    "* which page has max in-degree?\n",
    "* which page has max out-degree?\n",
    "* visualize the sparsity pattern of $\\mathbf{A}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2(3) Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_pages = size(A, 1) = 500\n",
      "n_edges = countnz(A) = 10853\n",
      "n_dangling_nodes = n_pages - countnz(r) = 103\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# connectivity matrix\n",
    "A = readcsv(\"A.txt\")\n",
    "# Make diagonals 0 so that probability of going from page i to page i is 0 !\n",
    "A = A - Diagonal(A)\n",
    "# Let Julia know A is a sparse matrix since most entries are 0's!\n",
    "A = sparse(A);\n",
    "# number of pages\n",
    "@show n_pages = size(A, 1)\n",
    "# number of edges (page links)\n",
    "@show n_edges = countnz(A)\n",
    "# number of dangling nodes (pages with no out links)\n",
    "r = sum(A, 2) #number of outlinks per page\n",
    "@show n_dangling_nodes = n_pages - countnz(r) # total number of pages minus those with out links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500-element Array{String,1}:\n",
       " \"http://www.ucla.edu\"                                                                                                                     \n",
       " \"http://4393665.fls.doubleclick.net/activityi;src=4393665;type=Prima0;cat=UCLAe0;ord=\"                                                    \n",
       " \"http://www.atmos.ucla.edu/weather/quick-look\"                                                                                            \n",
       " \"http://bit.ly/10Wf0Te\"                                                                                                                   \n",
       " \"http://bit.ly/1NMd3KF\"                                                                                                                   \n",
       " \"http://bit.ly/1JTGEHi\"                                                                                                                   \n",
       " \"http://bit.ly/1JTGEHj\"                                                                                                                   \n",
       " \"http://bit.ly/1JTGEHk\"                                                                                                                   \n",
       " \"http://bit.ly/1TooUWd\"                                                                                                                   \n",
       " \"http://bit.ly/1Sm3c4v\"                                                                                                                   \n",
       " \"http://bit.ly/1OV2jxs\"                                                                                                                   \n",
       " \"http://newsroom.ucla.edu/stories/international-olympic-committee-president-tours-facilities-at-ucla\"                                     \n",
       " \"http://newsroom.ucla.edu/releases/chemical-used-to-replace-bpa-in-plastic-accelerates-embryonic-development-disrupts-reproductive-system\"\n",
       " ⋮                                                                                                                                         \n",
       " \"http://schema.org/audio\"                                                                                                                 \n",
       " \"http://schema.org/AudioObject\"                                                                                                           \n",
       " \"http://schema.org/author\"                                                                                                                \n",
       " \"http://schema.org/Organization\"                                                                                                          \n",
       " \"http://schema.org/award\"                                                                                                                 \n",
       " \"http://schema.org/character\"                                                                                                             \n",
       " \"http://schema.org/citation\"                                                                                                              \n",
       " \"http://schema.org/CreativeWork\"                                                                                                          \n",
       " \"http://schema.org/comment\"                                                                                                               \n",
       " \"http://schema.org/Comment\"                                                                                                               \n",
       " \"http://schema.org/commentCount\"                                                                                                          \n",
       " \"http://schema.org/contentLocation\"                                                                                                       "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# U = readcsv(\"U.txt\")\n",
    "U = readlines(\"U.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 500 pages in the U.txt file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2(4)\n",
    "\n",
    "Set the _teleportation_ parameter at $p = 0.85$. Try the following methods to solve for $\\mathbf{x}$ using the `ucla.zip` data.\n",
    "\n",
    "0. A dense linear system solver such as LU decomposition.  \n",
    "0. A simple iterative linear system solver such as Jacobi or Gauss-Seidel.   \n",
    "0. A dense eigen-solver.  \n",
    "0. A simple iterative eigen-solver such as the power method.  \n",
    "\n",
    "For iterative methods, you can use the [`IterativeSolvers.jl`](https://github.com/JuliaMath/IterativeSolvers.jl) package. Make sure to utilize the special structure of $\\mathbf{P}$ (diagonal + rank 1) to speed up the matrix-vector multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2(4) Solution:\n",
    "\n",
    "### Using LU-decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LU_Decomposition_Linear_Solver (generic function with 1 method)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first make P\n",
    "# Matrix P is the sum of a sparse matrix + a rank 1 matrix\n",
    "n = size(A, 1)\n",
    "p = 0.85\n",
    "r = sum(A, 2) #sum each row of A to get r_i for each page i\n",
    "P = zeros(n, n)\n",
    "term1 = zeros(n)\n",
    "term2 = repeat([1/n], outer=[n]) \n",
    "for i in 1:n\n",
    "    if r[i] > 0 \n",
    "        term1[i] = p / r[i]\n",
    "        term2[i] = (1 - p) / n\n",
    "    end\n",
    "end\n",
    "#first make P^T\n",
    "P_transpose = (Diagonal(term1) * sparse(A) .+ term2*(ones(1, n))).'\n",
    "\n",
    "#then make C = (I- P^t)\n",
    "C = I - P_transpose\n",
    "C[1, :] = ones(n)      \n",
    "b = zeros(n)\n",
    "b[1] = 1.0\n",
    "\n",
    "x_old = ones(n, 1) / n\n",
    "x_new = zeros(n)\n",
    "function LU_Decomposition_Linear_Solver(C, b)\n",
    "#use the fact that each column sums to 1 and replace the first row of C with ones\n",
    "#then solve x = C \\ [1, 0, ... , 0] \n",
    "x = C \\ b\n",
    "    return x\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500-element Array{Float64,1}:\n",
       " 0.0127701 \n",
       " 0.00130393\n",
       " 0.00405405\n",
       " 0.00398338\n",
       " 0.00130393\n",
       " 0.00130393\n",
       " 0.00130393\n",
       " 0.00130393\n",
       " 0.00130393\n",
       " 0.00130393\n",
       " 0.00130393\n",
       " 0.00145083\n",
       " 0.00141309\n",
       " ⋮         \n",
       " 0.0011959 \n",
       " 0.00149245\n",
       " 0.0011959 \n",
       " 0.00345944\n",
       " 0.00211007\n",
       " 0.0011959 \n",
       " 0.0011959 \n",
       " 0.0084935 \n",
       " 0.0011959 \n",
       " 0.00149103\n",
       " 0.0011959 \n",
       " 0.0011959 "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_LU = LU_Decomposition_Linear_Solver(C, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2(5)\n",
    "\n",
    "List the top 20 ranked URLs you found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first concatenate 1:n and then sort and then get indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500×2 Array{Float64,2}:\n",
       " 0.0127701     1.0\n",
       " 0.00130393    2.0\n",
       " 0.00405405    3.0\n",
       " 0.00398338    4.0\n",
       " 0.00130393    5.0\n",
       " 0.00130393    6.0\n",
       " 0.00130393    7.0\n",
       " 0.00130393    8.0\n",
       " 0.00130393    9.0\n",
       " 0.00130393   10.0\n",
       " 0.00130393   11.0\n",
       " 0.00145083   12.0\n",
       " 0.00141309   13.0\n",
       " ⋮                \n",
       " 0.0011959   489.0\n",
       " 0.00149245  490.0\n",
       " 0.0011959   491.0\n",
       " 0.00345944  492.0\n",
       " 0.00211007  493.0\n",
       " 0.0011959   494.0\n",
       " 0.0011959   495.0\n",
       " 0.0084935   496.0\n",
       " 0.0011959   497.0\n",
       " 0.00149103  498.0\n",
       " 0.0011959   499.0\n",
       " 0.0011959   500.0"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using DataFrames\n",
    "indices = collect(1:n)\n",
    "x_with_indices = hcat(x, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500×2 Array{Float64,2}:\n",
       " 0.0228785    459.0\n",
       " 0.0193529     64.0\n",
       " 0.0127701      1.0\n",
       " 0.0115253     27.0\n",
       " 0.0107311     29.0\n",
       " 0.0084935    496.0\n",
       " 0.00810174   462.0\n",
       " 0.00796066    36.0\n",
       " 0.00754457    28.0\n",
       " 0.00676869    43.0\n",
       " 0.00653717    57.0\n",
       " 0.00653717    60.0\n",
       " 0.00652746    62.0\n",
       " ⋮                 \n",
       " 0.00075957    80.0\n",
       " 0.000756004  348.0\n",
       " 0.000756004  345.0\n",
       " 0.000756004  340.0\n",
       " 0.000756004  342.0\n",
       " 0.000756004  352.0\n",
       " 0.000756004  336.0\n",
       " 0.000756004  350.0\n",
       " 0.000754551  405.0\n",
       " 0.000748165  408.0\n",
       " 0.000746597  134.0\n",
       " 0.000746597   85.0"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted = sortrows(x_with_indices, rev = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that there is a 4 way tie between the 19th - 22nd sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22×2 Array{Float64,2}:\n",
       " 0.0228785   459.0\n",
       " 0.0193529    64.0\n",
       " 0.0127701     1.0\n",
       " 0.0115253    27.0\n",
       " 0.0107311    29.0\n",
       " 0.0084935   496.0\n",
       " 0.00810174  462.0\n",
       " 0.00796066   36.0\n",
       " 0.00754457   28.0\n",
       " 0.00676869   43.0\n",
       " 0.00653717   57.0\n",
       " 0.00653717   60.0\n",
       " 0.00652746   62.0\n",
       " 0.00652746   63.0\n",
       " 0.00652746   61.0\n",
       " 0.0065169    55.0\n",
       " 0.0065169    54.0\n",
       " 0.0065169    59.0\n",
       " 0.00650538   56.0\n",
       " 0.00650538   58.0\n",
       " 0.00650538   53.0\n",
       " 0.00650538   51.0"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top22 = sorted[1:22, 1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22-element Array{Any,1}:\n",
       " \"http://schema.org\"                                                                     \n",
       " \"http://www\"                                                                            \n",
       " \"http://www.ucla.edu\"                                                                   \n",
       " \"http://www.directory.ucla.edu\"                                                         \n",
       " \"http://www.universityofcalifornia.edu\"                                                 \n",
       " \"http://schema.org/CreativeWork\"                                                        \n",
       " \"http://schema.org/Text\"                                                                \n",
       " \"http://giveto.ucla.edu\"                                                                \n",
       " \"http://www.registrar.ucla.edu/calendar\"                                                \n",
       " \"http://www.uclalumni.net\"                                                              \n",
       " \"http://www.magazine.ucla.edu/depts/lifesigns/honoring-adolescence\"                     \n",
       " \"http://www.magazine.ucla.edu/depts/lifesigns/breaking-the-bad\"                         \n",
       " \"http://www.magazine.ucla.edu/depts/atissue/local-thought-global-action\"                \n",
       " \"http://www.magazine.ucla.edu/depts/hailhills/uncle-welton-in-westwood\"                 \n",
       " \"http://www.magazine.ucla.edu/depts/forward-thinker/homes-with-heart\"                   \n",
       " \"http://www.magazine.ucla.edu/depts/style/the-dancing-scientist\"                        \n",
       " \"http://www.magazine.ucla.edu/features/patient-test-thyself\"                            \n",
       " \"http://www.magazine.ucla.edu/depts/style/welcome-to-the-jungle\"                        \n",
       " \"http://www.magazine.ucla.edu/features/splendor-in-the-trash\"                           \n",
       " \"http://www.magazine.ucla.edu/features/not-your-fathers-mba\"                            \n",
       " \"http://www.magazine.ucla.edu/exclusives/brotherly-love\"                                \n",
       " \"http://www.magazine.ucla.edu/features/hope-is-real-the-ucla-depression-grand-challenge\""
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top22_indices = convert(Array{Int64}, top22[1:22, 2])\n",
    "top22_pages = U[top22_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jacobi Iterative Method\n",
    "\n",
    ">Note that for our problem we are finding the solution $\\mathbf{x}$ to the equation $\\mathbf{C}\\mathbf{x} = \\mathbf{b},$\n",
    "where $\\mathbf{C} = (\\mathbf{I} - \\mathbf{P^{T}})$\n",
    "\n",
    "$$x_i^{(t+1)} = \\frac{b_i - \\sum_{j=1}^{i-1} c_{ij} x_j^{(t)} - \\sum_{j=i+1}^n c_{ij} x_j^{(t)}}{c_{ii}}$$\n",
    "\n",
    "Splitting up Matrix A into the sum of a Lower Triangular, Diagonal and Upper triangular matrix we have: \n",
    "$$\\mathbf{C} = \\mathbf{L} + \\mathbf{D} + \\mathbf{U}$$<br>\n",
    "$$x_i^{(t+1)} = -\\mathbf{D^{-1}}\\mathbf{C}x_i^{(t)} + x_i^{(t)} + \\mathbf{D^{-1}}\\mathbf{b}$$\n",
    "\n",
    ">Now substituting back in $\\mathbf{C} = (\\mathbf{I} - \\mathbf{P^{T}})$ we have:\n",
    "$$x_i^{(t+1)} = -\\mathbf{D^{-1}}(\\mathbf{I} - \\mathbf{P^{T}})x_i^{(t)} + x_i^{(t)} + \\mathbf{D^{-1}}\\mathbf{b}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Jacobi_Linear_Solver (generic function with 2 methods)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function Jacobi_Linear_Solver(A::AbstractMatrix, telep::Float64 = 0.85; \n",
    "        maxiter::Int = 10000, tolerance::Float64 = 1e-9, \n",
    "        x_old::Vector = repeat([1/size(A, 1)], outer=[size(A, 1)]))\n",
    "#first make P\n",
    "# Matrix P is the sum of a sparse matrix + a rank 1 matrix\n",
    "n = size(A, 1)\n",
    "r = sum(A, 2) #sum each row of A to get r_i for each page i\n",
    "P = zeros(n, n)\n",
    "term1 = zeros(n)\n",
    "term2 = repeat([1/n], outer=[n]) \n",
    "for i in 1:n\n",
    "    if r[i] > 0 \n",
    "        term1[i] = telep / r[i]\n",
    "        term2[i] = (1 - telep) / n\n",
    "    end\n",
    "end\n",
    "#first make P^T\n",
    "P_transpose = (Diagonal(term1) * sparse(A) .+ term2*(ones(1, n))).'\n",
    "\n",
    "#then make C = (I- P^t)\n",
    "C = I - P_transpose\n",
    "b = zeros(n)\n",
    "    \n",
    "x_new = zeros(n)\n",
    "x_old = ones(n, 1) / n\n",
    "\n",
    "D_inv = inv(Diagonal(C)) \n",
    "for i in 1:maxiter\n",
    "x_new = -D_inv * (C * x_old) .+ x_old .+ (D_inv * b)\n",
    "    \n",
    "if vecnorm(x_new - x_old) < tolerance\n",
    "        break\n",
    "         end\n",
    "     end\n",
    " return x_new\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500×1 Array{Float64,2}:\n",
       " 0.015434   \n",
       " 0.000831992\n",
       " 0.0045823  \n",
       " 0.00451133 \n",
       " 0.000831992\n",
       " 0.000831992\n",
       " 0.000831992\n",
       " 0.000831992\n",
       " 0.000831992\n",
       " 0.000831992\n",
       " 0.000831992\n",
       " 0.00100879 \n",
       " 0.00109823 \n",
       " ⋮          \n",
       " 0.000920927\n",
       " 0.00143108 \n",
       " 0.000920927\n",
       " 0.00381179 \n",
       " 0.00150396 \n",
       " 0.000920927\n",
       " 0.000920927\n",
       " 0.0111382  \n",
       " 0.000920927\n",
       " 0.00142913 \n",
       " 0.000920927\n",
       " 0.000919089"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_jacobi = Jacobi_Linear_Solver(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.015755321026345477"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecnorm(x_LU - x_jacobi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense Eigen-Solver\n",
    "\n",
    "It is well-known that x is the left eigenvector corresponding to the top eigenvalue 1 of the transition matrix  P.  That is $P^{T}x = x$. Therefore x can be solved as an eigen-problem.\n",
    " \n",
    "We first perform eigen decomposition on $P^{T}$ to get the left eigenvector corresponding to the top eigenvalue 1, and find vector $x_i$, the proportion of times the surfer visits page $i$ in the long run. Therefore `x_eigen_solvers` serves as page ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500-element Array{Float64,1}:\n",
       " 0.0127701 \n",
       " 0.00130393\n",
       " 0.00405405\n",
       " 0.00398338\n",
       " 0.00130393\n",
       " 0.00130393\n",
       " 0.00130393\n",
       " 0.00130393\n",
       " 0.00130393\n",
       " 0.00130393\n",
       " 0.00130393\n",
       " 0.00145083\n",
       " 0.00141309\n",
       " ⋮         \n",
       " 0.0011959 \n",
       " 0.00149245\n",
       " 0.0011959 \n",
       " 0.00345944\n",
       " 0.00211007\n",
       " 0.0011959 \n",
       " 0.0011959 \n",
       " 0.0084935 \n",
       " 0.0011959 \n",
       " 0.00149103\n",
       " 0.0011959 \n",
       " 0.0011959 "
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# eig-decomposition C\n",
    "C = eigfact((P_transpose)) # this gets the eigen decomp of P^T which has the eigen values and vectors\n",
    "\n",
    "# we get the real elements of the first column of eigenvectors \n",
    "eigenvals_weights = real(C[:vectors][:, 1]) \n",
    "\n",
    "# use eigen solvers to get the long run frequency of each page\n",
    "x_eigen_solvers = eigenvals_weights / sum(eigenvals_weights) # find the proportion of time spent in each page over total time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2(6)\n",
    "\n",
    "As of Monday May 11 2018, there are at least 1.83 billion indexed webpages on internet according to <http://www.worldwidewebsize.com/>. Explain whether each of these methods works for the PageRank problem at this scale."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "87px",
    "width": "252px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
