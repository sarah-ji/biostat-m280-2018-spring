{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biostat M280 Homework 5\n",
    "\n",
    "Sarah Ji\n",
    "\n",
    "**Due June 15 @ 11:59PM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider again the MLE of the Dirichlet-multinomial model. In [HW4](http://hua-zhou.github.io/teaching/biostatm280-2018spring/hw/hw4/hw04.html), we worked out a Newton's method. In this homework, we explore the MM and EM approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1\n",
    "\n",
    "Show that, given iid observations $\\mathbf{x}_1,\\ldots,\\mathbf{x}_n$, the log-likelihood can be written as\n",
    "$$\n",
    "L(\\alpha) = \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i}\n",
    "+\\sum_{i=1}^n \\sum_{j=1}^d \\sum_{k=0}^{x_{ij}-1} \\ln(\\alpha_j+k) - \\sum_{i=1}^n \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\ln(|\\alpha|+k).\n",
    "$$\n",
    "Hint: $\\Gamma(a + k) / \\Gamma(a) = a (a + 1) \\cdots (a+k-1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 Solution:\n",
    "\n",
    "From HW 4, we have the following log-likelihood:\n",
    "$$\n",
    "L(\\alpha) = \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} + \\sum_{i=1}^n \\sum_{j=1}^d [\\ln \\Gamma(\\alpha_j + x_{ij}) - \\ln \\Gamma(\\alpha_j)] - \\sum_{i=1}^n [\\ln \\Gamma(|\\alpha|+|\\mathbf{x}_i|) - \\ln \\Gamma(|\\alpha|)].\n",
    "$$\n",
    "\n",
    ">Since the log of a quotient is the difference of the log of the numerator minus the denominator, we can rewrite the differences as fractions:\n",
    "$$\n",
    "L(\\alpha) = \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} + \\sum_{i=1}^n \\sum_{j=1}^d \\Big[\\ln \\cfrac{\\Gamma(\\alpha_j + x_{ij}) }{ \\Gamma(\\alpha_j)}\\Big] - \\sum_{i=1}^n \\Big[\\ln\\cfrac{ \\Gamma(|\\alpha|+|\\mathbf{x}_i|)}{\\Gamma(|\\alpha|)}\\Big]\n",
    "$$\n",
    "\n",
    ">Now using the hint, $\\Gamma(a + k) / \\Gamma(a) = a (a + 1) \\cdots (a+k-1)$, we can rewrite the gamma fractions in the likelihood as:\n",
    "$$\n",
    "= \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} + \\sum_{i=1}^n\\sum_{j=1}^d \\ln[\\alpha(\\alpha+1)\\cdots(\\alpha + x_{ij} - 1) ] - \\sum_{i=1}^n \\ln[|\\alpha|(|\\alpha|+1)\\cdots(|\\alpha| + |x_{ij}| - 1)]\n",
    "$$\n",
    "\n",
    ">Since the log of a product is the sum of the logs, now we can rewrite the terms:\n",
    "$$\n",
    "\\ln[\\alpha(\\alpha+1)\\cdots(\\alpha + x_{ij} - 1)] = \\sum_{k=0}^{x_{ij}-1} \\ln(\\alpha_j+k),\n",
    "$$ \n",
    "\n",
    "$$ \n",
    "ln[|\\alpha|(|\\alpha|+1)\\cdots(|\\alpha| + |x_{ij}| - 1)] = \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\ln(|\\alpha|+k)\n",
    "$$\n",
    "\n",
    "> And we get:\n",
    "$$\n",
    "= \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i}\n",
    "+\\sum_{i=1}^n \\sum_{j=1}^d \\sum_{k=0}^{x_{ij}-1} \\ln(\\alpha_j+k) - \\sum_{i=1}^n \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\ln(|\\alpha|+k) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2\n",
    "\n",
    "Suppose $(P_1,\\ldots,P_d) \\in \\Delta_d = \\{\\mathbf{p}: p_i \\ge 0, \\sum_i p_i = 1\\}$ follows a Dirichlet distribution with parameter $\\alpha = (\\alpha_1,\\ldots,\\alpha_d)$. Show that\n",
    "$$\n",
    "\t\\mathbf{E}(\\ln P_j) = \\Psi(\\alpha_j) - \\Psi(|\\alpha|),\n",
    "$$\n",
    "where $\\Psi(z) = \\Gamma'(z)/\\Gamma(z)$ is the digamma function and $|\\alpha| = \\sum_{j=1}^d \\alpha_j$. Hint: Differentiate the identity \n",
    "$$\n",
    "1 = \\int_{\\Delta_d} \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1} \\, d\\mathbf{p}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2 Solution:\n",
    "\n",
    "Using the hint, we will differentiate both sides of the identity to get:\n",
    "\n",
    "$$\n",
    "0 = \\frac{d}{d\\alpha_j}\\int_{\\Delta_d} \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1} \\, d\\mathbf{p}.\n",
    "$$\n",
    "\n",
    "> Note that while the derivative of the constant 1 in the left hand side is 0, we are taking the derivative of a product of two functions in the right hand side, thus we will proceed with the applying the product rule: (fg)' = fg' + gf'. Here I define the two functions f and g to be:\n",
    "\n",
    "$$\n",
    "f = \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "g = \\int_{\\Delta_d}\\prod_{j=1}^d p_j^{\\alpha_j-1} \\, d\\mathbf{p}\n",
    "$$\n",
    "\n",
    ">I take the first derivatives of both functions f and g to get the following f' and g' respectively:\n",
    "\n",
    "$$\n",
    "f' = \\frac{df}{d\\alpha_j} = \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\frac{-\\Gamma'(\\alpha_j)}{\\Gamma(\\alpha_j)} + \\frac{\\Gamma'(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\n",
    "$$\n",
    "<br>\n",
    "\n",
    ">Recall from LONG TIME AGO, $\\frac{d}{dx} a^x = ln(a) * a^x,$ applying that to take the derivative of g, we get:\n",
    "$$\n",
    "g' = \\frac{dg}{d\\alpha_j} = \\int_{\\Delta_d} \\Big[\\frac{d}{d\\alpha_j} \\prod_{j=1}^d p_j^{\\alpha_j-1} \\,\\Big] d\\mathbf{p} = \\int_{\\Delta_d} \\Big[ln(p_j) \\prod_{j=1}^d p_j^{\\alpha_j-1}\\Big] d\\mathbf{p}\n",
    "$$\n",
    "\n",
    ">Finally putting the Chain Rule in action, we have (fg)' = fg' + gf':\n",
    "$$\n",
    "(fg)' = \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\Big[\\int_{\\Delta_d} \\Big[ln(p_j) \\prod_{j=1}^d p_j^{\\alpha_j-1}\\Big] d\\mathbf{p}\\Big] + \\int_{\\Delta_d}\\prod_{j=1}^d p_j^{\\alpha_j-1} \\, d\\mathbf{p}\\Big[\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\frac{-\\Gamma'(\\alpha_j)}{\\Gamma(\\alpha_j)} + \\frac{\\Gamma'(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\Big]\n",
    "$$\n",
    "\n",
    "> A smart thing to do is to multiply the last term, $\\frac{\\Gamma'(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)},$ by $\\frac{\\Gamma(|\\alpha|)}{\\Gamma(|\\alpha|)} = 1,$ to get:\n",
    "\n",
    "$$\n",
    "(fg)' = \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\Big[\\int_{\\Delta_d} \\Big[ln(p_j) \\prod_{j=1}^d p_j^{\\alpha_j-1}\\Big] d\\mathbf{p}\\Big] + \\int_{\\Delta_d}\\prod_{j=1}^d p_j^{\\alpha_j-1} \\, \\Big[\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\frac{-\\Gamma'(\\alpha_j)}{\\Gamma(\\alpha_j)} + \\frac{\\Gamma'(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\frac{\\Gamma(|\\alpha|)}{\\Gamma(|\\alpha|)}\\Big]\n",
    "$$\n",
    "\n",
    ">Now we can use our cool awesome fact, $\\Psi(z) = \\Gamma'(z)/\\Gamma(z)$ is the digamma function to replace the terms $\\frac{\\Gamma'(\\alpha_j)}{\\Gamma(\\alpha_j)} = \\Psi(\\alpha_j)$ and $\\frac{\\Gamma'(|\\alpha|)}{\\Gamma(|\\alpha|)} = \\Psi(|\\alpha|)$\n",
    "<br>\n",
    "\n",
    ">Plugging those values in, we get:\n",
    "\n",
    "$$\n",
    "(fg)' = \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\Big[\\int_{\\Delta_d} \\Big[ln(p_j) \\prod_{j=1}^d p_j^{\\alpha_j-1}\\Big] d\\mathbf{p}\\Big] + \\int_{\\Delta_d}\\prod_{j=1}^d p_j^{\\alpha_j-1} \\, \\Big[\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d -\\Gamma(\\alpha_j)}\\Psi(\\alpha_j) + \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\frac{\\Gamma'(|\\alpha|)}{\\Gamma(|\\alpha|)}\\Big]d\\mathbf{p}\n",
    "$$\n",
    "\n",
    "$$\n",
    "(fg)' = \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\Big[\\int_{\\Delta_d} \\Big[ln(p_j) \\prod_{j=1}^d p_j^{\\alpha_j-1}\\Big] d\\mathbf{p}\\Big] + \\int_{\\Delta_d}\\prod_{j=1}^d p_j^{\\alpha_j-1} \\, d\\mathbf{p}\\Big[\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d -\\Gamma(\\alpha_j)}\\Psi(\\alpha_j) + \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\Psi(|\\alpha|)\\Big]d\\mathbf{p}\n",
    "$$\n",
    "\n",
    ">Now we can combine this into one integral and pull out the common term, $\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\prod_{j=1}^d p_j^{\\alpha_j-1}$\n",
    "\n",
    "$$\n",
    "(fg)' = \\int_{\\Delta_d}\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\prod_{j=1}^d p_j^{\\alpha_j-1}\\Big[ln(p_j) - \\Big(\\Psi(\\alpha_j) - \\Psi(|\\alpha|)\\Big)\\Big]d\\mathbf{p}\n",
    "$$\n",
    "\n",
    "$$\n",
    "(fg)' = \\int_{\\Delta_d}\\Big[ln(p_j)\\Big]\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\prod_{j=1}^d p_j^{\\alpha_j-1}d\\mathbf{p} - \\Big[\\Psi(\\alpha_j) - \\Psi(|\\alpha|)\\Big]\\int_{\\Delta_d}\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\prod_{j=1}^d p_j^{\\alpha_j-1}d\\mathbf{p}\n",
    "$$\n",
    "\n",
    ">Now recall the \"Law of the Unconscious Statistician\", $E[g(x)] = \\int_{\\Delta_x} g(x) f_x(x) dx$. \n",
    "<br>\n",
    "Applying this for on $\\mathbf{E}\\Big[\\ln P_j\\Big] = \\int_{\\Delta_x} \\ln (p_j) *  f_p(p) dp$ we can see:\n",
    "\n",
    "$$\n",
    "\\mathbf{E}\\Big[\\ln P_j\\Big] = \\int_{\\Delta_x} \\ln (p_j) *  f_p(p) dp = \\int_{\\Delta_x} \\ln (p_j) *\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\prod_{j=1}^d p_j^{\\alpha_j-1}\n",
    "$$\n",
    "\n",
    ">Thus substituing back in $\\mathbf{E}\\Big[\\ln P_j\\Big]$ we have:\n",
    "\n",
    "$$\n",
    "(fg)' = \\mathbf{E}\\Big[\\ln P_j\\Big] - \\Big[\\Psi(\\alpha_j) - \\Psi(|\\alpha|)\\Big]\\int_{\\Delta_d}\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\prod_{j=1}^d p_j^{\\alpha_j-1}d\\mathbf{p}\n",
    "$$\n",
    "\n",
    ">Recall one last awesome fact! We know that the pdf of p from previous homework:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\t\\pi(\\mathbf{p}) =  \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1} ,  \n",
    "\\end{eqnarray*} \n",
    "$$\n",
    "\n",
    "$$\n",
    "1 = \\int_{\\Delta_d}\\pi(\\mathbf{p})= \\int_{\\Delta_d} \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1} \\, d\\mathbf{p}\n",
    "$$\n",
    "\n",
    ">Thus using this trick we get the derivative of the right hand side to be:\n",
    "\n",
    "$$\n",
    "(fg)' = \\mathbf{E}\\Big[\\ln P_j\\Big] - \\Big[\\Psi(\\alpha_j) - \\Psi(|\\alpha|)\\Big]1\n",
    "$$\n",
    "\n",
    "<br>\n",
    ">Thus we have the following identity:\n",
    "$$\n",
    "0 = (fg)' = \\mathbf{E}\\Big[\\ln P_j\\Big] - \\Big[\\Psi(\\alpha_j) - \\Psi(|\\alpha|)\\Big]\n",
    "$$\n",
    "\n",
    ">Now moving $\\Big[\\Psi(\\alpha_j) - \\Psi(|\\alpha|)\\Big]$ over to the left we get:\n",
    "$$\n",
    "\\mathbf{E}\\Big[\\ln P_j\\Big] = \\Big[\\Psi(\\alpha_j) - \\Psi(|\\alpha|)\\Big]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3\n",
    "\n",
    "The admixture representation of the Dirichlet-multinomial distribution suggests that we can treat the unobserved multinomial parameters $\\mathbf{p}_1,\\ldots,\\mathbf{p}_n$ as missing data and derive an EM algorithm. Show that the Q function is\n",
    "$$\n",
    "    Q(\\alpha|\\alpha^{(t)}) = \n",
    "\\sum_{j=1}^d \\sum_{i=1}^n \\alpha_j \\left[\\Psi(x_{ij}+\\alpha_j^{(t)}) - \\Psi(|\\mathbf{x}_i|+|\\alpha^{(t)}|)\\right] - n \\sum_{j=1}^d \\ln \\Gamma(\\alpha_j) + n \\ln \\Gamma(|\\alpha|) + c^{(t)},\n",
    "$$\n",
    "where $c^{(t)}$ is a constant irrelevant to optimization. Comment on why it is not easy to maximize the Q function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3 Solution:\n",
    "\n",
    "Let $X = (x_1, ... , x_n)$ be the Observed data and $\\pi(\\mathbf{p}) = (p_1, ... , p_n)$ be the missing data. We have that X and P follow the following distributions:\n",
    "\n",
    "$$\\mathbf{X} \\sim Multinomial( |\\mathbf{x}|, \\mathbf{p})$$ $$\\pi(\\mathbf{p}) \\sim Dirichlet(\\mathbf{\\alpha^{(t)}})$$\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\t\\pi(\\mathbf{p}) =  \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1},\n",
    "\\end{eqnarray*} \n",
    "$$\n",
    "where $|\\alpha|=\\sum_{j=1}^d \\alpha_j$.\n",
    "\n",
    "Using Bayesian conjugate prior properties, we know that the conditional distribution of P given X is also Dirichlet:\n",
    "\n",
    "$$\n",
    "\\mathbf{p|X} \\sim Dirichlet(\\alpha^{(t)} + \\mathbf{x})\n",
    "$$\n",
    "\n",
    ">First we get the complete density, by getting the joing pdf of $\\mathbf{X}$ and $\\mathbf{p}$ to get $f(\\mathbf{x, p} \\mid \\mathbf{\\alpha})$ as below:\n",
    "\n",
    "$$\n",
    "    f(\\mathbf{x, p} \\mid \\alpha) \n",
    "\t= \\binom{|\\mathbf{x}|}{\\mathbf{x}} \\prod_{j=1}^d p_j^{x_j} \\pi(\\mathbf{p}) \\, \n",
    "$$\n",
    "\n",
    ">Now plugging in $\\pi(\\mathbf{p})$ to the equation above we get the complete density to be:\n",
    "\n",
    "$$\n",
    "    f(\\mathbf{x, p} \\mid \\alpha) \n",
    "\t= \\binom{|\\mathbf{x}|}{\\mathbf{x}} \\prod_{j=1}^d p_j^{x_j} \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1} \\, \n",
    "$$\n",
    "\n",
    ">Then we can take the log to get the complete log pdf to be:\n",
    "\n",
    "$$\n",
    "lnf(\\mathbf{x, p} \\mid \\alpha) = \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} + \\sum_{j=1}^d x_j ln(p_j) + ln\\Big(\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\Big) + \\sum_{j=1}^d (\\alpha_j - 1) ln(p_j)\n",
    "$$\n",
    "\n",
    ">Now we can combine the two terms with the same summation over j, and pull out the $ln(p_j)$ term to get:\n",
    "\n",
    "$$\n",
    "lnf(\\mathbf{x, p} \\mid \\alpha) = \\sum_{j=1}^d ln(p_j)[x_j + (\\alpha_j - 1)] + ln\\Big(\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\Big) +\\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i}\n",
    "$$\n",
    "\n",
    ">Now let the last term, $\\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i}$ be represented by constant $c$ to get:\n",
    "\n",
    "$$\n",
    "lnf(\\mathbf{x, p} \\mid \\alpha) = \\sum_{j=1}^d ln(p_j)[x_j + (\\alpha_j - 1)] + ln\\Big(\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\Big) + c\n",
    "$$\n",
    "\n",
    ">Recall that the EM algorithm finds the conditional expectation as the surrogate function for each pdf, $Q_j(\\alpha^{(t)} \\mid \\alpha)$. Thus we get:\n",
    "\n",
    "$$ \n",
    "Q_j(\\alpha^{(t)} \\mid \\alpha) = \\mathbf{E}_{p | X = x, \\alpha^{(t)}}\\Big[lnf(\\mathbf{x, p} \\mid \\alpha)   \\Big|   X = x, \\alpha^{(t)}\\Big]\n",
    "$$\n",
    "\n",
    ">Plugging in the log pdf, $lnf(\\mathbf{x, p} \\mid \\alpha)$, from above we get:\n",
    "\n",
    "$$\n",
    "Q_j(\\alpha^{(t)} \\mid \\alpha) = \\mathbf{E}_{p | X = x, \\alpha^{(t)}}\\Big[\\sum_{j=1}^d ln(p_j)[x_j + (\\alpha_j - 1)] + ln\\Big(\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\Big) + c   \\Big|   X = x, \\alpha^{(t)}\\Big]\n",
    "$$\n",
    "\n",
    ">Thus, if we take the expecation with respect to $p$, we get: \n",
    "\n",
    "$$\n",
    "Q_j(\\alpha^{(t)} \\mid \\alpha) = \\sum_{j=1}^d \\mathbf{E}\\Big[ln(p_j)| X = x, \\alpha^{(t)}\\Big][x_j + (\\alpha_j - 1)] + ln\\Big(\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\Big) + c \n",
    "$$\n",
    "\n",
    ">Now making note of the what's given in Question 2, if $(P_1,\\ldots,P_d) \\in \\Delta_d = \\{\\mathbf{p}: p_i \\ge 0, \\sum_i p_i = 1\\}$ follows a Dirichlet distribution with parameter $\\alpha = (\\alpha_1,\\ldots,\\alpha_d)$\n",
    "\n",
    "$$\n",
    "\t\\mathbf{E}(\\ln p_j) = \\Psi(\\alpha_j) - \\Psi(|\\alpha|),\n",
    "$$\n",
    "\n",
    ">So we know that for us, since $(P_1,\\ldots,P_d \\mid x_1, \\ldots,x_d)$ follows a Dirichlet distribution with parameter $\\alpha = (\\alpha_1^{(t)} + x_1,\\ldots,\\alpha_d^{(t)}+ x_d),$ so the expectation we have is:\n",
    "\n",
    "$$\n",
    "\t\\mathbf{E}(\\ln p_j \\mid x_j, \\alpha^{(t)}) = \\Psi(\\alpha_j^{(t)} + x_j) - \\Psi(|\\alpha^{(t)}| + |\\mathbf{x}|),\n",
    "$$\n",
    "\n",
    "\n",
    ">We can replace the $\\mathbf{E}(\\ln P_j)$ to get:\n",
    "\n",
    "$$\n",
    "Q_j(\\alpha^{(t)} \\mid \\alpha) = \\sum_{j=1}^d \\Big[\\Psi(\\alpha_j^{(t)} + x_j) - \\Psi(|\\alpha^{(t)}| + |\\mathbf{x}|)\\Big][x_j + (\\alpha_j - 1)] + ln\\Big(\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\Big) + c \n",
    "$$\n",
    "\n",
    ">Distributing out the $[x_j + (\\alpha_j - 1)]$ term above, we get:\n",
    "\n",
    "\n",
    "$$\n",
    "Q_j(\\alpha^{(t)} \\mid \\alpha) = \\sum_{j=1}^d \\alpha_j\\Big[\\Psi(\\alpha_j^{(t)} + x_j) - \\Psi(|\\alpha^{(t)}| + |\\mathbf{x}|)\\Big] + x_j\\Big[\\Psi(\\alpha_j^{(t)} + x_j) - \\Psi(|\\alpha^{(t)}| + |\\mathbf{x}|)\\Big] - 1\\Big[\\Psi(\\alpha_j^{(t)} + x_j) - \\Psi(|\\alpha^{(t)}| + |\\mathbf{x}|)\\Big] + ln\\Big(\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\Big) + c \n",
    "$$\n",
    "\n",
    ">But recall that $x_j, \\alpha_j^{(t)} , |\\alpha^{(t)}|$ are simply constants, so we can combine the second and third term into the constant term $c$, which then becomes $c*$ as below:\n",
    "\n",
    "$$\n",
    "Q_j(\\alpha^{(t)} \\mid \\alpha) = \\sum_{j=1}^d \\alpha_j\\Big[\\Psi(\\alpha_j^{(t)} + x_j) - \\Psi(|\\alpha^{(t)}| + |\\mathbf{x}|)\\Big]  + ln\\Big(\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\Big) + c* \n",
    "$$\n",
    "\n",
    "\n",
    ">Now expanding the second term as the difference of the logs we get:\n",
    "\n",
    "$$\n",
    "Q_j(\\alpha^{(t)} \\mid \\alpha) = \\sum_{j=1}^d \\alpha_j\\Big[\\Psi(\\alpha_j^{(t)} + x_{j}) - \\Psi(|\\alpha^{(t)}| + |\\mathbf{x}|)\\Big]  + ln(\\Gamma(|\\alpha|) - ln(\\prod_{j=1}^d \\Gamma(\\alpha_j)) + c*\n",
    "$$\n",
    "\n",
    ">Further expanding out the log of a product as the sum of the logs we get:\n",
    "$$\n",
    "Q_j(\\alpha^{(t)} \\mid \\alpha) = \\sum_{j=1}^d \\alpha_j\\Big[\\Psi(\\alpha_j^{(t)} + x_{j}) - \\Psi(|\\alpha^{(t)}| + |\\mathbf{x}|)\\Big]  + ln(\\Gamma(|\\alpha|) - \\sum_{j=1}^d ln(\\Gamma(\\alpha_j)) + c*\n",
    "$$\n",
    "\n",
    ">Switching the last two terms we get:\n",
    "\n",
    "$$\n",
    "Q_j(\\alpha^{(t)} \\mid \\alpha) = \\sum_{j=1}^d \\alpha_j\\Big[\\Psi(\\alpha_j^{(t)} + x_{j}) - \\Psi(|\\alpha^{(t)}| + |\\mathbf{x}|)\\Big] - \\Big[\\sum_{j=1}^d ln(\\Gamma(\\alpha_j)) - ln(\\Gamma(|\\alpha|)\\Big] + c*\n",
    "$$\n",
    "\n",
    ">Now recall that each logpdf is an independent Dirichlet($x_i + \\alpha^{(t)}$), thus the overall complete likelihood is just a product of each independent pdf, and consequently, the overall loglikelihood is just the sum of the logpdfs. Thus, we have the overall surrogate function for the n independent observations to be: \n",
    "\n",
    "$$\n",
    "Q(\\alpha|\\alpha^{(t)}) = \\sum^n_{i=1}Q_i(\\alpha|\\alpha^{(t)})\n",
    "$$\n",
    "\n",
    ">Thus for the whole sample of $n$ observations we have: \n",
    "\n",
    "$$\n",
    "Q(\\alpha|\\alpha^{(t)}) = \\sum^n_{i=1}\\sum_{j=1}^d \\alpha_j\\Big[\\Psi(\\alpha_j^{(t)} + x_{ij}) - \\Psi(|\\alpha^{(t)}| + |\\mathbf{x_i}|)\\Big]  - \\sum^n_{i=1}\\Big[\\sum_{j=1}^d ln(\\Gamma(\\alpha_j))  - ln(\\Gamma(|\\alpha|)\\Big] + c**\n",
    "$$\n",
    "\n",
    ">Note that there is no $i$ index, except for the first term in the hard brackets and the constant term collects over n observations to become $c**$, a constant irrelevant to optimization so we get:\n",
    "\n",
    "$$\n",
    "Q(\\alpha|\\alpha^{(t)}) = \\sum^n_{i=1}\\sum_{j=1}^d \\alpha_j\\Big[\\Psi(\\alpha_j^{(t)} + x_{ij}) - \\Psi(|\\alpha^{(t)}| + |\\mathbf{x_i}|)\\Big]  - n\\sum_{j=1}^d ln(\\Gamma(\\alpha_j))  + nln(\\Gamma(|\\alpha|) + c**\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4\n",
    "\n",
    "We derive an MM algorithm for maximing $L$. Consider the formulation of the log-likelihood that contains terms $\\ln (\\alpha_j + k)$ and $- \\ln (|\\alpha|+k)$. Applying Jensen's inequality to the concave term $\\ln (\\alpha_j + k)$ and supporting hyperplane inequality to the convex term $- \\ln (|\\alpha|+k)$, show that a minorizing function to $L(\\alpha)$ is\n",
    "$$\n",
    "\tg(\\alpha|\\alpha^{(t)}) = - \\sum_{k=0}^{\\max_i |\\mathbf{x}_i|-1} \\frac{r_k}{|\\alpha^{(t)}|+k} |\\alpha| + \\sum_{j=1}^d \\sum_{k=0}^{\\max_i x_{ij}-1} \\frac{s_{jk} \\alpha_j^{(t)}}{\\alpha_j^{(t)}+k} \\ln \\alpha_j + c^{(t)},\n",
    "$$\n",
    "where $s_{jk} = \\sum_{i=1}^n 1_{\\{x_{ij} > k\\}}$, $r_k = \\sum_{i=1}^n 1_{\\{|\\mathbf{x}_i| > k\\}}$, and  $c^{(t)}$ is a constant irrelevant to optimization. Maximizing the surrogate function $g(\\alpha|\\alpha^{(t)})$ is trivial since $\\alpha_j$ are separated. Show that the MM updates are\n",
    "$$\n",
    "\t\\alpha_j^{(t+1)} = \\frac{\\sum_{k=0}^{\\max_i x_{ij}-1} \\frac{s_{jk}}{\\alpha_j^{(t)}+k}}{\\sum_{k=0}^{\\max_i |\\mathbf{x}_i|-1} \\frac{r_k}{|\\alpha^{(t)}|+k}} \\alpha_j^{(t)}, \\quad j=1,\\ldots,d.\n",
    "$$\n",
    "The quantities $s_{jk}$, $r_k$, $\\max_i x_{ij}$ and $\\max_i |\\mathbf{x}_i|$ only depend on data and can be pre-computed. Comment on whether the MM updates respect the parameter constraint $\\alpha_j>0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4 Solution:\n",
    "\n",
    "Our goal is to derive an MM algorithm for maximing the log likelihood, $L$. Recall that in question 1 above, we showed that the log-likelihood is of the form:\n",
    "\n",
    "$$\n",
    "L(\\alpha) = \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i}\n",
    "+\\sum_{i=1}^n \\sum_{j=1}^d \\sum_{k=0}^{x_{ij}-1} \\ln(\\alpha_j+k) - \\sum_{i=1}^n \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\ln(|\\alpha|+k).\n",
    "$$\n",
    "\n",
    "Since the formulation of the log-likelihood contains terms $\\ln (\\alpha_j + k)$ and $- \\ln (|\\alpha|+k)$, we first apply Jensen's inequality to the concave term $\\ln (\\alpha_j + k)$ and then apply the supporting hyperplane inequality to the convex term $- \\ln (|\\alpha|+k)$. \n",
    "\n",
    "\n",
    "## JENSEN'S INEQUALITY: \n",
    "\n",
    ">1) Applying Jensen's inequality to the concave term $\\ln (\\alpha_j + k)$\n",
    "\n",
    "From Ken Lange's MM Algorithms book, we have that if a function, $f$, is concave, by applying Jensen's Inequality to the function we get: \n",
    "\n",
    "$$\n",
    "f(u + v) \\ge \\Big(\\frac{u_n}{u_n + v_n}\\Big)f(\\frac{u_n + v_n}{u_n} u) + \\Big(\\frac{v_n}{u_n + v_n}\\Big)f(\\frac{u_n + v_n}{v_n} v)\n",
    "$$\n",
    "<br>\n",
    "\n",
    "Let $f = \\ln (\\alpha_j + k)$ then applying Jensen's Inequality on this concave term, we have:\n",
    "\n",
    "$$\n",
    "\\ln (\\alpha_j + k) \\ge \\Big(\\frac{\\alpha_j^{(t)}}{\\alpha_j^{(t)} + k}\\Big)\\ln\\Big(\\frac{\\alpha_j^{(t)} + k}{\\alpha_j^{(t)}} \\alpha_j\\Big) + \\Big(\\frac{k}{\\alpha_j^{(t)} + k}\\Big)\\ln\\Big(\\frac{\\alpha_j^{(t)} + k}{k} k\\Big)\n",
    "$$\n",
    "<br>\n",
    "Now, if we rewrite the log of the product as the sum of the logs, we get:\n",
    "\n",
    "$$\n",
    "\\ln (\\alpha_j + k) \\ge \\Big(\\frac{\\alpha_j^{(t)}}{\\alpha_j^{(t)} + k}\\Big) \\Big[\\ln\\Big(\\frac{\\alpha_j^{(t)} + k}{\\alpha_j^{(t)}}\\Big) +\\ln\\alpha_j\\Big] + \\Big(\\frac{k}{\\alpha_j^{(t)} + k}\\Big)\\ln\\Big(\\frac{\\alpha_j^{(t)} + k}{k} k\\Big)\n",
    "$$\n",
    "<br>\n",
    "\n",
    "Recall that $x_j, \\alpha_j^{(t)} , k $ are simply constants, so the last term, $\\ln\\Big(\\frac{\\alpha_j^{(t)} + k}{k} k\\Big)$, is also just a constant so we will leave it as is! \n",
    "<br>\n",
    "\n",
    "Distributing out the $\\Big(\\frac{\\alpha_j^{(t)}}{\\alpha_j^{(t)} + k}\\Big)$ term, we have:\n",
    "\n",
    "$$\n",
    "\\ln (\\alpha_j + k) \\ge \\Big(\\frac{\\alpha_j^{(t)}}{\\alpha_j^{(t)} + k}\\Big) \\ln\\Big(\\frac{\\alpha_j^{(t)} + k}{\\alpha_j^{(t)}}\\Big) +\\Big(\\frac{\\alpha_j^{(t)}}{\\alpha_j^{(t)} + k}\\Big) \\ln\\alpha_j + \\Big(\\frac{k}{\\alpha_j^{(t)} + k}\\Big)\\ln\\Big(\\frac{\\alpha_j^{(t)} + k}{k} k\\Big)\n",
    "$$\n",
    "<br>\n",
    "\n",
    "Notice that we can combine the constant terms together into $c_1$ to get: \n",
    "\n",
    "$$\n",
    "\\ln (\\alpha_j + k) \\ge \\Big(\\frac{\\alpha_j^{(t)}}{\\alpha_j^{(t)} + k}\\Big) \\ln\\alpha_j + c_1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HYPERPLANE INEQUALITY: \n",
    "\n",
    ">2) Applying the supporting hyperplane inequality to the convex term $- \\ln (|\\alpha|+k)$\n",
    "\n",
    "From the Hyperplane Inequality found in Chapter 1 of Ken Lange's MM Algorithms book, we have that:\n",
    "$$\n",
    "- \\ln (|\\alpha| + k) \\ge - \\ln (|\\alpha^{(t)}| + k) - \\frac{|\\alpha| - |\\alpha^{(t)}|}{|\\alpha^{(t)}| + k}\n",
    "$$\n",
    "<br>\n",
    "\n",
    "Since each $|\\alpha^{(t)}|$ is a positive sum, we know that this inequality is less than the following inequality:\n",
    "\n",
    "$$\n",
    "- \\ln (|\\alpha| + k) \\ge - \\ln (|\\alpha^{(t)}| + k) - \\frac{|\\alpha| - |\\alpha^{(t)}|}{|\\alpha^{(t)}| + k}\n",
    "$$\n",
    "$$\n",
    "\\ge \\ln (|\\alpha^{(t)}| + k) - \\frac{|\\alpha|}{|\\alpha^{(t)}| + k}\n",
    "$$\n",
    "<br>\n",
    "\n",
    "Recall that $\\alpha_j^{(t)} , |\\alpha^{(t)}|, k $ are simply constants, so the first term, $\\ln (|\\alpha^{(t)}| + k)$, is just a constant so we will replace it with $c = \\ln (|\\alpha^{(t)}| + k)$ to get:\n",
    "<br>\n",
    "\n",
    "$$\n",
    "- \\ln (|\\alpha| + k) \\ge c_2 - \\frac{|\\alpha|}{|\\alpha^{(t)}| + k}\n",
    "$$\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minorizing Function for Loglikelihood\n",
    "\n",
    "To find a minorizing function to $L(\\alpha)$, we can combine the two terms from above. \n",
    "\n",
    "Recall that the Loglikelihood has the form:\n",
    "\n",
    "$$\n",
    "L(\\alpha) = \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i}\n",
    "+\\sum_{i=1}^n \\sum_{j=1}^d \\sum_{k=0}^{x_{ij}-1} \\ln(\\alpha_j+k) - \\sum_{i=1}^n \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\ln(|\\alpha|+k).\n",
    "$$\n",
    "<br>\n",
    ">Now substituting the terms $\\ln(\\alpha_j+k)$ and $-\\ln(|\\alpha|+k)$ we have the following inequality:\n",
    "\n",
    "$$\n",
    "L(\\alpha) \\ge \\sum_{i=1}^n \\sum_{j=1}^d \\sum_{k=0}^{x_{ij}-1} \\Big(\\frac{\\alpha_j^{(t)}}{\\alpha_j^{(t)} + k}\\Big) \\ln\\alpha_j + c_1 + \\sum_{i=1}^n \\sum_{k=0}^{|\\mathbf{x}_i|-1} c_2 - \\frac{|\\alpha|}{|\\alpha^{(t)}| + k}\n",
    "$$\n",
    "<br>\n",
    "\n",
    ">Now Reordering the second term and the first term, then combining the constant terms $c_1$ and $c_2$ together into $c^{(t)}$ we have: \n",
    "\n",
    "$$\n",
    "= - \\sum_{i=1}^n \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\frac{|\\alpha|}{|\\alpha^{(t)}| + k} + \\sum_{i=1}^n \\sum_{j=1}^d \\sum_{k=0}^{x_{ij}-1} \\Big(\\frac{\\alpha_j^{(t)}}{\\alpha_j^{(t)} + k}\\Big) \\ln\\alpha_j + c^{(t)}\n",
    "$$\n",
    "\n",
    "> Defining the terms $s_{jk} = \\sum_{i=1}^n 1_{\\{x_{ij} > k\\}}$, $r_k = \\sum_{i=1}^n 1_{\\{|\\mathbf{x}_i| > k\\}}$, we can rewrite this as:\n",
    "\n",
    "$$\n",
    "= - \\sum_{k=0}^{\\max_i |\\mathbf{x}_i|-1} \\frac{|\\alpha|r_k}{|\\alpha^{(t)}| + k} + \\sum_{j=1}^d \\sum_{k=0}^{\\max_i x_{ij}-1} \\Big(\\frac{\\alpha_j^{(t)}s_{jk}}{\\alpha_j^{(t)} + k}\\Big) \\ln\\alpha_j + c^{(t)}\n",
    "$$\n",
    "\n",
    ">Therefore we have have found a minorizing function to $L(\\alpha)$, that is equal to $g(\\alpha|\\alpha^{(t)})$\n",
    "\n",
    "$$\n",
    "\tg(\\alpha|\\alpha^{(t)}) = - \\sum_{k=0}^{\\max_i |\\mathbf{x}_i|-1} \\frac{r_k}{|\\alpha^{(t)}|+k} |\\alpha| + \\sum_{j=1}^d \\sum_{k=0}^{\\max_i x_{ij}-1} \\frac{s_{jk} \\alpha_j^{(t)}}{\\alpha_j^{(t)}+k} \\ln \\alpha_j + c^{(t)},\n",
    "$$\n",
    "\n",
    "where $s_{jk} = \\sum_{i=1}^n 1_{\\{x_{ij} > k\\}}$, $r_k = \\sum_{i=1}^n 1_{\\{|\\mathbf{x}_i| > k\\}}$, and  $c^{(t)}$ is a constant irrelevant to optimization. \n",
    "<br>\n",
    "Maximizing the surrogate function $g(\\alpha|\\alpha^{(t)})$ is trivial since $\\alpha_j$ are separated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deriving the MM update\n",
    "\n",
    "To derive the MM update for $\\alpha,$ we differentiate the minorization function with respect to $\\alpha_j$, set the derivative to zero, and solve for $\\alpha_j$.\n",
    "\n",
    "$$\n",
    "\\frac{d}{d\\alpha_j} g(\\alpha|\\alpha^{(t)}) = \\frac{d}{d\\alpha_j} \\Big[ - \\sum_{k=0}^{\\max_i |\\mathbf{x}_i|-1} \\frac{r_k}{|\\alpha^{(t)}|+k} |\\alpha| + \\sum_{j=1}^d \\sum_{k=0}^{\\max_i x_{ij}-1} \\frac{s_{jk} \\alpha_j^{(t)}}{\\alpha_j^{(t)}+k} \\ln \\alpha_j + c^{(t)}\\Big] = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\t\\alpha_j^{(t+1)} = \\frac{\\sum_{k=0}^{\\max_i x_{ij}-1} \\frac{s_{jk}}{\\alpha_j^{(t)}+k}}{\\sum_{k=0}^{\\max_i |\\mathbf{x}_i|-1} \\frac{r_k}{|\\alpha^{(t)}|+k}} \\alpha_j^{(t)}, \\quad j=1,\\ldots,d.\n",
    "$$\n",
    "The quantities $s_{jk}$, $r_k$, $\\max_i x_{ij}$ and $\\max_i |\\mathbf{x}_i|$ only depend on data and can be pre-computed. \n",
    "<br>\n",
    "<br>\n",
    "Comment on whether the MM updates respect the parameter constraint $\\alpha_j>0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5\n",
    "\n",
    "Write a function for finding MLE of Dirichlet-multinomial distribution given iid observations $\\mathbf{x}_1,\\ldots,\\mathbf{x}_n$, using MM algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_mm"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    dirmult_mm(X)\n",
    "\n",
    "Find the MLE of Dirichlet-multinomial distribution using MM algorithm.\n",
    "\n",
    "# Argument\n",
    "* `X`: an `d`-by-`n` matrix of counts; each column is one data point.\n",
    "\n",
    "# Optional argument  \n",
    "* `alpha0`: starting point. \n",
    "* `maxiters`: the maximum allowable Newton iterations (default 100). \n",
    "* `tolfun`: the tolerance for  relative change in objective values (default 1e-6). \n",
    "\n",
    "# Output\n",
    "# Output\n",
    "* `logl`: the log-likelihood at MLE.   \n",
    "* `niter`: the number of iterations performed.\n",
    "# `α`: the MLE.\n",
    "* `∇`: the gradient at MLE. \n",
    "* `obsinfo`: the observed information matrix at MLE. \n",
    "\"\"\"\n",
    "function dirmult_mm(\n",
    "    X::AbstractMatrix; \n",
    "    α0::Vector = dirmult_mom(X), \n",
    "    maxiters::Int = 100, \n",
    "    tolfun = 1e-6\n",
    "    )\n",
    "    \n",
    "    # your code here\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6\n",
    "\n",
    "Re-do [HW4 Q9](http://hua-zhou.github.io/teaching/biostatm280-2018spring/hw/hw4/hw04.html#Q9) using your new `dirmult_mm` function. Compare the number of iterations and run time by MM algorithm to those by Newton's method. Comment on the efficiency of Newton's algorithm vs MM algorithm for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7\n",
    "\n",
    "Finally let us re-consider the EM algorithm. The difficulty with the M step in EM algorithm can be remedied. Discuss how we can further minorize the $\\ln \\Gamma(|\\alpha|)$ term in the $Q$ function to produce a minorizing function with all $\\alpha_j$ separated. For this homework, you do **not** need to implement this EM-MM hybrid algorithm. Hint: $z \\mapsto \\ln \\Gamma(z)$ is a convex function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "65px",
    "width": "252px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
