{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biostat M280 Homework 5\n",
    "\n",
    "Sarah Ji\n",
    "\n",
    "**Due June 15 @ 11:59PM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider again the MLE of the Dirichlet-multinomial model. In [HW4](http://hua-zhou.github.io/teaching/biostatm280-2018spring/hw/hw4/hw04.html), we worked out a Newton's method. In this homework, we explore the MM and EM approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1\n",
    "\n",
    "Show that, given iid observations $\\mathbf{x}_1,\\ldots,\\mathbf{x}_n$, the log-likelihood can be written as\n",
    "$$\n",
    "L(\\alpha) = \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i}\n",
    "+\\sum_{i=1}^n \\sum_{j=1}^d \\sum_{k=0}^{x_{ij}-1} \\ln(\\alpha_j+k) - \\sum_{i=1}^n \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\ln(|\\alpha|+k).\n",
    "$$\n",
    "Hint: $\\Gamma(a + k) / \\Gamma(a) = a (a + 1) \\cdots (a+k-1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 Solution:\n",
    "\n",
    "From HW 4, we have the following log-likelihood:\n",
    "$$\n",
    "L(\\alpha) = \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} + \\sum_{i=1}^n \\sum_{j=1}^d [\\ln \\Gamma(\\alpha_j + x_{ij}) - \\ln \\Gamma(\\alpha_j)] - \\sum_{i=1}^n [\\ln \\Gamma(|\\alpha|+|\\mathbf{x}_i|) - \\ln \\Gamma(|\\alpha|)].\n",
    "$$\n",
    "\n",
    ">Since the log of a quotient is the difference of the log of the numerator minus the denominator, we can rewrite the differences as fractions:\n",
    "$$\n",
    "L(\\alpha) = \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} + \\sum_{i=1}^n \\sum_{j=1}^d \\Big[\\ln \\cfrac{\\Gamma(\\alpha_j + x_{ij}) }{ \\Gamma(\\alpha_j)}\\Big] - \\sum_{i=1}^n \\Big[\\ln\\cfrac{ \\Gamma(|\\alpha|+|\\mathbf{x}_i|)}{\\Gamma(|\\alpha|)}\\Big]\n",
    "$$\n",
    "\n",
    ">Now using the hint, $\\Gamma(a + k) / \\Gamma(a) = a (a + 1) \\cdots (a+k-1)$, we can rewrite the gamma fractions in the likelihood as:\n",
    "$$\n",
    "= \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} + \\sum_{i=1}^n\\sum_{j=1}^d \\ln[\\alpha(\\alpha+1)\\cdots(\\alpha + x_{ij} - 1) ] - \\sum_{i=1}^n \\ln[|\\alpha|(|\\alpha|+1)\\cdots(|\\alpha| + |x_{ij}| - 1)]\n",
    "$$\n",
    "\n",
    ">Since the log of a product is the sum of the logs, now we can rewrite the terms:\n",
    "$$\n",
    "\\ln[\\alpha(\\alpha+1)\\cdots(\\alpha + x_{ij} - 1)] = \\sum_{k=0}^{x_{ij}-1} \\ln(\\alpha_j+k),\n",
    "$$ \n",
    "\n",
    "$$ \n",
    "ln[|\\alpha|(|\\alpha|+1)\\cdots(|\\alpha| + |x_{ij}| - 1)] = \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\ln(|\\alpha|+k)\n",
    "$$\n",
    "\n",
    "> And we get:\n",
    "$$\n",
    "= \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i}\n",
    "+\\sum_{i=1}^n \\sum_{j=1}^d \\sum_{k=0}^{x_{ij}-1} \\ln(\\alpha_j+k) - \\sum_{i=1}^n \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\ln(|\\alpha|+k) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2\n",
    "\n",
    "Suppose $(P_1,\\ldots,P_d) \\in \\Delta_d = \\{\\mathbf{p}: p_i \\ge 0, \\sum_i p_i = 1\\}$ follows a Dirichlet distribution with parameter $\\alpha = (\\alpha_1,\\ldots,\\alpha_d)$. Show that\n",
    "$$\n",
    "\t\\mathbf{E}(\\ln P_j) = \\Psi(\\alpha_j) - \\Psi(|\\alpha|),\n",
    "$$\n",
    "where $\\Psi(z) = \\Gamma'(z)/\\Gamma(z)$ is the digamma function and $|\\alpha| = \\sum_{j=1}^d \\alpha_j$. Hint: Differentiate the identity \n",
    "$$\n",
    "1 = \\int_{\\Delta_d} \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1} \\, d\\mathbf{p}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2 Solution:\n",
    "\n",
    "Using the hint, we will differentiate both sides of the identity to get:\n",
    "\n",
    "$$\n",
    "0 = \\frac{d}{d\\alpha_j}\\int_{\\Delta_d} \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1} \\, d\\mathbf{p}.\n",
    "$$\n",
    "\n",
    "> Note that while the derivative of the constant 1 in the left hand side is 0, we are taking the derivative of a product of two functions in the right hand side, thus we will proceed with the applying the product rule: (fg)' = fg' + gf'. Here I define the two functions f and g to be:\n",
    "\n",
    "$$\n",
    "f = \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "g = \\int_{\\Delta_d}\\prod_{j=1}^d p_j^{\\alpha_j-1} \\, d\\mathbf{p}\n",
    "$$\n",
    "\n",
    ">I take the first derivatives of both functions f and g to get the following f' and g' respectively:\n",
    "\n",
    ">Notice that taking the derivative of $f$ is a product rule on its own! $f = ab$ where $a = \\Gamma(|\\alpha|)$ and $b = \\frac{1}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}.$ Then we have that $f' = ab' + ba'$ \n",
    "\n",
    "$\n",
    "a' = \\Gamma'(|\\alpha|)\n",
    "$\n",
    "\n",
    "$\n",
    "b' = \\frac{1}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} * \\frac{1}{\\Gamma(\\alpha_j)^2} * \\Gamma'(\\alpha_j)\n",
    "$\n",
    "\n",
    ">Thus we get that $f'$:\n",
    "\n",
    "$$\n",
    "f' = \\frac{df}{d\\alpha_j} = \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\frac{-\\Gamma'(\\alpha_j)}{\\Gamma(\\alpha_j)} + \\frac{\\Gamma'(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "f' = \\frac{df}{d\\alpha_j} = \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\frac{-\\Gamma'(\\alpha_j)}{\\Gamma(\\alpha_j)} + \\frac{\\Gamma'(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\n",
    "$$\n",
    "<br>\n",
    "\n",
    ">Recall from LONG TIME AGO, $\\frac{d}{dx} a^x = ln(a) * a^x,$ applying that to take the derivative of g, we get:\n",
    "$$\n",
    "g' = \\frac{dg}{d\\alpha_j} = \\int_{\\Delta_d} \\Big[\\frac{d}{d\\alpha_j} \\prod_{j=1}^d p_j^{\\alpha_j-1} \\,\\Big] d\\mathbf{p} = \\int_{\\Delta_d} \\Big[ln(p_j) \\prod_{j=1}^d p_j^{\\alpha_j-1}\\Big] d\\mathbf{p}\n",
    "$$\n",
    "\n",
    ">Finally putting the Chain Rule in action, we have (fg)' = fg' + gf':\n",
    "$$\n",
    "(fg)' = \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\Big[\\int_{\\Delta_d} \\Big[ln(p_j) \\prod_{j=1}^d p_j^{\\alpha_j-1}\\Big] d\\mathbf{p}\\Big] + \\int_{\\Delta_d}\\prod_{j=1}^d p_j^{\\alpha_j-1} \\, d\\mathbf{p}\\Big[\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\frac{-\\Gamma'(\\alpha_j)}{\\Gamma(\\alpha_j)} + \\frac{\\Gamma'(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\Big]\n",
    "$$\n",
    "\n",
    "> A smart thing to do is to multiply the last term, $\\frac{\\Gamma'(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)},$ by $\\frac{\\Gamma(|\\alpha|)}{\\Gamma(|\\alpha|)} = 1,$ to get:\n",
    "\n",
    "$$\n",
    "(fg)' = \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\Big[\\int_{\\Delta_d} \\Big[ln(p_j) \\prod_{j=1}^d p_j^{\\alpha_j-1}\\Big] d\\mathbf{p}\\Big] + \\int_{\\Delta_d}\\prod_{j=1}^d p_j^{\\alpha_j-1} \\, \\Big[\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\frac{-\\Gamma'(\\alpha_j)}{\\Gamma(\\alpha_j)} + \\frac{\\Gamma'(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\frac{\\Gamma(|\\alpha|)}{\\Gamma(|\\alpha|)}\\Big]\n",
    "$$\n",
    "\n",
    ">Now we can use our cool awesome fact, $\\Psi(z) = \\Gamma'(z)/\\Gamma(z)$ is the digamma function to replace the terms $\\frac{\\Gamma'(\\alpha_j)}{\\Gamma(\\alpha_j)} = \\Psi(\\alpha_j)$ and $\\frac{\\Gamma'(|\\alpha|)}{\\Gamma(|\\alpha|)} = \\Psi(|\\alpha|)$\n",
    "<br>\n",
    "\n",
    ">Plugging those values in, we get:\n",
    "\n",
    "$$\n",
    "(fg)' = \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\Big[\\int_{\\Delta_d} \\Big[ln(p_j) \\prod_{j=1}^d p_j^{\\alpha_j-1}\\Big] d\\mathbf{p}\\Big] + \\int_{\\Delta_d}\\prod_{j=1}^d p_j^{\\alpha_j-1} \\, \\Big[\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d -\\Gamma(\\alpha_j)}\\Psi(\\alpha_j) + \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\frac{\\Gamma'(|\\alpha|)}{\\Gamma(|\\alpha|)}\\Big]d\\mathbf{p}\n",
    "$$\n",
    "\n",
    "$$\n",
    "(fg)' = \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\Big[\\int_{\\Delta_d} \\Big[ln(p_j) \\prod_{j=1}^d p_j^{\\alpha_j-1}\\Big] d\\mathbf{p}\\Big] + \\int_{\\Delta_d}\\prod_{j=1}^d p_j^{\\alpha_j-1} \\, d\\mathbf{p}\\Big[\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d -\\Gamma(\\alpha_j)}\\Psi(\\alpha_j) + \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\Psi(|\\alpha|)\\Big]d\\mathbf{p}\n",
    "$$\n",
    "\n",
    ">Now we can combine this into one integral and pull out the common term, $\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\prod_{j=1}^d p_j^{\\alpha_j-1}$\n",
    "\n",
    "$$\n",
    "(fg)' = \\int_{\\Delta_d}\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\prod_{j=1}^d p_j^{\\alpha_j-1}\\Big[ln(p_j) - \\Big(\\Psi(\\alpha_j) - \\Psi(|\\alpha|)\\Big)\\Big]d\\mathbf{p}\n",
    "$$\n",
    "\n",
    "$$\n",
    "(fg)' = \\int_{\\Delta_d}\\Big[ln(p_j)\\Big]\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\prod_{j=1}^d p_j^{\\alpha_j-1}d\\mathbf{p} - \\Big[\\Psi(\\alpha_j) - \\Psi(|\\alpha|)\\Big]\\int_{\\Delta_d}\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\prod_{j=1}^d p_j^{\\alpha_j-1}d\\mathbf{p}\n",
    "$$\n",
    "\n",
    ">Now recall the \"Law of the Unconscious Statistician\", $E[g(x)] = \\int_{\\Delta_x} g(x) f_x(x) dx$. \n",
    "<br>\n",
    "Applying this for on $\\mathbf{E}\\Big[\\ln P_j\\Big] = \\int_{\\Delta_x} \\ln (p_j) *  f_p(p) dp$ we can see:\n",
    "\n",
    "$$\n",
    "\\mathbf{E}\\Big[\\ln P_j\\Big] = \\int_{\\Delta_x} \\ln (p_j) *  f_p(p) dp = \\int_{\\Delta_x} \\ln (p_j) *\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\prod_{j=1}^d p_j^{\\alpha_j-1}\n",
    "$$\n",
    "\n",
    ">Thus substituing back in $\\mathbf{E}\\Big[\\ln P_j\\Big]$ we have:\n",
    "\n",
    "$$\n",
    "(fg)' = \\mathbf{E}\\Big[\\ln P_j\\Big] - \\Big[\\Psi(\\alpha_j) - \\Psi(|\\alpha|)\\Big]\\int_{\\Delta_d}\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\prod_{j=1}^d p_j^{\\alpha_j-1}d\\mathbf{p}\n",
    "$$\n",
    "\n",
    ">Recall one last awesome fact! We know that the pdf of p from previous homework:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\t\\pi(\\mathbf{p}) =  \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1} ,  \n",
    "\\end{eqnarray*} \n",
    "$$\n",
    "\n",
    "$$\n",
    "1 = \\int_{\\Delta_d}\\pi(\\mathbf{p})= \\int_{\\Delta_d} \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1} \\, d\\mathbf{p}\n",
    "$$\n",
    "\n",
    ">Thus using this trick we get the derivative of the right hand side to be:\n",
    "\n",
    "$$\n",
    "(fg)' = \\mathbf{E}\\Big[\\ln P_j\\Big] - \\Big[\\Psi(\\alpha_j) - \\Psi(|\\alpha|)\\Big]1\n",
    "$$\n",
    "\n",
    "<br>\n",
    ">Thus we have the following identity:\n",
    "$$\n",
    "0 = (fg)' = \\mathbf{E}\\Big[\\ln P_j\\Big] - \\Big[\\Psi(\\alpha_j) - \\Psi(|\\alpha|)\\Big]\n",
    "$$\n",
    "\n",
    ">Now moving $\\Big[\\Psi(\\alpha_j) - \\Psi(|\\alpha|)\\Big]$ over to the left we get:\n",
    "$$\n",
    "\\mathbf{E}\\Big[\\ln P_j\\Big] = \\Big[\\Psi(\\alpha_j) - \\Psi(|\\alpha|)\\Big]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3\n",
    "\n",
    "The admixture representation of the Dirichlet-multinomial distribution suggests that we can treat the unobserved multinomial parameters $\\mathbf{p}_1,\\ldots,\\mathbf{p}_n$ as missing data and derive an EM algorithm. Show that the Q function is\n",
    "$$\n",
    "    Q(\\alpha|\\alpha^{(t)}) = \n",
    "\\sum_{j=1}^d \\sum_{i=1}^n \\alpha_j \\left[\\Psi(x_{ij}+\\alpha_j^{(t)}) - \\Psi(|\\mathbf{x}_i|+|\\alpha^{(t)}|)\\right] - n \\sum_{j=1}^d \\ln \\Gamma(\\alpha_j) + n \\ln \\Gamma(|\\alpha|) + c^{(t)},\n",
    "$$\n",
    "where $c^{(t)}$ is a constant irrelevant to optimization. Comment on why it is not easy to maximize the Q function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3 Solution:\n",
    "\n",
    "Let $\\mathbf{X} = \\mathbf{x}_1,\\ldots,\\mathbf{x}_n$ be the Observed data and $\\mathbf{p} = (p_1, ... , p_n)$ be the missing data. We have that X and P follow the following distributions:\n",
    "\n",
    "$$\\mathbf{X} \\sim Multinomial( |\\mathbf{x}|, \\mathbf{p})$$ $$\\pi(\\mathbf{p}) \\sim Dirichlet(\\mathbf{\\alpha})$$\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\t\\pi(\\mathbf{p}) =  \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1},\n",
    "\\end{eqnarray*} \n",
    "$$\n",
    "where $|\\alpha|=\\sum_{j=1}^d \\alpha_j$.\n",
    "\n",
    "To use iterations, we have the $t^{th}$ update of $\\alpha = \\alpha^{(t)}$ to get: \n",
    "\n",
    "$$\\mathbf{X} \\sim Multinomial( |\\mathbf{x}|, \\mathbf{p})$$ $$\\pi(\\mathbf{p}) \\sim Dirichlet(\\mathbf{\\alpha^{(t)}})$$\n",
    "<br>\n",
    "\n",
    "Using Bayesian conjugate prior properties, we know that the conditional distribution of $\\mathbf{p}$ given $\\mathbf{X}$ is also Dirichlet:\n",
    "\n",
    "$$\n",
    "\\mathbf{p|X = x} \\sim Dirichlet(\\alpha^{(t)} + \\mathbf{x})\n",
    "$$\n",
    "\n",
    ">That is, $(P_1,\\ldots,P_d \\mid x_1, \\ldots,x_d)$ follows a Dirichlet distribution with parameter $\\alpha = (\\alpha_1^{(t)} + x_1,\\ldots,\\alpha_d^{(t)}+ x_d),$\n",
    "\n",
    ">First we get the complete density, by getting the joint pdf of $\\mathbf{X}$ and $\\mathbf{p}$ to get $f(\\mathbf{x, p} \\mid \\mathbf{\\alpha})$ for one observation as below:\n",
    "\n",
    "$$\n",
    "    f(\\mathbf{x, p} \\mid \\alpha) \n",
    "\t= \\binom{|\\mathbf{x}|}{\\mathbf{x}} \\prod_{j=1}^d p_j^{x_j} \\pi(\\mathbf{p}) \\, \n",
    "$$\n",
    "\n",
    ">Now plugging in $\\pi(\\mathbf{p})$ to the equation above we get the complete density to be:\n",
    "\n",
    "$$\n",
    "    f(\\mathbf{x, p} \\mid \\alpha) \n",
    "\t= \\binom{|\\mathbf{x}|}{\\mathbf{x}} \\prod_{j=1}^d p_j^{x_j} \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1} \\, \n",
    "$$\n",
    "\n",
    ">Then we can take the log to get the complete log pdf to be:\n",
    "\n",
    "$$\n",
    "lnf(\\mathbf{x, p} \\mid \\alpha) = \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} + \\sum_{j=1}^d x_j ln(p_j) + ln\\Big(\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\Big) + \\sum_{j=1}^d (\\alpha_j - 1) ln(p_j)\n",
    "$$\n",
    "\n",
    ">Now we can combine the two terms with the same summation over j, and pull out the $ln(p_j)$ term to get:\n",
    "\n",
    "$$\n",
    "lnf(\\mathbf{x, p} \\mid \\alpha) = \\sum_{j=1}^d ln(p_j)[x_j + (\\alpha_j - 1)] + ln\\Big(\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\Big) +\\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i}\n",
    "$$\n",
    "\n",
    ">Now let the last term, $\\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i}$ be represented by constant $c$ to get:\n",
    "\n",
    "$$\n",
    "lnf(\\mathbf{x, p} \\mid \\alpha) = \\sum_{j=1}^d ln(p_j)[x_j + (\\alpha_j - 1)] + ln\\Big(\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\Big) + c\n",
    "$$\n",
    "\n",
    ">Recall that the EM algorithm finds the conditional expectation as the surrogate function for each pdf, $Q_j(\\alpha^{(t)} \\mid \\alpha)$. Thus we get:\n",
    "\n",
    "$$ \n",
    "Q_j(\\alpha^{(t)} \\mid \\alpha) = \\mathbf{E}_{p | X = \\mathbf{x}, \\alpha^{(t)}}\\Big[lnf(\\mathbf{x, p} \\mid \\alpha)   \\Big|   X = \\mathbf{x}, \\alpha = \\alpha^{(t)}\\Big]\n",
    "$$\n",
    "\n",
    ">Plugging in the log pdf, $lnf(\\mathbf{x, p} \\mid \\alpha)$, from above we get:\n",
    "\n",
    "$$\n",
    "Q_j(\\alpha^{(t)} \\mid \\alpha) = \\mathbf{E}_{p | X = x, \\alpha^{(t)}}\\Big[\\sum_{j=1}^d ln(p_j)[x_j + (\\alpha_j - 1)] + ln\\Big(\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\Big) + c   \\Big|   X = \\mathbf{x}, \\alpha = \\alpha^{(t)}\\Big]\n",
    "$$\n",
    "\n",
    ">Thus, if we take the expecation with respect to $p$, we get the surrogate function for the $j^{th}$ person: \n",
    "\n",
    "$$\n",
    "Q_j(\\alpha^{(t)} \\mid \\alpha) = \\sum_{j=1}^d \\mathbf{E}\\Big[ln(p_j)| X = \\mathbf{x}, \\alpha = \\alpha^{(t)}\\Big][x_j + (\\alpha_j - 1)] + ln\\Big(\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\Big) + c \n",
    "$$\n",
    "\n",
    ">Now making note of what's given in Question 2, if $(P_1,\\ldots,P_d) \\in \\Delta_d = \\{\\mathbf{p}: p_i \\ge 0, \\sum_i p_i = 1\\}$ follows a Dirichlet distribution with parameter $\\alpha = (\\alpha_1,\\ldots,\\alpha_d)$\n",
    "\n",
    "$$\n",
    "\t\\mathbf{E}(\\ln p_j) = \\Psi(\\alpha_j) - \\Psi(|\\alpha|),\n",
    "$$\n",
    "\n",
    ">So we know that for us, since $(P_1,\\ldots,P_d \\mid x_1, \\ldots,x_d)$ follows a Dirichlet distribution with parameter $\\alpha = (\\alpha_1^{(t)} + x_1,\\ldots,\\alpha_d^{(t)}+ x_d),$ so the expectation we have is:\n",
    "\n",
    "$$\n",
    "\t\\mathbf{E}(\\ln p_j \\mid x_j, \\alpha^{(t)}) = \\Psi(\\alpha_j^{(t)} + x_j) - \\Psi(|\\alpha^{(t)}| + |\\mathbf{x}|),\n",
    "$$\n",
    "\n",
    "\n",
    ">We can replace the $\\mathbf{E}(\\ln P_j)$ to get:\n",
    "\n",
    "$$\n",
    "Q_j(\\alpha^{(t)} \\mid \\alpha) = \\sum_{j=1}^d \\Big[\\Psi(\\alpha_j^{(t)} + x_j) - \\Psi(|\\alpha^{(t)}| + |\\mathbf{x}|)\\Big][x_j + (\\alpha_j - 1)] + ln\\Big(\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\Big) + c \n",
    "$$\n",
    "\n",
    ">Distributing out the $[x_j + (\\alpha_j - 1)]$ term above, we get:\n",
    "\n",
    "\n",
    "$$\n",
    "Q_j(\\alpha^{(t)} \\mid \\alpha) = \\sum_{j=1}^d \\alpha_j\\Big[\\Psi(\\alpha_j^{(t)} + x_j) - \\Psi(|\\alpha^{(t)}| + |\\mathbf{x}|)\\Big] + x_j\\Big[\\Psi(\\alpha_j^{(t)} + x_j) - \\Psi(|\\alpha^{(t)}| + |\\mathbf{x}|)\\Big] - 1\\Big[\\Psi(\\alpha_j^{(t)} + x_j) - \\Psi(|\\alpha^{(t)}| + |\\mathbf{x}|)\\Big] + ln\\Big(\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\Big) + c \n",
    "$$\n",
    "\n",
    ">But recall that $x_j, \\alpha_j^{(t)} , |\\alpha^{(t)}|$ are simply constants, so we can combine the second and third term into the constant term $c$, which then becomes $c*$ as below:\n",
    "\n",
    "$$\n",
    "Q_j(\\alpha^{(t)} \\mid \\alpha) = \\sum_{j=1}^d \\alpha_j\\Big[\\Psi(\\alpha_j^{(t)} + x_j) - \\Psi(|\\alpha^{(t)}| + |\\mathbf{x}|)\\Big]  + ln\\Big(\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\Big) + c* \n",
    "$$\n",
    "\n",
    "\n",
    ">Now expanding the second term as the difference of the logs we get:\n",
    "\n",
    "$$\n",
    "Q_j(\\alpha^{(t)} \\mid \\alpha) = \\sum_{j=1}^d \\alpha_j\\Big[\\Psi(\\alpha_j^{(t)} + x_{j}) - \\Psi(|\\alpha^{(t)}| + |\\mathbf{x}|)\\Big]  + ln(\\Gamma(|\\alpha|) - ln(\\prod_{j=1}^d \\Gamma(\\alpha_j)) + c*\n",
    "$$\n",
    "\n",
    ">Further expanding out the log of a product as the sum of the logs we get:\n",
    "$$\n",
    "Q_j(\\alpha^{(t)} \\mid \\alpha) = \\sum_{j=1}^d \\alpha_j\\Big[\\Psi(\\alpha_j^{(t)} + x_{j}) - \\Psi(|\\alpha^{(t)}| + |\\mathbf{x}|)\\Big]  + ln(\\Gamma(|\\alpha|) - \\sum_{j=1}^d ln(\\Gamma(\\alpha_j)) + c*\n",
    "$$\n",
    "\n",
    ">Switching the last two terms we get:\n",
    "\n",
    "$$\n",
    "Q_j(\\alpha^{(t)} \\mid \\alpha) = \\sum_{j=1}^d \\alpha_j\\Big[\\Psi(\\alpha_j^{(t)} + x_{j}) - \\Psi(|\\alpha^{(t)}| + |\\mathbf{x}|)\\Big] - \\Big[\\sum_{j=1}^d ln(\\Gamma(\\alpha_j)) - ln(\\Gamma(|\\alpha|)\\Big] + c*\n",
    "$$\n",
    "\n",
    ">Now recall that each logpdf is an independent Dirichlet($x_i + \\alpha^{(t)}$), thus the overall complete likelihood is just a product of each independent pdf, and consequently, the overall loglikelihood is just the sum of the logpdfs. Thus, we have the overall surrogate function for the n independent observations to be: \n",
    "\n",
    "$$\n",
    "Q(\\alpha|\\alpha^{(t)}) = \\sum^n_{i=1}Q_i(\\alpha|\\alpha^{(t)})\n",
    "$$\n",
    "\n",
    ">Thus for the whole sample of $n$ observations we have: \n",
    "\n",
    "$$\n",
    "Q(\\alpha|\\alpha^{(t)}) = \\sum^n_{i=1}\\sum_{j=1}^d \\alpha_j\\Big[\\Psi(\\alpha_j^{(t)} + x_{ij}) - \\Psi(|\\alpha^{(t)}| + |\\mathbf{x_i}|)\\Big]  - \\sum^n_{i=1}\\Big[\\sum_{j=1}^d ln(\\Gamma(\\alpha_j))  - ln(\\Gamma(|\\alpha|)\\Big] + c**\n",
    "$$\n",
    "\n",
    ">Note that there is no $i$ index, except for the first term in the hard brackets and the constant term collects over n observations to become $c**$, a constant irrelevant to optimization so we get:\n",
    "\n",
    "$$\n",
    "Q(\\alpha|\\alpha^{(t)}) = \\sum^n_{i=1}\\sum_{j=1}^d \\alpha_j\\Big[\\Psi(\\alpha_j^{(t)} + x_{ij}) - \\Psi(|\\alpha^{(t)}| + |\\mathbf{x_i}|)\\Big]  - n\\sum_{j=1}^d ln(\\Gamma(\\alpha_j))  + nln(\\Gamma(|\\alpha|) + c**\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4\n",
    "\n",
    "We derive an MM algorithm for maximing $L$. Consider the formulation of the log-likelihood that contains terms $\\ln (\\alpha_j + k)$ and $- \\ln (|\\alpha|+k)$. Applying Jensen's inequality to the concave term $\\ln (\\alpha_j + k)$ and supporting hyperplane inequality to the convex term $- \\ln (|\\alpha|+k)$, show that a minorizing function to $L(\\alpha)$ is\n",
    "$$\n",
    "\tg(\\alpha|\\alpha^{(t)}) = - \\sum_{k=0}^{\\max_i |\\mathbf{x}_i|-1} \\frac{r_k}{|\\alpha^{(t)}|+k} |\\alpha| + \\sum_{j=1}^d \\sum_{k=0}^{\\max_i x_{ij}-1} \\frac{s_{jk} \\alpha_j^{(t)}}{\\alpha_j^{(t)}+k} \\ln \\alpha_j + c^{(t)},\n",
    "$$\n",
    "where $s_{jk} = \\sum_{i=1}^n 1_{\\{x_{ij} > k\\}}$, $r_k = \\sum_{i=1}^n 1_{\\{|\\mathbf{x}_i| > k\\}}$, and  $c^{(t)}$ is a constant irrelevant to optimization. Maximizing the surrogate function $g(\\alpha|\\alpha^{(t)})$ is trivial since $\\alpha_j$ are separated. Show that the MM updates are\n",
    "$$\n",
    "\t\\alpha_j^{(t+1)} = \\frac{\\sum_{k=0}^{\\max_i x_{ij}-1} \\frac{s_{jk}}{\\alpha_j^{(t)}+k}}{\\sum_{k=0}^{\\max_i |\\mathbf{x}_i|-1} \\frac{r_k}{|\\alpha^{(t)}|+k}} \\alpha_j^{(t)}, \\quad j=1,\\ldots,d.\n",
    "$$\n",
    "The quantities $s_{jk}$, $r_k$, $\\max_i x_{ij}$ and $\\max_i |\\mathbf{x}_i|$ only depend on data and can be pre-computed. Comment on whether the MM updates respect the parameter constraint $\\alpha_j>0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4 Solution:\n",
    "\n",
    "Our goal is to derive an MM algorithm for maximing the log likelihood, $L$. Recall that in question 1 above, we showed that the log-likelihood is of the form:\n",
    "\n",
    "$$\n",
    "L(\\alpha) = \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i}\n",
    "+\\sum_{i=1}^n \\sum_{j=1}^d \\sum_{k=0}^{x_{ij}-1} \\ln(\\alpha_j+k) - \\sum_{i=1}^n \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\ln(|\\alpha|+k).\n",
    "$$\n",
    "\n",
    "Since the formulation of the log-likelihood contains terms $\\ln (\\alpha_j + k)$ and $- \\ln (|\\alpha|+k)$, we first apply Jensen's inequality to the concave term $\\ln (\\alpha_j + k)$ and then apply the supporting hyperplane inequality to the convex term $- \\ln (|\\alpha|+k)$. \n",
    "\n",
    "\n",
    "## JENSEN'S INEQUALITY: \n",
    "\n",
    ">1) Applying Jensen's inequality to the concave term $\\ln (\\alpha_j + k)$\n",
    "\n",
    "From Ken Lange's MM Algorithms book, we have that if a function, $f$, is concave, by applying Jensen's Inequality to the function we get: \n",
    "\n",
    "$$\n",
    "f(u + v) \\ge \\Big(\\frac{u_n}{u_n + v_n}\\Big)f(\\frac{u_n + v_n}{u_n} u) + \\Big(\\frac{v_n}{u_n + v_n}\\Big)f(\\frac{u_n + v_n}{v_n} v)\n",
    "$$\n",
    "<br>\n",
    "\n",
    "Let $f = \\ln (\\alpha_j + k)$ then applying Jensen's Inequality on this concave term, we have:\n",
    "\n",
    "$$\n",
    "\\ln (\\alpha_j + k) \\ge \\Big(\\frac{\\alpha_j^{(t)}}{\\alpha_j^{(t)} + k}\\Big)\\ln\\Big(\\frac{\\alpha_j^{(t)} + k}{\\alpha_j^{(t)}} \\alpha_j\\Big) + \\Big(\\frac{k}{\\alpha_j^{(t)} + k}\\Big)\\ln\\Big(\\frac{\\alpha_j^{(t)} + k}{k} k\\Big)\n",
    "$$\n",
    "<br>\n",
    "Now, if we rewrite the log of the product as the sum of the logs, we get:\n",
    "\n",
    "$$\n",
    "\\ln (\\alpha_j + k) \\ge \\Big(\\frac{\\alpha_j^{(t)}}{\\alpha_j^{(t)} + k}\\Big) \\Big[\\ln\\Big(\\frac{\\alpha_j^{(t)} + k}{\\alpha_j^{(t)}}\\Big) +\\ln\\alpha_j\\Big] + \\Big(\\frac{k}{\\alpha_j^{(t)} + k}\\Big)\\ln\\Big(\\frac{\\alpha_j^{(t)} + k}{k} k\\Big)\n",
    "$$\n",
    "<br>\n",
    "\n",
    "Recall that $x_j, \\alpha_j^{(t)} , k $ are simply constants, so the last term, $\\ln\\Big(\\frac{\\alpha_j^{(t)} + k}{k} k\\Big)$, is also just a constant so we will leave it as is! \n",
    "<br>\n",
    "\n",
    "Distributing out the $\\Big(\\frac{\\alpha_j^{(t)}}{\\alpha_j^{(t)} + k}\\Big)$ term, we have:\n",
    "\n",
    "$$\n",
    "\\ln (\\alpha_j + k) \\ge \\Big(\\frac{\\alpha_j^{(t)}}{\\alpha_j^{(t)} + k}\\Big) \\ln\\Big(\\frac{\\alpha_j^{(t)} + k}{\\alpha_j^{(t)}}\\Big) +\\Big(\\frac{\\alpha_j^{(t)}}{\\alpha_j^{(t)} + k}\\Big) \\ln\\alpha_j + \\Big(\\frac{k}{\\alpha_j^{(t)} + k}\\Big)\\ln\\Big(\\frac{\\alpha_j^{(t)} + k}{k} k\\Big)\n",
    "$$\n",
    "<br>\n",
    "\n",
    "Notice that we can combine the constant terms together into $c_1$ to get: \n",
    "\n",
    "$$\n",
    "\\ln (\\alpha_j + k) \\ge \\Big(\\frac{\\alpha_j^{(t)}}{\\alpha_j^{(t)} + k}\\Big) \\ln\\alpha_j + c_1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HYPERPLANE INEQUALITY: \n",
    "\n",
    ">2) Applying the supporting hyperplane inequality to the convex term $- \\ln (|\\alpha|+k)$\n",
    "\n",
    "From the Hyperplane Inequality found in Chapter 1 of Ken Lange's MM Algorithms book, we have that:\n",
    "$$\n",
    "- \\ln (|\\alpha| + k) \\ge - \\ln (|\\alpha^{(t)}| + k) - \\frac{|\\alpha| - |\\alpha^{(t)}|}{|\\alpha^{(t)}| + k}\n",
    "$$\n",
    "<br>\n",
    "\n",
    "Since each $|\\alpha^{(t)}|$ is a positive sum, we know that this inequality is less than the following inequality:\n",
    "\n",
    "$$\n",
    "- \\ln (|\\alpha| + k) \\ge - \\ln (|\\alpha^{(t)}| + k) - \\frac{|\\alpha| - |\\alpha^{(t)}|}{|\\alpha^{(t)}| + k}\n",
    "$$\n",
    "$$\n",
    "\\ge -\\ln (|\\alpha^{(t)}| + k) - \\frac{|\\alpha|}{|\\alpha^{(t)}| + k}\n",
    "$$\n",
    "<br>\n",
    "\n",
    "Recall that $\\alpha_j^{(t)} , |\\alpha^{(t)}|, k $ are simply constants, so the first term, $\\ln (|\\alpha^{(t)}| + k)$, is just a constant so we will replace it with $c = \\ln (|\\alpha^{(t)}| + k)$ to get:\n",
    "<br>\n",
    "\n",
    "$$\n",
    "- \\ln (|\\alpha| + k) \\ge c_2 - \\frac{|\\alpha|}{|\\alpha^{(t)}| + k}\n",
    "$$\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minorizing Function for Loglikelihood\n",
    "\n",
    "To find a minorizing function to $L(\\alpha)$, we can combine the two terms from above. \n",
    "\n",
    "Recall that the Loglikelihood has the form:\n",
    "\n",
    "$$\n",
    "L(\\alpha) = \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i}\n",
    "+\\sum_{i=1}^n \\sum_{j=1}^d \\sum_{k=0}^{x_{ij}-1} \\ln(\\alpha_j+k) - \\sum_{i=1}^n \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\ln(|\\alpha|+k).\n",
    "$$\n",
    "<br>\n",
    ">Now substituting the terms $\\ln(\\alpha_j+k)$ and $-\\ln(|\\alpha|+k)$ we have the following inequality:\n",
    "\n",
    "$$\n",
    "L(\\alpha) \\ge \\sum_{i=1}^n \\sum_{j=1}^d \\sum_{k=0}^{x_{ij}-1} \\Big(\\frac{\\alpha_j^{(t)}}{\\alpha_j^{(t)} + k}\\Big) \\ln\\alpha_j + c_1 + \\sum_{i=1}^n \\sum_{k=0}^{|\\mathbf{x}_i|-1} c_2 - \\frac{|\\alpha|}{|\\alpha^{(t)}| + k}\n",
    "$$\n",
    "<br>\n",
    "\n",
    ">Now Reordering the second term and the first term, then combining the constant terms $c_1$ and $c_2$ together into $c^{(t)}$ we have: \n",
    "\n",
    "$$\n",
    "= - \\sum_{i=1}^n \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\frac{|\\alpha|}{|\\alpha^{(t)}| + k} + \\sum_{i=1}^n \\sum_{j=1}^d \\sum_{k=0}^{x_{ij}-1} \\Big(\\frac{\\alpha_j^{(t)}}{\\alpha_j^{(t)} + k}\\Big) \\ln\\alpha_j + c^{(t)}\n",
    "$$\n",
    "\n",
    "> Defining the terms $s_{jk} = \\sum_{i=1}^n 1_{\\{x_{ij} > k\\}}$, $r_k = \\sum_{i=1}^n 1_{\\{|\\mathbf{x}_i| > k\\}}$, we can rewrite this as:\n",
    "\n",
    "$$\n",
    "= - \\sum_{k=0}^{\\max_i |\\mathbf{x}_i|-1} \\frac{|\\alpha|r_k}{|\\alpha^{(t)}| + k} + \\sum_{j=1}^d \\sum_{k=0}^{\\max_i x_{ij}-1} \\Big(\\frac{\\alpha_j^{(t)}s_{jk}}{\\alpha_j^{(t)} + k}\\Big) \\ln\\alpha_j + c^{(t)}\n",
    "$$\n",
    "\n",
    ">Therefore we have have found a minorizing function to $L(\\alpha)$, that is equal to $g(\\alpha|\\alpha^{(t)})$\n",
    "\n",
    "$$\n",
    "\tg(\\alpha|\\alpha^{(t)}) = - \\sum_{k=0}^{\\max_i |\\mathbf{x}_i|-1} \\frac{r_k}{|\\alpha^{(t)}|+k} |\\alpha| + \\sum_{j=1}^d \\sum_{k=0}^{\\max_i x_{ij}-1} \\frac{s_{jk} \\alpha_j^{(t)}}{\\alpha_j^{(t)}+k} \\ln \\alpha_j + c^{(t)},\n",
    "$$\n",
    "\n",
    "where $s_{jk} = \\sum_{i=1}^n 1_{\\{x_{ij} > k\\}}$, $r_k = \\sum_{i=1}^n 1_{\\{|\\mathbf{x}_i| > k\\}}$, and  $c^{(t)}$ is a constant irrelevant to optimization. \n",
    "<br>\n",
    "Maximizing the surrogate function $g(\\alpha|\\alpha^{(t)})$ is trivial since $\\alpha_j$ are separated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deriving the MM update\n",
    "\n",
    "To derive the MM update for $\\alpha,$ we differentiate the minorization function with respect to $\\alpha_j$, set the derivative to zero, and solve for $\\alpha_j$.\n",
    "\n",
    "$$\n",
    "\\frac{d}{d\\alpha_j} g(\\alpha|\\alpha^{(t)}) = \\frac{d}{d\\alpha_j} \\Big[ - \\sum_{k=0}^{\\max_i |\\mathbf{x}_i|-1} \\frac{r_k}{|\\alpha^{(t)}|+k} |\\alpha| + \\sum_{j=1}^d \\sum_{k=0}^{\\max_i x_{ij}-1} \\frac{s_{jk} \\alpha_j^{(t)}}{\\alpha_j^{(t)}+k} \\ln \\alpha_j + c^{(t)}\\Big] = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\t\\alpha_j^{(t+1)} = \\frac{\\sum_{k=0}^{\\max_i x_{ij}-1} \\frac{s_{jk}}{\\alpha_j^{(t)}+k}}{\\sum_{k=0}^{\\max_i |\\mathbf{x}_i|-1} \\frac{r_k}{|\\alpha^{(t)}|+k}} \\alpha_j^{(t)}, \\quad j=1,\\ldots,d.\n",
    "$$\n",
    "The quantities $s_{jk}$, $r_k$, $\\max_i x_{ij}$ and $\\max_i |\\mathbf{x}_i|$ only depend on data and can be pre-computed. \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "To comment on whether the MM updates respect the parameter constraint $\\alpha_j > 0$, it does because we initialize $\\alpha$ with positive values, thus the update in the next iteration will also be positive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5\n",
    "\n",
    "Write a function for finding MLE of Dirichlet-multinomial distribution given iid observations $\\mathbf{x}_1,\\ldots,\\mathbf{x}_n$, using MM algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5 Solution:\n",
    "\n",
    "First we use the `log_pdf(X)` functions and the `dirmult_mom(X)` function from the previous homework as below, and use them to find the MLE of the Dirichlet-multinomial distribution using the MM algorithm below in the function `dirmult_mm`.\n",
    "\n",
    "## Finding log_pdf of the Dirichlet-multinomial distribution with parameter `α` at data point `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_logpdf"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using SpecialFunctions\n",
    "\"\"\"\n",
    "    dirmult_logpdf(x::Vector, α::Vector)\n",
    "    \n",
    "Compute the log-pdf of Dirichlet-multinomial distribution with parameter `α` \n",
    "at data point `x`.\n",
    "\"\"\"\n",
    "function dirmult_logpdf(x::Vector, α::Vector)\n",
    "    Logpdf = 0.0\n",
    "    sumalpha = sum(α)\n",
    "    sumx = sum(x)\n",
    "    Logpdf = lfact(convert(Int,sumx)) + lgamma(sumalpha) - lgamma(sumx + sumalpha)\n",
    "    for i in 1:length(x)\n",
    "        if α[i] == 0 && x[i] == 0\n",
    "            Logpdf += 0\n",
    "            elseif α[i] == 0 && x[i] > 0\n",
    "            Logpdf += - Inf\n",
    "        else\n",
    "            Logpdf += lgamma(α[i] + x[i]) - lfact(x[i]) - lgamma(α[i])\n",
    "        end\n",
    "    end\n",
    "    return Logpdf\n",
    "end\n",
    "\n",
    "function dirmult_logpdf!(r::Vector, X::Matrix, α::Vector)\n",
    "    for j in 1:size(X, 2)\n",
    "        r[j] = dirmult_logpdf(X[:, j], α)\n",
    "    end\n",
    "    return r\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    dirmult_logpdf(X, α)\n",
    "    \n",
    "Compute the log-pdf of Dirichlet-multinomial distribution with parameter `α` \n",
    "at each data point in `X`. Each column of `X` is one data point.\n",
    "\"\"\"\n",
    "function dirmult_logpdf(X::Matrix, α::Vector)\n",
    "    r = zeros(size(X, 2))\n",
    "    dirmult_logpdf!(r, X, α)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Method of Moments estimate for initial alpha values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_mom (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## code for finding initial alpha\n",
    "function dirmult_mom(X::Matrix{Int})\n",
    "    n, d = size(X)\n",
    "    alpha_0 = zeros(d)#creates 64x1 matrix\n",
    "    Z_hat = 0.0\n",
    "    Z_numerator = 0.0\n",
    "    sum_Xcol = sum(X, 1)\n",
    "    sum_Xrow = sum(X, 2)\n",
    "    for j in 1:d\n",
    "        if sum_Xcol[j] < 0.001 #check to see if there is data in the column\n",
    "            continue\n",
    "        end\n",
    "        \n",
    "        Z_numerator = 0.0 #reset each value\n",
    "        for i in 1:n #for each column\n",
    "            Z_numerator += (X[i, j] / sum_Xrow[i])^2\n",
    "            alpha_0[j] += (X[i, j] / sum_Xrow[i])\n",
    "        end\n",
    "        \n",
    "        Z_hat += (Z_numerator / alpha_0[j]) #calculating Zhat\n",
    "        alpha_0[j] /= n #divide by n in front of the sigma\n",
    "\n",
    "    end\n",
    "    alpha_0 *= ((d - Z_hat) / (Z_hat - 1))\n",
    "    return alpha_0\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the MLE, loglikelihood value at the MLE, the gradient at the MLE, the observed information at the MLE and the number of iterations for convergence of Dirichlet-multinomial distribution using MM algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_mm"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    dirmult_mm(X)\n",
    "\n",
    "Find the MLE of Dirichlet-multinomial distribution using MM algorithm.\n",
    "\n",
    "# Argument\n",
    "* `X`: an `d`-by-`n` matrix of counts; each column is one data point.\n",
    "\n",
    "# Optional argument  \n",
    "* `alpha0`: starting point. \n",
    "* `maxiters`: the maximum allowable Newton iterations (default 100). \n",
    "* `tolfun`: the tolerance for  relative change in objective values (default 1e-6). \n",
    "\n",
    "# Output\n",
    "# Output\n",
    "* `logl`: the log-likelihood at MLE.   \n",
    "* `niter`: the number of iterations performed.\n",
    "# `α`: the MLE.\n",
    "* `∇`: the gradient at MLE. \n",
    "* `obsinfo`: the observed information matrix at MLE. \n",
    "\"\"\"\n",
    "function dirmult_mm(\n",
    "        X::AbstractMatrix; \n",
    "        α0::Vector = dirmult_mom(X'), \n",
    "        maxiters::Int = 100, \n",
    "        tolfun = 1e-6,\n",
    "    )\n",
    "    \n",
    "    #calculate max_i(x_ijk)  and max_i(|x_i|)\n",
    "    d, n = size(X)\n",
    "    #numerator \n",
    "    max_xij = maximum(maximum(X, 2) - 1)\n",
    "    #denominator\n",
    "    col_sum_X = sum(X, 1)\n",
    "    max_sum_xi = maximum(col_sum_X) - 1\n",
    "\n",
    "    #calculate s_jk , r_k\n",
    "    s_jk = zeros(d, max_xij + 1)\n",
    "    r_k = zeros(max_sum_xi + 1)\n",
    "    \n",
    "    for i in 0:max_sum_xi\n",
    "        r_k[i + 1] += countnz(col_sum_X .> i)\n",
    "    end\n",
    "    \n",
    "    for k in 0:max_xij, j in 1:d\n",
    "            s_jk[j, k + 1] = countnz(X[j, :] .> k)\n",
    "        end\n",
    "    \n",
    "    α_new = zeros(d)\n",
    "    α_old = copy(α0)\n",
    "    loglold = sum(dirmult_logpdf(X, α0))\n",
    "    niter = 0\n",
    "    \n",
    "    # of alpha update\n",
    "    for iter in 1:maxiters\n",
    "        αupdate_num = zeros(d)\n",
    "        αupdate_denom = 0.0\n",
    "        sum_α = sum(α_old)\n",
    "        for k in 0:max_sum_xi\n",
    "            if (sum_α + k) == 0 \n",
    "                continue\n",
    "            end\n",
    "            αupdate_denom += r_k[k + 1] / (sum_α + k)\n",
    "        end\n",
    "        \n",
    "        for k in 0:max_xij, j in 1:d\n",
    "            if (α_old[j] + k) == 0\n",
    "                continue\n",
    "            end\n",
    "            αupdate_num[j] += s_jk[j, k + 1] / (α_old[j] + k)\n",
    "        end\n",
    "        \n",
    "        α_new = (α_old .* αupdate_num) / αupdate_denom\n",
    "        \n",
    "        logl = sum(dirmult_logpdf(X, α_new))\n",
    "        \n",
    "        if abs(logl - loglold) < tolfun * (abs(loglold) + 1)\n",
    "            niter = iter\n",
    "            break;\n",
    "        end\n",
    "        \n",
    "        α_old = α_new\n",
    "        loglold = logl \n",
    "    end\n",
    "   \n",
    "    logl = sum(dirmult_logpdf(X, α_new))\n",
    "    \n",
    "    row_sumα = sum(α_new)\n",
    "    row_sum_X = sum(X, 2)\n",
    "    ∇ = zeros(d)\n",
    "    obsinfo = zeros(d, d)\n",
    "    \n",
    "    # evaluate gradient (score)\n",
    "    dg_α = digamma.(α_new)\n",
    "    dg_α_sum = digamma.(row_sumα)\n",
    "    \n",
    "    for j in 1:d, i in 1:n\n",
    "        if row_sum_X[j] == 0\n",
    "            continue\n",
    "        end\n",
    "            ∇[j] += digamma.(α_new[j] + X[j, i]) - dg_α[j] - \n",
    "            digamma.(row_sumα + col_sum_X[i]) + dg_α_sum\n",
    "    end\n",
    "\n",
    "    D = zeros(d)\n",
    "    c = 0.0\n",
    "    tg_α = trigamma.(α_new)\n",
    "    tg_α_sum = trigamma.(row_sumα)\n",
    "    \n",
    "    for j = 1:d, i = 1:n\n",
    "        if row_sum_X[j] > 0\n",
    "            D[j] += - trigamma.(α_new[j] + X[j, i]) + tg_α[j]\n",
    "            c += - trigamma.(row_sumα + col_sum_X[i]) + tg_α_sum\n",
    "        end\n",
    "    end\n",
    "\n",
    "    obsinfo = Diagonal(D) + (c * ones(d, d))\n",
    "    \n",
    "    α_mle = α_new\n",
    "    \n",
    "    return logl, niter, α_mle, ∇, obsinfo\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = readcsv(\"optdigits.tra\", Int)\n",
    "X = traindata[:, 1:64]'\n",
    "logl, niter, α_mle, ∇, obsinfo = dirmult_mm(X);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  125.89 MiB\n",
       "  allocs estimate:  405090\n",
       "  --------------\n",
       "  minimum time:     539.663 ms (1.05% GC)\n",
       "  median time:      566.135 ms (1.04% GC)\n",
       "  mean time:        570.114 ms (2.18% GC)\n",
       "  maximum time:     627.329 ms (10.48% GC)\n",
       "  --------------\n",
       "  samples:          9\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using BenchmarkTools\n",
    "@benchmark dirmult_mm(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Iterations\n",
    "Running the function on the training set, we have that the MM algorithm finds the MLE in 34 iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "niter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## α_mle\n",
    "\n",
    "These estimates of α_mle are similar in value to that of Newton's method, which is good~!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64-element Array{Float64,1}:\n",
       " 0.0        \n",
       " 0.0809462  \n",
       " 0.896006   \n",
       " 2.10579    \n",
       " 2.02128    \n",
       " 0.76506    \n",
       " 0.146443   \n",
       " 0.0151447  \n",
       " 0.000389661\n",
       " 0.301589   \n",
       " 1.74968    \n",
       " 2.14562    \n",
       " 1.87275    \n",
       " ⋮          \n",
       " 1.66123    \n",
       " 1.40425    \n",
       " 0.460086   \n",
       " 0.0272511  \n",
       " 0.000129835\n",
       " 0.0654668  \n",
       " 0.920939   \n",
       " 2.11522    \n",
       " 1.94414    \n",
       " 0.946323   \n",
       " 0.23171    \n",
       " 0.0277971  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "α_mle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ∇\n",
    "Here is the gradient, ∇, of the Loglikelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64-element Array{Float64,1}:\n",
       "  0.0    \n",
       " -4.02214\n",
       " -6.94268\n",
       " -8.17294\n",
       " -8.14829\n",
       " -6.9806 \n",
       " -4.74385\n",
       " -3.91382\n",
       " -3.81641\n",
       " -5.24808\n",
       " -8.2123 \n",
       " -8.04137\n",
       " -7.99428\n",
       "  ⋮      \n",
       " -7.90485\n",
       " -8.28078\n",
       " -6.35788\n",
       " -3.93669\n",
       " -3.81469\n",
       " -4.03058\n",
       " -7.11613\n",
       " -8.2327 \n",
       " -8.31926\n",
       " -7.59352\n",
       " -5.23192\n",
       " -3.9599 "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "∇"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observed Information Matrix\n",
    "\n",
    "Here is the observed information matrix, otherwise known as the negative hessian, calculated as the negative second derivative of the loglikelihood function. Notice how each entry is positive, ensuring a good positive definite matrix!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64×64 Array{Float64,2}:\n",
       " 4261.83   4261.83  4261.83  4261.83  …  4261.83   4261.83    4261.83\n",
       " 4261.83  95061.2   4261.83  4261.83     4261.83   4261.83    4261.83\n",
       " 4261.83   4261.83  9422.67  4261.83     4261.83   4261.83    4261.83\n",
       " 4261.83   4261.83  4261.83  6188.59     4261.83   4261.83    4261.83\n",
       " 4261.83   4261.83  4261.83  4261.83     4261.83   4261.83    4261.83\n",
       " 4261.83   4261.83  4261.83  4261.83  …  4261.83   4261.83    4261.83\n",
       " 4261.83   4261.83  4261.83  4261.83     4261.83   4261.83    4261.83\n",
       " 4261.83   4261.83  4261.83  4261.83     4261.83   4261.83    4261.83\n",
       " 4261.83   4261.83  4261.83  4261.83     4261.83   4261.83    4261.83\n",
       " 4261.83   4261.83  4261.83  4261.83     4261.83   4261.83    4261.83\n",
       " 4261.83   4261.83  4261.83  4261.83  …  4261.83   4261.83    4261.83\n",
       " 4261.83   4261.83  4261.83  4261.83     4261.83   4261.83    4261.83\n",
       " 4261.83   4261.83  4261.83  4261.83     4261.83   4261.83    4261.83\n",
       "    ⋮                                 ⋱                              \n",
       " 4261.83   4261.83  4261.83  4261.83     4261.83   4261.83    4261.83\n",
       " 4261.83   4261.83  4261.83  4261.83     4261.83   4261.83    4261.83\n",
       " 4261.83   4261.83  4261.83  4261.83     4261.83   4261.83    4261.83\n",
       " 4261.83   4261.83  4261.83  4261.83  …  4261.83   4261.83    4261.83\n",
       " 4261.83   4261.83  4261.83  4261.83     4261.83   4261.83    4261.83\n",
       " 4261.83   4261.83  4261.83  4261.83     4261.83   4261.83    4261.83\n",
       " 4261.83   4261.83  4261.83  4261.83     4261.83   4261.83    4261.83\n",
       " 4261.83   4261.83  4261.83  4261.83     4261.83   4261.83    4261.83\n",
       " 4261.83   4261.83  4261.83  4261.83  …  4261.83   4261.83    4261.83\n",
       " 4261.83   4261.83  4261.83  4261.83     8808.2    4261.83    4261.83\n",
       " 4261.83   4261.83  4261.83  4261.83     4261.83  29518.4     4261.83\n",
       " 4261.83   4261.83  4261.83  4261.83     4261.83   4261.83  272335.0 "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obsinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6\n",
    "\n",
    "Re-do [HW4 Q9](http://hua-zhou.github.io/teaching/biostatm280-2018spring/hw/hw4/hw04.html#Q9) using your new `dirmult_mm` function. Compare the number of iterations and run time by MM algorithm to those by Newton's method. Comment on the efficiency of Newton's algorithm vs MM algorithm for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6 Solution:\n",
    "\n",
    "Below, I re-do HW4 Q9, which finds the MLE for each digit (`α_MLE_mm`), the loglikelihood at the MLE (`logl_mm`), along with the number of iterations (`niter`) and the computation time (`time`) to find the MLE, using my new `dirmult_mm` function above. \n",
    "\n",
    "Comparing the number of iterations and run time by MM algorithm to those by Newton's method Newton's algorithm took 18 iterations to converge to the MLE (in HW 4) while the MM algorithm takes 34 iterations to converge to the MLE (this HW). \n",
    "\n",
    "However, while Newton's algorithm takes fewer iterations to converge, each iteration takes a longer computing time than each iteration of the MM algorithm. That is, the per iteration cost of the MM algorithm is cheaper than that of Newton's alogrithm.\n",
    "\n",
    "Recall tha Newton's method may be unstable, so we needed to implement a line search to check the step-size and tweak the observed information matrix to ensure positive definiteness. From above, we see that these are no longer issues in the MM-algorithm. Thus, the MM-algorithm may be more efficient to implement, as there are less conditions to check for, but takes more iterations to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time: 1.238399451 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: imported binding for time overwritten in module Main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time: 0.079741091 seconds\n",
      "elapsed time: 0.019048413 seconds\n",
      "elapsed time: 0.083260568 seconds\n",
      "elapsed time: 0.052390434 seconds\n",
      "elapsed time: 0.054195326 seconds\n",
      "elapsed time: 0.029935962 seconds\n",
      "elapsed time: 0.016278475 seconds\n",
      "elapsed time: 0.03928438 seconds\n",
      "elapsed time: 0.059043435 seconds\n",
      "niter = [0.0, 47.0, 6.0, 48.0, 26.0, 36.0, 16.0, 6.0, 21.0, 39.0]\n",
      "logl_mm = [-37361.9, -42179.5, -39985.3, -40519.9, -43489.0, -41191.6, -37703.0, -40304.1, -43131.3, -43709.9]\n"
     ]
    }
   ],
   "source": [
    "testdata = readcsv(\"optdigits.tra\", Int)\n",
    "X = testdata[:, 1:64]\n",
    "\n",
    "α_MLE_mm = zeros(64, 10)\n",
    "logl_mm = zeros(10)\n",
    "niter = zeros(10)\n",
    "time = zeros(10)\n",
    "\n",
    "for digit in 0:9\n",
    "    tic()\n",
    "    data = X[find(testdata[:, end] .== digit), :]\n",
    "    logl_mm[digit + 1], niter[digit + 1], α_MLE_mm[:, digit + 1] = \n",
    "    dirmult_mm(data')[1:3]\n",
    "    time[digit + 1] = toc()\n",
    "end\n",
    "\n",
    "@show niter;\n",
    "@show logl_mm;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64×10 Array{Float64,2}:\n",
       "  0.0        0.0         0.0         …  0.0         0.0         0.0       \n",
       "  0.035746   0.00807463  0.387487       0.140417    0.0889348   0.077002  \n",
       "  4.6969     0.518581    3.79758        2.51696     2.53803     1.54882   \n",
       " 14.0031     2.12319     5.21788        5.10037     5.93295     3.75345   \n",
       " 11.429      3.09084     2.49952        5.4966      5.67823     3.78586   \n",
       "  2.30456    1.33515     0.309763    …  4.50094     2.46441     1.53304   \n",
       "  0.0605893  0.138706    0.0157358      1.73779     0.202048    0.301609  \n",
       "  0.0        0.0         0.0            0.180032    0.0         0.016061  \n",
       "  0.0        0.0         0.0            0.0         0.00222093  0.0       \n",
       "  0.963464   0.0576451   1.72579        0.256385    1.03838     0.673872  \n",
       " 13.7205     1.05169     5.47169     …  3.2022      6.20934     4.07707   \n",
       " 13.8722     3.27794     4.72917        4.0231      4.64121     3.39517   \n",
       " 12.9177     4.1516      4.42079        4.20448     4.15643     3.33475   \n",
       "  ⋮                                  ⋱                                    \n",
       " 11.0161     4.09516     4.37446        1.57091     3.40438     2.25272   \n",
       " 14.3265     2.29961     3.8242         0.05343     4.1082      2.65534   \n",
       "  2.42999    0.275084    2.35657        0.00192423  0.789555    0.997247  \n",
       "  0.0        0.0305188   0.135773    …  0.0         0.0         0.0161003 \n",
       "  0.0        0.0         0.00193139     0.0         0.0         0.0       \n",
       "  0.0198231  0.0178489   0.353931       0.0997776   0.0788355   0.0539702 \n",
       "  4.72904    0.519159    3.86792        2.86315     2.61614     1.48428   \n",
       " 14.4576     2.04241     5.23807        4.15487     6.27559     3.818     \n",
       " 14.1727     3.56199     4.91266     …  0.454493    5.81142     3.60798   \n",
       "  5.46526    2.2333      4.49837        0.0118244   2.73852     1.63869   \n",
       "  0.184952   0.411946    3.00692        0.0         0.303773    0.338917  \n",
       "  0.0        0.0422012   0.333751       0.0         0.00667371  0.00707155"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "α_MLE_mm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7\n",
    "\n",
    "Finally let us re-consider the EM algorithm. The difficulty with the M step in EM algorithm can be remedied. Discuss how we can further minorize the $\\ln \\Gamma(|\\alpha|)$ term in the $Q$ function to produce a minorizing function with all $\\alpha_j$ separated. For this homework, you do **not** need to implement this EM-MM hybrid algorithm. Hint: $z \\mapsto \\ln \\Gamma(z)$ is a convex function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7 Solution:\n",
    "\n",
    "If we re-consider the EM algorithm, the difficulty with the M step in EM algorithm can be remedied by further minorizing the $\\ln \\Gamma(|\\alpha|)$ term in the $Q$ function to produce a minorizing function with all $\\alpha_j$ separated.\n",
    "\n",
    "Recall the $Q$ function from above:\n",
    "\n",
    "$$\n",
    "    Q(\\alpha|\\alpha^{(t)}) = \n",
    "\\sum_{j=1}^d \\sum_{i=1}^n \\alpha_j \\left[\\Psi(x_{ij}+\\alpha_j^{(t)}) - \\Psi(|\\mathbf{x}_i|+|\\alpha^{(t)}|)\\right] - n \\sum_{j=1}^d \\ln \\Gamma(\\alpha_j) + n \\ln \\Gamma(|\\alpha|) + c^{(t)},\n",
    "$$\n",
    "where $c^{(t)}$ is a constant irrelevant to optimization.\n",
    "<br>\n",
    "Using the hint given, $f_1(z) = \\ln \\Gamma(z)$ is a convex function. Thus, $f_2(z) = - \\ln \\Gamma(z)$ is a concave function. Now we can apply Jensen's Inequality on the function $f_2(z)$ to find a minorizing function. \n",
    "<br>\n",
    "<br>\n",
    "Let $f = -\\ln \\Gamma(|\\alpha|),$ which can be rewritten, with $|\\alpha|$ as the sum of $\\alpha_j + \\sum_{i = 1 | i\\ne j}^d \\alpha_i$ \n",
    "\n",
    "$f = \\ln \\Gamma(|\\alpha|) = \\ln \\Gamma (\\alpha_j + \\sum_{i = 1 | i\\ne j}^d \\alpha_i)$\n",
    "\n",
    "Now applying Jensen's Inequality on this concave term, we have:\n",
    "\n",
    "$$\n",
    "-\\ln \\Gamma (\\alpha_j + \\sum_{i = 1 | i\\ne j}^d \\alpha_i) \\ge \\Big(\\frac{\\alpha_j}{\\alpha_j + \\sum_{i = 1 | i\\ne j}^d \\alpha_i}\\Big)\\ln \\Gamma\\Big(\\frac{\\alpha_j + \\sum_{i = 1 | i\\ne j}^d \\alpha_i}{\\alpha_j} \\alpha_j\\Big) + \\Big(\\frac{\\sum_{i = 1 | i\\ne j}^d \\alpha_i}{\\alpha_j + \\sum_{i = 1 | i\\ne j}^d \\alpha_i}\\Big)\\ln \\Gamma\\Big(\\frac{\\alpha_j + \\sum_{i = 1 | i\\ne j}^d \\alpha_i}{\\sum_{i = 1 | i\\ne j}^d \\alpha_i} \\sum_{i = 1 | i\\ne j}^d \\alpha_i\\Big)\n",
    "$$\n",
    "<br>\n",
    "\n",
    "$$\n",
    "    Q(\\alpha|\\alpha^{(t)}) = \n",
    "\\sum_{j=1}^d \\sum_{i=1}^n \\alpha_j \\left[\\Psi(x_{ij}+\\alpha_j^{(t)}) - \\Psi(|\\mathbf{x}_i|+|\\alpha^{(t)}|)\\right] - n\\Big( \\sum_{j=1}^d \\ln \\Gamma(\\alpha_j) - \\ln \\Gamma(|\\alpha|)\\Big)+ c^{(t)},\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\ge \\sum_{j=1}^d \\sum_{i=1}^n \\alpha_j \\left[\\Psi(x_{ij}+\\alpha_j^{(t)}) - \\Psi(|\\mathbf{x}_i|+|\\alpha^{(t)}|)\\right] - n\\Big( \\sum_{j=1}^d \\ln \\Gamma(\\alpha_j) + \\Big[\\Big(\\frac{\\alpha_j}{\\alpha_j + \\sum_{i = 1 | i\\ne j}^d \\alpha_i}\\Big)\\ln \\Gamma\\Big(\\frac{\\alpha_j + \\sum_{i = 1 | i\\ne j}^d \\alpha_i}{\\alpha_j} \\alpha_j\\Big) + \\Big(\\frac{\\sum_{i = 1 | i\\ne j}^d \\alpha_i}{\\alpha_j + \\sum_{i = 1 | i\\ne j}^d \\alpha_i}\\Big)\\ln \\Gamma\\Big(\\frac{\\alpha_j + \\sum_{i = 1 | i\\ne j}^d \\alpha_i}{\\sum_{i = 1 | i\\ne j}^d \\alpha_i} \\sum_{i = 1 | i\\ne j}^d \\alpha_i\\Big)\\Big]\\Big)+ c^{(t)}\n",
    "$$\n",
    "\n",
    "\n",
    "Now all of the $\\alpha_j$ parameters are separated, and we can easily maximize with respect to each $\\alpha_j$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "65px",
    "width": "252px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
