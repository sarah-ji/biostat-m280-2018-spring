{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biostat M280 Homework 5\n",
    "\n",
    "Sarah Ji\n",
    "\n",
    "**Due June 15 @ 11:59PM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider again the MLE of the Dirichlet-multinomial model. In [HW4](http://hua-zhou.github.io/teaching/biostatm280-2018spring/hw/hw4/hw04.html), we worked out a Newton's method. In this homework, we explore the MM and EM approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1\n",
    "\n",
    "Show that, given iid observations $\\mathbf{x}_1,\\ldots,\\mathbf{x}_n$, the log-likelihood can be written as\n",
    "$$\n",
    "L(\\alpha) = \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i}\n",
    "+\\sum_{i=1}^n \\sum_{j=1}^d \\sum_{k=0}^{x_{ij}-1} \\ln(\\alpha_j+k) - \\sum_{i=1}^n \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\ln(|\\alpha|+k).\n",
    "$$\n",
    "Hint: $\\Gamma(a + k) / \\Gamma(a) = a (a + 1) \\cdots (a+k-1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 Solution:\n",
    "\n",
    "From HW 4, we have the following log-likelihood:\n",
    "$$\n",
    "L(\\alpha) = \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} + \\sum_{i=1}^n \\sum_{j=1}^d [\\ln \\Gamma(\\alpha_j + x_{ij}) - \\ln \\Gamma(\\alpha_j)] - \\sum_{i=1}^n [\\ln \\Gamma(|\\alpha|+|\\mathbf{x}_i|) - \\ln \\Gamma(|\\alpha|)].\n",
    "$$\n",
    "\n",
    ">Since the log of a quotient is the difference of the log of the numerator minus the denominator, we can rewrite the differences as fractions:\n",
    "$$\n",
    "L(\\alpha) = \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} + \\sum_{i=1}^n \\sum_{j=1}^d \\Big[\\ln \\cfrac{\\Gamma(\\alpha_j + x_{ij}) }{ \\Gamma(\\alpha_j)}\\Big] - \\sum_{i=1}^n \\Big[\\ln\\cfrac{ \\Gamma(|\\alpha|+|\\mathbf{x}_i|)}{\\Gamma(|\\alpha|)}\\Big]\n",
    "$$\n",
    "\n",
    ">Now using the hint, $\\Gamma(a + k) / \\Gamma(a) = a (a + 1) \\cdots (a+k-1)$, we can rewrite the gamma fractions in the likelihood as:\n",
    "$$\n",
    "= \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} + \\sum_{i=1}^n\\sum_{j=1}^d \\ln[\\alpha(\\alpha+1)\\cdots(\\alpha + x_{ij} - 1) ] - \\sum_{i=1}^n \\ln[|\\alpha|(|\\alpha|+1)\\cdots(|\\alpha| + |x_{ij}| - 1)]\n",
    "$$\n",
    "\n",
    ">Since the log of a product is the sum of the logs, now we can rewrite the terms:\n",
    "$$\n",
    "\\ln[\\alpha(\\alpha+1)\\cdots(\\alpha + x_{ij} - 1)] = \\sum_{k=0}^{x_{ij}-1} \\ln(\\alpha_j+k),\n",
    "$$ \n",
    "\n",
    "$$ \n",
    "ln[|\\alpha|(|\\alpha|+1)\\cdots(|\\alpha| + |x_{ij}| - 1)] = \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\ln(|\\alpha|+k)\n",
    "$$\n",
    "\n",
    "> And we get:\n",
    "$$\n",
    "= \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i}\n",
    "+\\sum_{i=1}^n \\sum_{j=1}^d \\sum_{k=0}^{x_{ij}-1} \\ln(\\alpha_j+k) - \\sum_{i=1}^n \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\ln(|\\alpha|+k) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2\n",
    "\n",
    "Suppose $(P_1,\\ldots,P_d) \\in \\Delta_d = \\{\\mathbf{p}: p_i \\ge 0, \\sum_i p_i = 1\\}$ follows a Dirichlet distribution with parameter $\\alpha = (\\alpha_1,\\ldots,\\alpha_d)$. Show that\n",
    "$$\n",
    "\t\\mathbf{E}(\\ln P_j) = \\Psi(\\alpha_j) - \\Psi(|\\alpha|),\n",
    "$$\n",
    "where $\\Psi(z) = \\Gamma'(z)/\\Gamma(z)$ is the digamma function and $|\\alpha| = \\sum_{j=1}^d \\alpha_j$. Hint: Differentiate the identity \n",
    "$$\n",
    "1 = \\int_{\\Delta_d} \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1} \\, d\\mathbf{p}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2 Solution:\n",
    "\n",
    "Using the hint, we will differentiate both sides of the identity to get:\n",
    "\n",
    "$$\n",
    "0 = \\frac{d}{d\\alpha_j}\\int_{\\Delta_d} \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1} \\, d\\mathbf{p}.\n",
    "$$\n",
    "\n",
    "> Note that while the derivative of the constant 1 in the left hand side is 0, we are taking the derivative of a product of two functions in the right hand side, thus we will proceed with the applying the product rule: (fg)' = fg' + gf'. Here I define the two functions f and g to be:\n",
    "\n",
    "$$\n",
    "f = \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "g = \\int_{\\Delta_d}\\prod_{j=1}^d p_j^{\\alpha_j-1} \\, d\\mathbf{p}\n",
    "$$\n",
    "\n",
    ">I take the first derivatives of both functions f and g to get the following f' and g' respectively:\n",
    "\n",
    "$$\n",
    "f' = \\frac{df}{d\\alpha_j} = \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\frac{-\\Gamma'(\\alpha_j)}{\\Gamma(\\alpha_j)} + \\frac{\\Gamma'(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\n",
    "$$\n",
    "<br>\n",
    "\n",
    ">Recall from LONG TIME AGO, $\\frac{d}{dx} a^x = ln(a) * a^x,$ applying that to take the derivative of g, we get:\n",
    "$$\n",
    "g' = \\frac{dg}{d\\alpha_j} = \\int_{\\Delta_d} \\Big[\\frac{d}{d\\alpha_j} \\prod_{j=1}^d p_j^{\\alpha_j-1} \\,\\Big] d\\mathbf{p} = \\int_{\\Delta_d} \\Big[ln(p_j) \\prod_{j=1}^d p_j^{\\alpha_j-1}\\Big] d\\mathbf{p}\n",
    "$$\n",
    "\n",
    ">Finally putting the Chain Rule in action, we have (fg)' = fg' + gf':\n",
    "$$\n",
    "(fg)' = \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\Big[\\int_{\\Delta_d} \\Big[ln(p_j) \\prod_{j=1}^d p_j^{\\alpha_j-1}\\Big] d\\mathbf{p}\\Big] + \\int_{\\Delta_d}\\prod_{j=1}^d p_j^{\\alpha_j-1} \\, d\\mathbf{p}\\Big[\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\frac{-\\Gamma'(\\alpha_j)}{\\Gamma(\\alpha_j)} + \\frac{\\Gamma'(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\Big]\n",
    "$$\n",
    "\n",
    "> A smart thing to do is to multiply the last term, $\\frac{\\Gamma'(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)},$ by $\\frac{\\Gamma(|\\alpha|)}{\\Gamma(|\\alpha|)} = 1,$ to get:\n",
    "\n",
    "$$\n",
    "(fg)' = \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\Big[\\int_{\\Delta_d} \\Big[ln(p_j) \\prod_{j=1}^d p_j^{\\alpha_j-1}\\Big] d\\mathbf{p}\\Big] + \\int_{\\Delta_d}\\prod_{j=1}^d p_j^{\\alpha_j-1} \\, \\Big[\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\frac{-\\Gamma'(\\alpha_j)}{\\Gamma(\\alpha_j)} + \\frac{\\Gamma'(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\frac{\\Gamma(|\\alpha|)}{\\Gamma(|\\alpha|)}\\Big]\n",
    "$$\n",
    "\n",
    ">Now we can use our cool awesome fact, $\\Psi(z) = \\Gamma'(z)/\\Gamma(z)$ is the digamma function to replace the terms $\\frac{\\Gamma'(\\alpha_j)}{\\Gamma(\\alpha_j)} = \\Psi(\\alpha_j)$ and $\\frac{\\Gamma'(|\\alpha|)}{\\Gamma(|\\alpha|)} = \\Psi(|\\alpha|)$\n",
    "<br>\n",
    "\n",
    ">Plugging those values in, we get:\n",
    "\n",
    "$$\n",
    "(fg)' = \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\Big[\\int_{\\Delta_d} \\Big[ln(p_j) \\prod_{j=1}^d p_j^{\\alpha_j-1}\\Big] d\\mathbf{p}\\Big] + \\int_{\\Delta_d}\\prod_{j=1}^d p_j^{\\alpha_j-1} \\, \\Big[\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d -\\Gamma(\\alpha_j)}\\Psi(\\alpha_j) + \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\frac{\\Gamma'(|\\alpha|)}{\\Gamma(|\\alpha|)}\\Big]d\\mathbf{p}\n",
    "$$\n",
    "\n",
    "$$\n",
    "(fg)' = \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\Big[\\int_{\\Delta_d} \\Big[ln(p_j) \\prod_{j=1}^d p_j^{\\alpha_j-1}\\Big] d\\mathbf{p}\\Big] + \\int_{\\Delta_d}\\prod_{j=1}^d p_j^{\\alpha_j-1} \\, d\\mathbf{p}\\Big[\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d -\\Gamma(\\alpha_j)}\\Psi(\\alpha_j) + \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\Psi(|\\alpha|)\\Big]d\\mathbf{p}\n",
    "$$\n",
    "\n",
    ">Now we can combine this into one integral and pull out the common term, $\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\prod_{j=1}^d p_j^{\\alpha_j-1}$\n",
    "\n",
    "$$\n",
    "(fg)' = \\int_{\\Delta_d}\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\prod_{j=1}^d p_j^{\\alpha_j-1}\\Big[ln(p_j) - \\Big(\\Psi(\\alpha_j) - \\Psi(|\\alpha|)\\Big)\\Big]d\\mathbf{p}\n",
    "$$\n",
    "\n",
    "$$\n",
    "(fg)' = \\int_{\\Delta_d}\\Big[ln(p_j)\\Big]\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\prod_{j=1}^d p_j^{\\alpha_j-1}d\\mathbf{p} - \\Big[\\Psi(\\alpha_j) - \\Psi(|\\alpha|)\\Big]\\int_{\\Delta_d}\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\prod_{j=1}^d p_j^{\\alpha_j-1}d\\mathbf{p}\n",
    "$$\n",
    "\n",
    ">Now recall the \"Law of the Unconscious Statistician\", $E[g(x)] = \\int_{\\Delta_x} g(x) f_x(x) dx$. \n",
    "<br>\n",
    "Applying this for on $\\mathbf{E}\\Big[\\ln P_j\\Big] = \\int_{\\Delta_x} \\ln (p_j) *  f_p(p) dp$ we can see:\n",
    "\n",
    "$$\n",
    "\\mathbf{E}\\Big[\\ln P_j\\Big] = \\int_{\\Delta_x} \\ln (p_j) *  f_p(p) dp = \\int_{\\Delta_x} \\ln (p_j) *\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\prod_{j=1}^d p_j^{\\alpha_j-1}\n",
    "$$\n",
    "\n",
    ">Thus substituing back in $\\mathbf{E}\\Big[\\ln P_j\\Big]$ we have:\n",
    "\n",
    "$$\n",
    "(fg)' = \\mathbf{E}\\Big[\\ln P_j\\Big] - \\Big[\\Psi(\\alpha_j) - \\Psi(|\\alpha|)\\Big]\\int_{\\Delta_d}\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\prod_{j=1}^d p_j^{\\alpha_j-1}d\\mathbf{p}\n",
    "$$\n",
    "\n",
    ">Recall one last awesome fact! We know that the pdf of p from previous homework:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\t\\pi(\\mathbf{p}) =  \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1} ,  \n",
    "\\end{eqnarray*} \n",
    "$$\n",
    "\n",
    "$$\n",
    "1 = \\int_{\\Delta_d}\\pi(\\mathbf{p})= \\int_{\\Delta_d} \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1} \\, d\\mathbf{p}\n",
    "$$\n",
    "\n",
    ">Thus using this trick we get the derivative of the right hand side to be:\n",
    "\n",
    "$$\n",
    "(fg)' = \\mathbf{E}\\Big[\\ln P_j\\Big] - \\Big[\\Psi(\\alpha_j) - \\Psi(|\\alpha|)\\Big]1\n",
    "$$\n",
    "\n",
    "<br>\n",
    ">Thus we have the following identity:\n",
    "$$\n",
    "0 = (fg)' = \\mathbf{E}\\Big[\\ln P_j\\Big] - \\Big[\\Psi(\\alpha_j) - \\Psi(|\\alpha|)\\Big]\n",
    "$$\n",
    "\n",
    ">Now moving $\\Big[\\Psi(\\alpha_j) - \\Psi(|\\alpha|)\\Big]$ over to the left we get:\n",
    "$$\n",
    "\\mathbf{E}\\Big[\\ln P_j\\Big] = \\Big[\\Psi(\\alpha_j) - \\Psi(|\\alpha|)\\Big]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3\n",
    "\n",
    "The admixture representation of the Dirichlet-multinomial distribution suggests that we can treat the unobserved multinomial parameters $\\mathbf{p}_1,\\ldots,\\mathbf{p}_n$ as missing data and derive an EM algorithm. Show that the Q function is\n",
    "$$\n",
    "    Q(\\alpha|\\alpha^{(t)}) = \n",
    "\\sum_{j=1}^d \\sum_{i=1}^n \\alpha_j \\left[\\Psi(x_{ij}+\\alpha_j^{(t)}) - \\Psi(|\\mathbf{x}_i|+|\\alpha^{(t)}|)\\right] - n \\sum_{j=1}^d \\ln \\Gamma(\\alpha_j) + n \\ln \\Gamma(|\\alpha|) + c^{(t)},\n",
    "$$\n",
    "where $c^{(t)}$ is a constant irrelevant to optimization. Comment on why it is not easy to maximize the Q function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3 Solution:\n",
    "\n",
    "Let $\\mathbf{X} = \\mathbf{x}_1,\\ldots,\\mathbf{x}_n$ be the Observed data and $\\mathbf{p} = (p_1, ... , p_n)$ be the missing data. We have that X and P follow the following distributions:\n",
    "\n",
    "$$\\mathbf{X} \\sim Multinomial( |\\mathbf{x}|, \\mathbf{p})$$ $$\\pi(\\mathbf{p}) \\sim Dirichlet(\\mathbf{\\alpha})$$\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\t\\pi(\\mathbf{p}) =  \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1},\n",
    "\\end{eqnarray*} \n",
    "$$\n",
    "where $|\\alpha|=\\sum_{j=1}^d \\alpha_j$.\n",
    "\n",
    "To use iterations, we have the $t^{th}$ update of $\\alpha = \\alpha^{(t)}$ to get: \n",
    "\n",
    "$$\\mathbf{X} \\sim Multinomial( |\\mathbf{x}|, \\mathbf{p})$$ $$\\pi(\\mathbf{p}) \\sim Dirichlet(\\mathbf{\\alpha^{(t)}})$$\n",
    "<br>\n",
    "\n",
    "Using Bayesian conjugate prior properties, we know that the conditional distribution of $\\mathbf{p}$ given $\\mathbf{X}$ is also Dirichlet:\n",
    "\n",
    "$$\n",
    "\\mathbf{p|X = x} \\sim Dirichlet(\\alpha^{(t)} + \\mathbf{x})\n",
    "$$\n",
    "\n",
    ">That is, $(P_1,\\ldots,P_d \\mid x_1, \\ldots,x_d)$ follows a Dirichlet distribution with parameter $\\alpha = (\\alpha_1^{(t)} + x_1,\\ldots,\\alpha_d^{(t)}+ x_d),$\n",
    "\n",
    ">First we get the complete density, by getting the joint pdf of $\\mathbf{X}$ and $\\mathbf{p}$ to get $f(\\mathbf{x, p} \\mid \\mathbf{\\alpha})$ for one observation as below:\n",
    "\n",
    "$$\n",
    "    f(\\mathbf{x, p} \\mid \\alpha) \n",
    "\t= \\binom{|\\mathbf{x}|}{\\mathbf{x}} \\prod_{j=1}^d p_j^{x_j} \\pi(\\mathbf{p}) \\, \n",
    "$$\n",
    "\n",
    ">Now plugging in $\\pi(\\mathbf{p})$ to the equation above we get the complete density to be:\n",
    "\n",
    "$$\n",
    "    f(\\mathbf{x, p} \\mid \\alpha) \n",
    "\t= \\binom{|\\mathbf{x}|}{\\mathbf{x}} \\prod_{j=1}^d p_j^{x_j} \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1} \\, \n",
    "$$\n",
    "\n",
    ">Then we can take the log to get the complete log pdf to be:\n",
    "\n",
    "$$\n",
    "lnf(\\mathbf{x, p} \\mid \\alpha) = \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} + \\sum_{j=1}^d x_j ln(p_j) + ln\\Big(\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\Big) + \\sum_{j=1}^d (\\alpha_j - 1) ln(p_j)\n",
    "$$\n",
    "\n",
    ">Now we can combine the two terms with the same summation over j, and pull out the $ln(p_j)$ term to get:\n",
    "\n",
    "$$\n",
    "lnf(\\mathbf{x, p} \\mid \\alpha) = \\sum_{j=1}^d ln(p_j)[x_j + (\\alpha_j - 1)] + ln\\Big(\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\Big) +\\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i}\n",
    "$$\n",
    "\n",
    ">Now let the last term, $\\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i}$ be represented by constant $c$ to get:\n",
    "\n",
    "$$\n",
    "lnf(\\mathbf{x, p} \\mid \\alpha) = \\sum_{j=1}^d ln(p_j)[x_j + (\\alpha_j - 1)] + ln\\Big(\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\Big) + c\n",
    "$$\n",
    "\n",
    ">Recall that the EM algorithm finds the conditional expectation as the surrogate function for each pdf, $Q_j(\\alpha^{(t)} \\mid \\alpha)$. Thus we get:\n",
    "\n",
    "$$ \n",
    "Q_j(\\alpha^{(t)} \\mid \\alpha) = \\mathbf{E}_{p | X = \\mathbf{x}, \\alpha^{(t)}}\\Big[lnf(\\mathbf{x, p} \\mid \\alpha)   \\Big|   X = \\mathbf{x}, \\alpha = \\alpha^{(t)}\\Big]\n",
    "$$\n",
    "\n",
    ">Plugging in the log pdf, $lnf(\\mathbf{x, p} \\mid \\alpha)$, from above we get:\n",
    "\n",
    "$$\n",
    "Q_j(\\alpha^{(t)} \\mid \\alpha) = \\mathbf{E}_{p | X = x, \\alpha^{(t)}}\\Big[\\sum_{j=1}^d ln(p_j)[x_j + (\\alpha_j - 1)] + ln\\Big(\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\Big) + c   \\Big|   X = \\mathbf{x}, \\alpha = \\alpha^{(t)}\\Big]\n",
    "$$\n",
    "\n",
    ">Thus, if we take the expecation with respect to $p$, we get the surrogate function for the $j^{th}$ person: \n",
    "\n",
    "$$\n",
    "Q_j(\\alpha^{(t)} \\mid \\alpha) = \\sum_{j=1}^d \\mathbf{E}\\Big[ln(p_j)| X = \\mathbf{x}, \\alpha = \\alpha^{(t)}\\Big][x_j + (\\alpha_j - 1)] + ln\\Big(\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\Big) + c \n",
    "$$\n",
    "\n",
    ">Now making note of what's given in Question 2, if $(P_1,\\ldots,P_d) \\in \\Delta_d = \\{\\mathbf{p}: p_i \\ge 0, \\sum_i p_i = 1\\}$ follows a Dirichlet distribution with parameter $\\alpha = (\\alpha_1,\\ldots,\\alpha_d)$\n",
    "\n",
    "$$\n",
    "\t\\mathbf{E}(\\ln p_j) = \\Psi(\\alpha_j) - \\Psi(|\\alpha|),\n",
    "$$\n",
    "\n",
    ">So we know that for us, since $(P_1,\\ldots,P_d \\mid x_1, \\ldots,x_d)$ follows a Dirichlet distribution with parameter $\\alpha = (\\alpha_1^{(t)} + x_1,\\ldots,\\alpha_d^{(t)}+ x_d),$ so the expectation we have is:\n",
    "\n",
    "$$\n",
    "\t\\mathbf{E}(\\ln p_j \\mid x_j, \\alpha^{(t)}) = \\Psi(\\alpha_j^{(t)} + x_j) - \\Psi(|\\alpha^{(t)}| + |\\mathbf{x}|),\n",
    "$$\n",
    "\n",
    "\n",
    ">We can replace the $\\mathbf{E}(\\ln P_j)$ to get:\n",
    "\n",
    "$$\n",
    "Q_j(\\alpha^{(t)} \\mid \\alpha) = \\sum_{j=1}^d \\Big[\\Psi(\\alpha_j^{(t)} + x_j) - \\Psi(|\\alpha^{(t)}| + |\\mathbf{x}|)\\Big][x_j + (\\alpha_j - 1)] + ln\\Big(\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\Big) + c \n",
    "$$\n",
    "\n",
    ">Distributing out the $[x_j + (\\alpha_j - 1)]$ term above, we get:\n",
    "\n",
    "\n",
    "$$\n",
    "Q_j(\\alpha^{(t)} \\mid \\alpha) = \\sum_{j=1}^d \\alpha_j\\Big[\\Psi(\\alpha_j^{(t)} + x_j) - \\Psi(|\\alpha^{(t)}| + |\\mathbf{x}|)\\Big] + x_j\\Big[\\Psi(\\alpha_j^{(t)} + x_j) - \\Psi(|\\alpha^{(t)}| + |\\mathbf{x}|)\\Big] - 1\\Big[\\Psi(\\alpha_j^{(t)} + x_j) - \\Psi(|\\alpha^{(t)}| + |\\mathbf{x}|)\\Big] + ln\\Big(\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\Big) + c \n",
    "$$\n",
    "\n",
    ">But recall that $x_j, \\alpha_j^{(t)} , |\\alpha^{(t)}|$ are simply constants, so we can combine the second and third term into the constant term $c$, which then becomes $c*$ as below:\n",
    "\n",
    "$$\n",
    "Q_j(\\alpha^{(t)} \\mid \\alpha) = \\sum_{j=1}^d \\alpha_j\\Big[\\Psi(\\alpha_j^{(t)} + x_j) - \\Psi(|\\alpha^{(t)}| + |\\mathbf{x}|)\\Big]  + ln\\Big(\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\Big) + c* \n",
    "$$\n",
    "\n",
    "\n",
    ">Now expanding the second term as the difference of the logs we get:\n",
    "\n",
    "$$\n",
    "Q_j(\\alpha^{(t)} \\mid \\alpha) = \\sum_{j=1}^d \\alpha_j\\Big[\\Psi(\\alpha_j^{(t)} + x_{j}) - \\Psi(|\\alpha^{(t)}| + |\\mathbf{x}|)\\Big]  + ln(\\Gamma(|\\alpha|) - ln(\\prod_{j=1}^d \\Gamma(\\alpha_j)) + c*\n",
    "$$\n",
    "\n",
    ">Further expanding out the log of a product as the sum of the logs we get:\n",
    "$$\n",
    "Q_j(\\alpha^{(t)} \\mid \\alpha) = \\sum_{j=1}^d \\alpha_j\\Big[\\Psi(\\alpha_j^{(t)} + x_{j}) - \\Psi(|\\alpha^{(t)}| + |\\mathbf{x}|)\\Big]  + ln(\\Gamma(|\\alpha|) - \\sum_{j=1}^d ln(\\Gamma(\\alpha_j)) + c*\n",
    "$$\n",
    "\n",
    ">Switching the last two terms we get:\n",
    "\n",
    "$$\n",
    "Q_j(\\alpha^{(t)} \\mid \\alpha) = \\sum_{j=1}^d \\alpha_j\\Big[\\Psi(\\alpha_j^{(t)} + x_{j}) - \\Psi(|\\alpha^{(t)}| + |\\mathbf{x}|)\\Big] - \\Big[\\sum_{j=1}^d ln(\\Gamma(\\alpha_j)) - ln(\\Gamma(|\\alpha|)\\Big] + c*\n",
    "$$\n",
    "\n",
    ">Now recall that each logpdf is an independent Dirichlet($x_i + \\alpha^{(t)}$), thus the overall complete likelihood is just a product of each independent pdf, and consequently, the overall loglikelihood is just the sum of the logpdfs. Thus, we have the overall surrogate function for the n independent observations to be: \n",
    "\n",
    "$$\n",
    "Q(\\alpha|\\alpha^{(t)}) = \\sum^n_{i=1}Q_i(\\alpha|\\alpha^{(t)})\n",
    "$$\n",
    "\n",
    ">Thus for the whole sample of $n$ observations we have: \n",
    "\n",
    "$$\n",
    "Q(\\alpha|\\alpha^{(t)}) = \\sum^n_{i=1}\\sum_{j=1}^d \\alpha_j\\Big[\\Psi(\\alpha_j^{(t)} + x_{ij}) - \\Psi(|\\alpha^{(t)}| + |\\mathbf{x_i}|)\\Big]  - \\sum^n_{i=1}\\Big[\\sum_{j=1}^d ln(\\Gamma(\\alpha_j))  - ln(\\Gamma(|\\alpha|)\\Big] + c**\n",
    "$$\n",
    "\n",
    ">Note that there is no $i$ index, except for the first term in the hard brackets and the constant term collects over n observations to become $c**$, a constant irrelevant to optimization so we get:\n",
    "\n",
    "$$\n",
    "Q(\\alpha|\\alpha^{(t)}) = \\sum^n_{i=1}\\sum_{j=1}^d \\alpha_j\\Big[\\Psi(\\alpha_j^{(t)} + x_{ij}) - \\Psi(|\\alpha^{(t)}| + |\\mathbf{x_i}|)\\Big]  - n\\sum_{j=1}^d ln(\\Gamma(\\alpha_j))  + nln(\\Gamma(|\\alpha|) + c**\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4\n",
    "\n",
    "We derive an MM algorithm for maximing $L$. Consider the formulation of the log-likelihood that contains terms $\\ln (\\alpha_j + k)$ and $- \\ln (|\\alpha|+k)$. Applying Jensen's inequality to the concave term $\\ln (\\alpha_j + k)$ and supporting hyperplane inequality to the convex term $- \\ln (|\\alpha|+k)$, show that a minorizing function to $L(\\alpha)$ is\n",
    "$$\n",
    "\tg(\\alpha|\\alpha^{(t)}) = - \\sum_{k=0}^{\\max_i |\\mathbf{x}_i|-1} \\frac{r_k}{|\\alpha^{(t)}|+k} |\\alpha| + \\sum_{j=1}^d \\sum_{k=0}^{\\max_i x_{ij}-1} \\frac{s_{jk} \\alpha_j^{(t)}}{\\alpha_j^{(t)}+k} \\ln \\alpha_j + c^{(t)},\n",
    "$$\n",
    "where $s_{jk} = \\sum_{i=1}^n 1_{\\{x_{ij} > k\\}}$, $r_k = \\sum_{i=1}^n 1_{\\{|\\mathbf{x}_i| > k\\}}$, and  $c^{(t)}$ is a constant irrelevant to optimization. Maximizing the surrogate function $g(\\alpha|\\alpha^{(t)})$ is trivial since $\\alpha_j$ are separated. Show that the MM updates are\n",
    "$$\n",
    "\t\\alpha_j^{(t+1)} = \\frac{\\sum_{k=0}^{\\max_i x_{ij}-1} \\frac{s_{jk}}{\\alpha_j^{(t)}+k}}{\\sum_{k=0}^{\\max_i |\\mathbf{x}_i|-1} \\frac{r_k}{|\\alpha^{(t)}|+k}} \\alpha_j^{(t)}, \\quad j=1,\\ldots,d.\n",
    "$$\n",
    "The quantities $s_{jk}$, $r_k$, $\\max_i x_{ij}$ and $\\max_i |\\mathbf{x}_i|$ only depend on data and can be pre-computed. Comment on whether the MM updates respect the parameter constraint $\\alpha_j>0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4 Solution:\n",
    "\n",
    "Our goal is to derive an MM algorithm for maximing the log likelihood, $L$. Recall that in question 1 above, we showed that the log-likelihood is of the form:\n",
    "\n",
    "$$\n",
    "L(\\alpha) = \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i}\n",
    "+\\sum_{i=1}^n \\sum_{j=1}^d \\sum_{k=0}^{x_{ij}-1} \\ln(\\alpha_j+k) - \\sum_{i=1}^n \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\ln(|\\alpha|+k).\n",
    "$$\n",
    "\n",
    "Since the formulation of the log-likelihood contains terms $\\ln (\\alpha_j + k)$ and $- \\ln (|\\alpha|+k)$, we first apply Jensen's inequality to the concave term $\\ln (\\alpha_j + k)$ and then apply the supporting hyperplane inequality to the convex term $- \\ln (|\\alpha|+k)$. \n",
    "\n",
    "\n",
    "## JENSEN'S INEQUALITY: \n",
    "\n",
    ">1) Applying Jensen's inequality to the concave term $\\ln (\\alpha_j + k)$\n",
    "\n",
    "From Ken Lange's MM Algorithms book, we have that if a function, $f$, is concave, by applying Jensen's Inequality to the function we get: \n",
    "\n",
    "$$\n",
    "f(u + v) \\ge \\Big(\\frac{u_n}{u_n + v_n}\\Big)f(\\frac{u_n + v_n}{u_n} u) + \\Big(\\frac{v_n}{u_n + v_n}\\Big)f(\\frac{u_n + v_n}{v_n} v)\n",
    "$$\n",
    "<br>\n",
    "\n",
    "Let $f = \\ln (\\alpha_j + k)$ then applying Jensen's Inequality on this concave term, we have:\n",
    "\n",
    "$$\n",
    "\\ln (\\alpha_j + k) \\ge \\Big(\\frac{\\alpha_j^{(t)}}{\\alpha_j^{(t)} + k}\\Big)\\ln\\Big(\\frac{\\alpha_j^{(t)} + k}{\\alpha_j^{(t)}} \\alpha_j\\Big) + \\Big(\\frac{k}{\\alpha_j^{(t)} + k}\\Big)\\ln\\Big(\\frac{\\alpha_j^{(t)} + k}{k} k\\Big)\n",
    "$$\n",
    "<br>\n",
    "Now, if we rewrite the log of the product as the sum of the logs, we get:\n",
    "\n",
    "$$\n",
    "\\ln (\\alpha_j + k) \\ge \\Big(\\frac{\\alpha_j^{(t)}}{\\alpha_j^{(t)} + k}\\Big) \\Big[\\ln\\Big(\\frac{\\alpha_j^{(t)} + k}{\\alpha_j^{(t)}}\\Big) +\\ln\\alpha_j\\Big] + \\Big(\\frac{k}{\\alpha_j^{(t)} + k}\\Big)\\ln\\Big(\\frac{\\alpha_j^{(t)} + k}{k} k\\Big)\n",
    "$$\n",
    "<br>\n",
    "\n",
    "Recall that $x_j, \\alpha_j^{(t)} , k $ are simply constants, so the last term, $\\ln\\Big(\\frac{\\alpha_j^{(t)} + k}{k} k\\Big)$, is also just a constant so we will leave it as is! \n",
    "<br>\n",
    "\n",
    "Distributing out the $\\Big(\\frac{\\alpha_j^{(t)}}{\\alpha_j^{(t)} + k}\\Big)$ term, we have:\n",
    "\n",
    "$$\n",
    "\\ln (\\alpha_j + k) \\ge \\Big(\\frac{\\alpha_j^{(t)}}{\\alpha_j^{(t)} + k}\\Big) \\ln\\Big(\\frac{\\alpha_j^{(t)} + k}{\\alpha_j^{(t)}}\\Big) +\\Big(\\frac{\\alpha_j^{(t)}}{\\alpha_j^{(t)} + k}\\Big) \\ln\\alpha_j + \\Big(\\frac{k}{\\alpha_j^{(t)} + k}\\Big)\\ln\\Big(\\frac{\\alpha_j^{(t)} + k}{k} k\\Big)\n",
    "$$\n",
    "<br>\n",
    "\n",
    "Notice that we can combine the constant terms together into $c_1$ to get: \n",
    "\n",
    "$$\n",
    "\\ln (\\alpha_j + k) \\ge \\Big(\\frac{\\alpha_j^{(t)}}{\\alpha_j^{(t)} + k}\\Big) \\ln\\alpha_j + c_1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HYPERPLANE INEQUALITY: \n",
    "\n",
    ">2) Applying the supporting hyperplane inequality to the convex term $- \\ln (|\\alpha|+k)$\n",
    "\n",
    "From the Hyperplane Inequality found in Chapter 1 of Ken Lange's MM Algorithms book, we have that:\n",
    "$$\n",
    "- \\ln (|\\alpha| + k) \\ge - \\ln (|\\alpha^{(t)}| + k) - \\frac{|\\alpha| - |\\alpha^{(t)}|}{|\\alpha^{(t)}| + k}\n",
    "$$\n",
    "<br>\n",
    "\n",
    "Since each $|\\alpha^{(t)}|$ is a positive sum, we know that this inequality is less than the following inequality:\n",
    "\n",
    "$$\n",
    "- \\ln (|\\alpha| + k) \\ge - \\ln (|\\alpha^{(t)}| + k) - \\frac{|\\alpha| - |\\alpha^{(t)}|}{|\\alpha^{(t)}| + k}\n",
    "$$\n",
    "$$\n",
    "\\ge -\\ln (|\\alpha^{(t)}| + k) - \\frac{|\\alpha|}{|\\alpha^{(t)}| + k}\n",
    "$$\n",
    "<br>\n",
    "\n",
    "Recall that $\\alpha_j^{(t)} , |\\alpha^{(t)}|, k $ are simply constants, so the first term, $\\ln (|\\alpha^{(t)}| + k)$, is just a constant so we will replace it with $c = \\ln (|\\alpha^{(t)}| + k)$ to get:\n",
    "<br>\n",
    "\n",
    "$$\n",
    "- \\ln (|\\alpha| + k) \\ge c_2 - \\frac{|\\alpha|}{|\\alpha^{(t)}| + k}\n",
    "$$\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minorizing Function for Loglikelihood\n",
    "\n",
    "To find a minorizing function to $L(\\alpha)$, we can combine the two terms from above. \n",
    "\n",
    "Recall that the Loglikelihood has the form:\n",
    "\n",
    "$$\n",
    "L(\\alpha) = \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i}\n",
    "+\\sum_{i=1}^n \\sum_{j=1}^d \\sum_{k=0}^{x_{ij}-1} \\ln(\\alpha_j+k) - \\sum_{i=1}^n \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\ln(|\\alpha|+k).\n",
    "$$\n",
    "<br>\n",
    ">Now substituting the terms $\\ln(\\alpha_j+k)$ and $-\\ln(|\\alpha|+k)$ we have the following inequality:\n",
    "\n",
    "$$\n",
    "L(\\alpha) \\ge \\sum_{i=1}^n \\sum_{j=1}^d \\sum_{k=0}^{x_{ij}-1} \\Big(\\frac{\\alpha_j^{(t)}}{\\alpha_j^{(t)} + k}\\Big) \\ln\\alpha_j + c_1 + \\sum_{i=1}^n \\sum_{k=0}^{|\\mathbf{x}_i|-1} c_2 - \\frac{|\\alpha|}{|\\alpha^{(t)}| + k}\n",
    "$$\n",
    "<br>\n",
    "\n",
    ">Now Reordering the second term and the first term, then combining the constant terms $c_1$ and $c_2$ together into $c^{(t)}$ we have: \n",
    "\n",
    "$$\n",
    "= - \\sum_{i=1}^n \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\frac{|\\alpha|}{|\\alpha^{(t)}| + k} + \\sum_{i=1}^n \\sum_{j=1}^d \\sum_{k=0}^{x_{ij}-1} \\Big(\\frac{\\alpha_j^{(t)}}{\\alpha_j^{(t)} + k}\\Big) \\ln\\alpha_j + c^{(t)}\n",
    "$$\n",
    "\n",
    "> Defining the terms $s_{jk} = \\sum_{i=1}^n 1_{\\{x_{ij} > k\\}}$, $r_k = \\sum_{i=1}^n 1_{\\{|\\mathbf{x}_i| > k\\}}$, we can rewrite this as:\n",
    "\n",
    "$$\n",
    "= - \\sum_{k=0}^{\\max_i |\\mathbf{x}_i|-1} \\frac{|\\alpha|r_k}{|\\alpha^{(t)}| + k} + \\sum_{j=1}^d \\sum_{k=0}^{\\max_i x_{ij}-1} \\Big(\\frac{\\alpha_j^{(t)}s_{jk}}{\\alpha_j^{(t)} + k}\\Big) \\ln\\alpha_j + c^{(t)}\n",
    "$$\n",
    "\n",
    ">Therefore we have have found a minorizing function to $L(\\alpha)$, that is equal to $g(\\alpha|\\alpha^{(t)})$\n",
    "\n",
    "$$\n",
    "\tg(\\alpha|\\alpha^{(t)}) = - \\sum_{k=0}^{\\max_i |\\mathbf{x}_i|-1} \\frac{r_k}{|\\alpha^{(t)}|+k} |\\alpha| + \\sum_{j=1}^d \\sum_{k=0}^{\\max_i x_{ij}-1} \\frac{s_{jk} \\alpha_j^{(t)}}{\\alpha_j^{(t)}+k} \\ln \\alpha_j + c^{(t)},\n",
    "$$\n",
    "\n",
    "where $s_{jk} = \\sum_{i=1}^n 1_{\\{x_{ij} > k\\}}$, $r_k = \\sum_{i=1}^n 1_{\\{|\\mathbf{x}_i| > k\\}}$, and  $c^{(t)}$ is a constant irrelevant to optimization. \n",
    "<br>\n",
    "Maximizing the surrogate function $g(\\alpha|\\alpha^{(t)})$ is trivial since $\\alpha_j$ are separated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deriving the MM update\n",
    "\n",
    "To derive the MM update for $\\alpha,$ we differentiate the minorization function with respect to $\\alpha_j$, set the derivative to zero, and solve for $\\alpha_j$.\n",
    "\n",
    "$$\n",
    "\\frac{d}{d\\alpha_j} g(\\alpha|\\alpha^{(t)}) = \\frac{d}{d\\alpha_j} \\Big[ - \\sum_{k=0}^{\\max_i |\\mathbf{x}_i|-1} \\frac{r_k}{|\\alpha^{(t)}|+k} |\\alpha| + \\sum_{j=1}^d \\sum_{k=0}^{\\max_i x_{ij}-1} \\frac{s_{jk} \\alpha_j^{(t)}}{\\alpha_j^{(t)}+k} \\ln \\alpha_j + c^{(t)}\\Big] = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\t\\alpha_j^{(t+1)} = \\frac{\\sum_{k=0}^{\\max_i x_{ij}-1} \\frac{s_{jk}}{\\alpha_j^{(t)}+k}}{\\sum_{k=0}^{\\max_i |\\mathbf{x}_i|-1} \\frac{r_k}{|\\alpha^{(t)}|+k}} \\alpha_j^{(t)}, \\quad j=1,\\ldots,d.\n",
    "$$\n",
    "The quantities $s_{jk}$, $r_k$, $\\max_i x_{ij}$ and $\\max_i |\\mathbf{x}_i|$ only depend on data and can be pre-computed. \n",
    "<br>\n",
    "<br>\n",
    "Comment on whether the MM updates respect the parameter constraint $\\alpha_j>0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5\n",
    "\n",
    "Write a function for finding MLE of Dirichlet-multinomial distribution given iid observations $\\mathbf{x}_1,\\ldots,\\mathbf{x}_n$, using MM algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5 Solution:\n",
    "\n",
    "First we use the `log_pdf(X)` functions and the `dirmult_mom(X)` function from the previous homework as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_logpdf"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using SpecialFunctions\n",
    "\"\"\"\n",
    "    dirmult_logpdf(x::Vector, α::Vector)\n",
    "    \n",
    "Compute the log-pdf of Dirichlet-multinomial distribution with parameter `α` \n",
    "at data point `x`.\n",
    "\"\"\"\n",
    "function dirmult_logpdf(x::Vector, α::Vector)\n",
    "    term1 = 0.0\n",
    "    Logpdf = 0.0\n",
    "    sumalpha = sum(α)\n",
    "    sumx = sum(x)\n",
    "    for i in 1:length(x)\n",
    "        if α[i] == 0 && x[i] == 0 # we have to hard code these values\n",
    "            Logpdf += 0\n",
    "        elseif α[i] == 0 && x[i] > 0\n",
    "            Logpdf += - Inf # we have to hard code these values\n",
    "        else\n",
    "        term1 += lgamma(α[i] + x[i]) - lgamma(α[i]) - lfact(x[i])\n",
    "        Logpdf = term1 + lfact(sumx) + lgamma(sumalpha) - lgamma(sumx + sumalpha)\n",
    "    end\n",
    "    end\n",
    "    return Logpdf\n",
    "end\n",
    "\n",
    "function dirmult_logpdf!(r::Vector, X::Matrix, α::Vector)\n",
    "    for j in 1:size(X, 2)\n",
    "        r[j] = dirmult_logpdf(X[:, j], α)\n",
    "    end\n",
    "    return r\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    dirmult_logpdf(X, α)\n",
    "    \n",
    "Compute the log-pdf of Dirichlet-multinomial distribution with parameter `α` \n",
    "at each data point in `X`. Each column of `X` is one data point.\n",
    "\"\"\"\n",
    "function dirmult_logpdf(X::Matrix, α::Vector)\n",
    "    r = zeros(size(X, 2))\n",
    "    dirmult_logpdf!(r, X, α)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_mom (generic function with 1 method)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## code for finding initial alpha\n",
    "function dirmult_mom(X::Matrix{Int})\n",
    "    n, d = size(X)\n",
    "    alpha_0 = zeros(d)#creates 64x1 matrix\n",
    "    tolerance = 0.001\n",
    "    Z_hat = 0.0\n",
    "    Z_numerator = 0.0\n",
    "    sum_Xcol = sum(X, 1)\n",
    "    sum_Xrow = sum(X, 2)\n",
    "    \n",
    "    for j in 1:d\n",
    "        if sum_Xcol[j] < tolerance #check to see if there is data in the column\n",
    "            continue\n",
    "        end\n",
    "        \n",
    "        Z_numerator = 0 #reset each value\n",
    "        for i in 1:n #for each column\n",
    "            Z_numerator += (X[i, j] / sum_Xrow[i])^2\n",
    "            alpha_0[j] += (X[i, j] / sum_Xrow[i])\n",
    "        end\n",
    "        \n",
    "        Z_hat += (Z_numerator / alpha_0[j]) #calculating Zhat\n",
    "        alpha_0[j] /= n #divide by n in front of the sigma\n",
    "\n",
    "    end\n",
    "    alpha_0 *= ((d - Z_hat) / (Z_hat - 1))\n",
    "    return alpha_0\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64-element Array{Float64,1}:\n",
       " 0.0        \n",
       " 0.0714198  \n",
       " 1.30543    \n",
       " 2.83281    \n",
       " 2.75176    \n",
       " 1.32356    \n",
       " 0.336655   \n",
       " 0.0343316  \n",
       " 0.000461199\n",
       " 0.463798   \n",
       " 2.52787    \n",
       " 2.8072     \n",
       " 2.53299    \n",
       " ⋮          \n",
       " 2.33097    \n",
       " 2.20214    \n",
       " 0.894695   \n",
       " 0.0353529  \n",
       " 5.71236e-5 \n",
       " 0.066766   \n",
       " 1.39754    \n",
       " 2.8661     \n",
       " 2.744      \n",
       " 1.59808    \n",
       " 0.506003   \n",
       " 0.0475158  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sarah = dirmult_mom(X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64×3823 Array{Int64,2}:\n",
       "  0   0   0   0   0   0   0   0   0  …   0   0   0   0   0   0   0   0   0\n",
       "  1   0   0   0   0   0   0   0   0      0   0   1   0   0   0   0   0   0\n",
       "  6  10   8   0   5  11   1   8  15      9   9  10   6   5   0   3   6   2\n",
       " 15  16  15   3  14  16  11  10   2     16  16  16  16  13   1  15  16  15\n",
       " 12   6  16  11   4  10  13   8  14      6  12  16  11  11  12   0   2  16\n",
       "  1   0  13  16   0   1  11   7  13  …   0   1   4   0   2   1   0   0  13\n",
       "  0   0   0   0   0   0   7   2   2      0   0   0   0   0   0   0   0   1\n",
       "  0   0   0   0   0   0   0   0   0      0   0   0   0   0   0   0   0   0\n",
       "  0   0   0   0   0   0   0   0   0      0   0   0   0   0   0   0   0   0\n",
       "  7   7   1   0   0   4   0   1   0      2   3   8   1   2   0   0   0   0\n",
       " 16  16  11   5  13  16   9  15  16  …  14  16  13  16  15   0  11  15   3\n",
       "  6   8   9  16   8  10  14  14  15     16  10  12   2   6  14  14  10   7\n",
       "  6  16  11  11   0  15   6  12  12     16  15  16  12   5  10   0   0  10\n",
       "  ⋮                   ⋮              ⋱       ⋮                   ⋮        \n",
       " 15  10   4  16   4  15  15  14  16      8   4  10   0   1  15   9   2   7\n",
       "  9  15   0   0  11   8   3   5   6      0   9  16   8   8   1  15   8   0\n",
       "  0   3   0   0  12   8   0   0   0      0  16   5   4  10   0   4  15   0\n",
       "  0   0   0   0   0   3   0   0   0  …   0   0   0   0   0   0   0   0   0\n",
       "  0   0   0   0   0   0   0   0   0      0   0   0   0   0   0   0   0   0\n",
       "  0   0   0   0   0   0   0   0   0      0   0   2   1   0   0   0   0   0\n",
       "  6  10   9   0   4  10   1   4  10     10   8  13   7   8   0   4   5   4\n",
       " 14  16  14   1  12  16  13  13  12     13  16  16  14  13   4  14  16  14\n",
       "  7  15   0  15  14  16   5   8   5  …   1  16  12  16  15   9  16  16   1\n",
       "  1   3   0   2   7  16   0   0   0      0  16   5  12  10   0   9  16   0\n",
       "  0   0   0   0   0  16   0   0   0      0   8   0   1   1   0   0   5   0\n",
       "  0   0   0   0   0   6   0   0   0      0   0   0   0   0   0   0   0   0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindata = readcsv(\"optdigits.tra\", Int)\n",
    "X = traindata[:, 1:64]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_mm (generic function with 1 method)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function dirmult_mm(\n",
    "        X::AbstractMatrix; \n",
    "        α0::Vector = seee(X'), \n",
    "        maxiters::Int = 100, \n",
    "        tolfun = 1e-6,\n",
    "    )\n",
    "    \n",
    "#calculate max_i(x_ijk)  and max_i(|x_i|)\n",
    "    d, n = size(X)\n",
    "    #numerator \n",
    "    max_xij = maximum(maximum(X, 2) - 1)\n",
    "    #denominator\n",
    "    rowsum_x = sum(X, 1)\n",
    "    max_sum_xi = maximum(rowsum_x) - 1\n",
    "\n",
    "#calculate s_jk , r_k\n",
    "    s_jk = zeros(d, max_xij + 1)\n",
    "    r_k = zeros(max_sum_xi + 1)\n",
    "    \n",
    "    for i in 0:max_sum_xi\n",
    "        r_k[i + 1] += countnz(rowsum_x .> i)\n",
    "    end\n",
    "    \n",
    "    for k in 0:max_xij\n",
    "        for j in 1:d\n",
    "            s_jk[j, k + 1] = countnz(X[j, :] .> k)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    α_new = zeros(d)\n",
    "    α_old = copy(α0)\n",
    "    loglold = sum(dirmult_logpdf(X, α0))\n",
    "    niter = 0\n",
    "    \n",
    "    # of alpha update\n",
    "    for iter in 1:maxiters\n",
    "        αupdate_num = zeros(d)\n",
    "        αupdate_denom = 0.0\n",
    "        sum_α = sum(α_old)\n",
    "        for k in 0:max_sum_xi\n",
    "            if (sum_α + k) == 0 \n",
    "                continue\n",
    "            end\n",
    "            αupdate_denom += r_k[k + 1] / (sum_α + k)\n",
    "        end\n",
    "        \n",
    "        for k in 0:max_xij\n",
    "            for j in 1:d\n",
    "                if (α_old[j] + k) == 0\n",
    "                    continue\n",
    "                end\n",
    "                αupdate_num[j] += s_jk[j, k + 1] / (α_old[j] + k)\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        α_new = (α_old .* αupdate_num) / αupdate_denom\n",
    "        \n",
    "        logl = sum(dirmult_logpdf(X, α_new))\n",
    "        \n",
    "        if abs(logl - loglold) < tolfun * (abs(loglold) + 1)\n",
    "            niter = iter\n",
    "            break;\n",
    "        end\n",
    "        \n",
    "        α_old = α_new\n",
    "        loglold = logl \n",
    "    end\n",
    "   \n",
    "    logl = sum(dirmult_logpdf(X, α_new))\n",
    "    \n",
    "    row_sumα = sum(α_new)\n",
    "    row_sum_X = sum(X, 2)\n",
    "    ∇ = zeros(d)\n",
    "    obsinfo = zeros(d, d)\n",
    "    \n",
    "    # evaluate gradient (score)\n",
    "    dg_α = digamma.(α_new)\n",
    "    dg_α_sum = digamma.(row_sumα)\n",
    "    \n",
    "    for j in 1:d, i in 1:n\n",
    "        if row_sum_X[j] == 0\n",
    "            continue\n",
    "        end\n",
    "            ∇[j] += digamma.(α_new[j] + X[j, i]) - dg_α[j] - \n",
    "            digamma.(row_sumα + row_sum_X[j]) + dg_α_sum\n",
    "    end\n",
    "\n",
    "    D = zeros(d)\n",
    "    c = 0.0\n",
    "    tg_α = trigamma.(α_new)\n",
    "    tg_α_sum = trigamma.(row_sumα)\n",
    "    \n",
    "    for j = 1:d, i = 1:n\n",
    "        if row_sum_X[j] > 0\n",
    "            D[j] += - trigamma.(α_new[j] + X[j, i]) + tg_α[j]\n",
    "            c += - trigamma.(row_sumα + row_sum_X[j]) + tg_α_sum\n",
    "        end\n",
    "    end\n",
    "\n",
    "    obsinfo = Diagonal(D) + (c * ones(d, d))\n",
    "    \n",
    "    α_mle = zeros(d)\n",
    "    α_mle = α_new\n",
    "    \n",
    "    return logl, niter, α_mle, ∇, obsinfo\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-472076.27153735596, 34, [0.0, 0.0809462, 0.896006, 2.10579, 2.02128, 0.76506, 0.146443, 0.0151447, 0.000389661, 0.301589  …  0.460086, 0.0272511, 0.000129835, 0.0654668, 0.920939, 2.11522, 1.94414, 0.946323, 0.23171, 0.0277971], [0.0, -4590.96, -15536.5, -18465.9, -18349.4, -15552.9, -10307.2, -1890.18, 7114.7, -11619.5  …  -14082.2, -2035.88, 7623.54, -4361.22, -15788.4, -18510.0, -18352.8, -16303.0, -11890.9, -3140.11], [4363.39 4363.39 … 4363.39 4363.39; 4363.39 95162.7 … 4363.39 4363.39; … ; 4363.39 4363.39 … 29620.0 4363.39; 4363.39 4363.39 … 4363.39 2.72437e5])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logl, niter, α_mle, ∇, obsinfo = dirmult_mm(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-475801.97944981215"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "niter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64-element Array{Float64,1}:\n",
       " 0.0        \n",
       " 0.0809462  \n",
       " 0.896006   \n",
       " 2.10579    \n",
       " 2.02128    \n",
       " 0.76506    \n",
       " 0.146443   \n",
       " 0.0151447  \n",
       " 0.000389661\n",
       " 0.301589   \n",
       " 1.74968    \n",
       " 2.14562    \n",
       " 1.87275    \n",
       " ⋮          \n",
       " 1.66123    \n",
       " 1.40425    \n",
       " 0.460086   \n",
       " 0.0272511  \n",
       " 0.000129835\n",
       " 0.0654668  \n",
       " 0.920939   \n",
       " 2.11522    \n",
       " 1.94414    \n",
       " 0.946323   \n",
       " 0.23171    \n",
       " 0.0277971  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "α_mle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64-element Array{Float64,1}:\n",
       "      0.0 \n",
       "  -4590.96\n",
       " -15536.5 \n",
       " -18465.9 \n",
       " -18349.4 \n",
       " -15552.9 \n",
       " -10307.2 \n",
       "  -1890.18\n",
       "   7114.7 \n",
       " -11619.5 \n",
       " -18046.3 \n",
       " -18436.3 \n",
       " -18063.2 \n",
       "      ⋮   \n",
       " -17740.8 \n",
       " -17548.1 \n",
       " -14082.2 \n",
       "  -2035.88\n",
       "   7623.54\n",
       "  -4361.22\n",
       " -15788.4 \n",
       " -18510.0 \n",
       " -18352.8 \n",
       " -16303.0 \n",
       " -11890.9 \n",
       "  -3140.11"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "∇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64×64 Array{Float64,2}:\n",
       " 4363.39   4363.39  4363.39  4363.39  …  4363.39   4363.39  4363.39     \n",
       " 4363.39  95162.7   4363.39  4363.39     4363.39   4363.39  4363.39     \n",
       " 4363.39   4363.39  9524.22  4363.39     4363.39   4363.39  4363.39     \n",
       " 4363.39   4363.39  4363.39  6290.14     4363.39   4363.39  4363.39     \n",
       " 4363.39   4363.39  4363.39  4363.39     4363.39   4363.39  4363.39     \n",
       " 4363.39   4363.39  4363.39  4363.39  …  4363.39   4363.39  4363.39     \n",
       " 4363.39   4363.39  4363.39  4363.39     4363.39   4363.39  4363.39     \n",
       " 4363.39   4363.39  4363.39  4363.39     4363.39   4363.39  4363.39     \n",
       " 4363.39   4363.39  4363.39  4363.39     4363.39   4363.39  4363.39     \n",
       " 4363.39   4363.39  4363.39  4363.39     4363.39   4363.39  4363.39     \n",
       " 4363.39   4363.39  4363.39  4363.39  …  4363.39   4363.39  4363.39     \n",
       " 4363.39   4363.39  4363.39  4363.39     4363.39   4363.39  4363.39     \n",
       " 4363.39   4363.39  4363.39  4363.39     4363.39   4363.39  4363.39     \n",
       "    ⋮                                 ⋱                                 \n",
       " 4363.39   4363.39  4363.39  4363.39     4363.39   4363.39  4363.39     \n",
       " 4363.39   4363.39  4363.39  4363.39     4363.39   4363.39  4363.39     \n",
       " 4363.39   4363.39  4363.39  4363.39     4363.39   4363.39  4363.39     \n",
       " 4363.39   4363.39  4363.39  4363.39  …  4363.39   4363.39  4363.39     \n",
       " 4363.39   4363.39  4363.39  4363.39     4363.39   4363.39  4363.39     \n",
       " 4363.39   4363.39  4363.39  4363.39     4363.39   4363.39  4363.39     \n",
       " 4363.39   4363.39  4363.39  4363.39     4363.39   4363.39  4363.39     \n",
       " 4363.39   4363.39  4363.39  4363.39     4363.39   4363.39  4363.39     \n",
       " 4363.39   4363.39  4363.39  4363.39  …  4363.39   4363.39  4363.39     \n",
       " 4363.39   4363.39  4363.39  4363.39     8909.75   4363.39  4363.39     \n",
       " 4363.39   4363.39  4363.39  4363.39     4363.39  29620.0   4363.39     \n",
       " 4363.39   4363.39  4363.39  4363.39     4363.39   4363.39     2.72437e5"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obsinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6\n",
    "\n",
    "Re-do [HW4 Q9](http://hua-zhou.github.io/teaching/biostatm280-2018spring/hw/hw4/hw04.html#Q9) using your new `dirmult_mm` function. Compare the number of iterations and run time by MM algorithm to those by Newton's method. Comment on the efficiency of Newton's algorithm vs MM algorithm for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7\n",
    "\n",
    "Finally let us re-consider the EM algorithm. The difficulty with the M step in EM algorithm can be remedied. Discuss how we can further minorize the $\\ln \\Gamma(|\\alpha|)$ term in the $Q$ function to produce a minorizing function with all $\\alpha_j$ separated. For this homework, you do **not** need to implement this EM-MM hybrid algorithm. Hint: $z \\mapsto \\ln \\Gamma(z)$ is a convex function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7 Solution:\n",
    "\n",
    "If we re-consider the EM algorithm, the difficulty with the M step in EM algorithm can be remedied by further minorizing the $\\ln \\Gamma(|\\alpha|)$ term in the $Q$ function to produce a minorizing function with all $\\alpha_j$ separated.\n",
    "\n",
    "Recall the $Q$ function from above:\n",
    "\n",
    "$$\n",
    "    Q(\\alpha|\\alpha^{(t)}) = \n",
    "\\sum_{j=1}^d \\sum_{i=1}^n \\alpha_j \\left[\\Psi(x_{ij}+\\alpha_j^{(t)}) - \\Psi(|\\mathbf{x}_i|+|\\alpha^{(t)}|)\\right] - n \\sum_{j=1}^d \\ln \\Gamma(\\alpha_j) + n \\ln \\Gamma(|\\alpha|) + c^{(t)},\n",
    "$$\n",
    "where $c^{(t)}$ is a constant irrelevant to optimization.\n",
    "<br>\n",
    "Using the hint given, $f_1(z) = \\ln \\Gamma(z)$ is a convex function. Thus, $f_2(z) = - \\ln \\Gamma(z)$ is a concave function. Now we can apply Jensen's Inequality on the function $f_2(z)$ to find a minorizing function. \n",
    "<br>\n",
    "<br>\n",
    "Let $f = -\\ln \\Gamma(|\\alpha|),$ which can be rewritten, with $|\\alpha|$ as the sum of $\\alpha_j + \\sum_{i = 1 | i\\ne j}^d \\alpha_i$ \n",
    "\n",
    "$f = \\ln \\Gamma(|\\alpha|) = \\ln \\Gamma (\\alpha_j + \\sum_{i = 1 | i\\ne j}^d \\alpha_i)$\n",
    "\n",
    "Now applying Jensen's Inequality on this concave term, we have:\n",
    "\n",
    "$$\n",
    "-\\ln \\Gamma (\\alpha_j + \\sum_{i = 1 | i\\ne j}^d \\alpha_i) \\ge \\Big(\\frac{\\alpha_j}{\\alpha_j + \\sum_{i = 1 | i\\ne j}^d \\alpha_i}\\Big)\\ln \\Gamma\\Big(\\frac{\\alpha_j + \\sum_{i = 1 | i\\ne j}^d \\alpha_i}{\\alpha_j} \\alpha_j\\Big) + \\Big(\\frac{\\sum_{i = 1 | i\\ne j}^d \\alpha_i}{\\alpha_j + \\sum_{i = 1 | i\\ne j}^d \\alpha_i}\\Big)\\ln \\Gamma\\Big(\\frac{\\alpha_j + \\sum_{i = 1 | i\\ne j}^d \\alpha_i}{\\sum_{i = 1 | i\\ne j}^d \\alpha_i} \\sum_{i = 1 | i\\ne j}^d \\alpha_i\\Big)\n",
    "$$\n",
    "<br>\n",
    "\n",
    "$$\n",
    "    Q(\\alpha|\\alpha^{(t)}) = \n",
    "\\sum_{j=1}^d \\sum_{i=1}^n \\alpha_j \\left[\\Psi(x_{ij}+\\alpha_j^{(t)}) - \\Psi(|\\mathbf{x}_i|+|\\alpha^{(t)}|)\\right] - n\\Big( \\sum_{j=1}^d \\ln \\Gamma(\\alpha_j) - \\ln \\Gamma(|\\alpha|)\\Big)+ c^{(t)},\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\ge \\sum_{j=1}^d \\sum_{i=1}^n \\alpha_j \\left[\\Psi(x_{ij}+\\alpha_j^{(t)}) - \\Psi(|\\mathbf{x}_i|+|\\alpha^{(t)}|)\\right] - n\\Big( \\sum_{j=1}^d \\ln \\Gamma(\\alpha_j) + \\Big[\\Big(\\frac{\\alpha_j}{\\alpha_j + \\sum_{i = 1 | i\\ne j}^d \\alpha_i}\\Big)\\ln \\Gamma\\Big(\\frac{\\alpha_j + \\sum_{i = 1 | i\\ne j}^d \\alpha_i}{\\alpha_j} \\alpha_j\\Big) + \\Big(\\frac{\\sum_{i = 1 | i\\ne j}^d \\alpha_i}{\\alpha_j + \\sum_{i = 1 | i\\ne j}^d \\alpha_i}\\Big)\\ln \\Gamma\\Big(\\frac{\\alpha_j + \\sum_{i = 1 | i\\ne j}^d \\alpha_i}{\\sum_{i = 1 | i\\ne j}^d \\alpha_i} \\sum_{i = 1 | i\\ne j}^d \\alpha_i\\Big)\\Big]\\Big)+ c^{(t)}\n",
    "$$\n",
    "\n",
    "\n",
    "Now all of the $\\alpha_j$ parameters are separated, and we can easily maximize with respect to each $\\alpha_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "65px",
    "width": "252px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
